{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG School 2025 \u00b6 Could you transform your research with vast amounts of computing? Learn how this summer at the lovely University of Wisconsin\u2013Madison During the School \u2014 June 23\u201327 \u2014 you will learn to use high-throughput computing (HTC) systems, at your own campus or using the national-scale Open Science Pool , to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, you will learn how HTC systems work, how to run and manage long lists of computing tasks and work with huge datasets to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for: Researchers (especially graduate students and post-docs) in any research area for which large-scale computing is a vital part of the research process; Anyone (especially students and staff) who supports researchers who are current or potential users of high-throughput computing; Instructors (at the post-secondary level) who teach future researchers and see value in integrating high-throughput computing into their curriculum. People accepted to this program will receive financial support for basic travel and local costs associated with the School. Welcome! \u00b6 The School is the week of June 23\u201327. We welcome the roughly sixty participants from around the United States, plus Makerere University in Uganda! Contact Us \u00b6 The OSG School is the premier training event of the OSG Consortium and is held annually at UW\u2013Madison. If you have any questions about the event, feel free to email us: school@osg-htc.org OSGSchool * Image provided by Wikimedia user Av9 under Creative Commons License","title":"Home"},{"location":"#osg-school-2025","text":"Could you transform your research with vast amounts of computing? Learn how this summer at the lovely University of Wisconsin\u2013Madison During the School \u2014 June 23\u201327 \u2014 you will learn to use high-throughput computing (HTC) systems, at your own campus or using the national-scale Open Science Pool , to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, you will learn how HTC systems work, how to run and manage long lists of computing tasks and work with huge datasets to implement a scientific computing workflow, and where to turn for more information and help. The School is ideal for: Researchers (especially graduate students and post-docs) in any research area for which large-scale computing is a vital part of the research process; Anyone (especially students and staff) who supports researchers who are current or potential users of high-throughput computing; Instructors (at the post-secondary level) who teach future researchers and see value in integrating high-throughput computing into their curriculum. People accepted to this program will receive financial support for basic travel and local costs associated with the School.","title":"OSG School 2025"},{"location":"#welcome","text":"The School is the week of June 23\u201327. We welcome the roughly sixty participants from around the United States, plus Makerere University in Uganda!","title":"Welcome!"},{"location":"#contact-us","text":"The OSG School is the premier training event of the OSG Consortium and is held annually at UW\u2013Madison. If you have any questions about the event, feel free to email us: school@osg-htc.org OSGSchool * Image provided by Wikimedia user Av9 under Creative Commons License","title":"Contact Us"},{"location":"health/","text":"Health Guidelines \u00b6 The OSG School 2025 at the UW\u2013Madison welcomes participants from around the United States plus Uganda. This page contains health guidelines for this year\u2019s School. While the focus is on COVID-19, most of these guidelines also apply to preventing the spread of other infectious illnesses (flu, colds, GI viruses, etc.). It is very important to us that everyone stays safe and healthy throughout the whole School. We will have the best event possible if everyone stays well! There are no hard rules here, just a reminder that we are all in this together . If you have any questions, concerns, or comments about these guidelines, please email us at school@osg-htc.org or message us on Slack. Before Traveling to the School \u00b6 If you tested positive for COVID recently (past 2 weeks or so), please follow CDC guidelines for what to do when sick. Even if you have no symptoms or known exposure, consider taking a rapid test before traveling to improve the odds that you are not bringing COVID to the event. If you DO test positive before the School, or if you do not feel well enough to travel for any reason, please let us know immediately so we can accommodate (see below for remote participation options). While in Madison \u00b6 Wearing a mask is welcome at the School itself when indoors or in other poorly ventilated areas. We can provide a few high-quality KN95 masks for people who would like them and have not brought their own. We encourage everyone to consider outdoor dining options when reasonable \u2014 not just for reducing risk, but also because Madison is beautiful in the summer! While in Madison, if you feel unwell, stay home or at the hotel. When you can, let School staff know why you are absent \u2014 by email or Slack \u2014 and if you would like to keep up with exercises and lectures, we will help support you remotely (see below). If you experience possible symptoms of COVID-19 , or test positive for COVID-19, follow CDC guidelines for what to do when sick. Remote Attendance \u00b6 If you are in Madison and are sick or quarantined, or if you are not able to travel to Madison, we will do our best to support you via remote attendance. We learned a lot about remote events during the pandemic! We can: Stream lectures live over Zoom Post all slides and exercises on the website Be active on Slack and email Conduct one-on-one consultations over Zoom As long as you feel up to it, we will do our best to support you during the School.","title":"Health Guidelines"},{"location":"health/#health-guidelines","text":"The OSG School 2025 at the UW\u2013Madison welcomes participants from around the United States plus Uganda. This page contains health guidelines for this year\u2019s School. While the focus is on COVID-19, most of these guidelines also apply to preventing the spread of other infectious illnesses (flu, colds, GI viruses, etc.). It is very important to us that everyone stays safe and healthy throughout the whole School. We will have the best event possible if everyone stays well! There are no hard rules here, just a reminder that we are all in this together . If you have any questions, concerns, or comments about these guidelines, please email us at school@osg-htc.org or message us on Slack.","title":"Health Guidelines"},{"location":"health/#before-traveling-to-the-school","text":"If you tested positive for COVID recently (past 2 weeks or so), please follow CDC guidelines for what to do when sick. Even if you have no symptoms or known exposure, consider taking a rapid test before traveling to improve the odds that you are not bringing COVID to the event. If you DO test positive before the School, or if you do not feel well enough to travel for any reason, please let us know immediately so we can accommodate (see below for remote participation options).","title":"Before Traveling to the School"},{"location":"health/#while-in-madison","text":"Wearing a mask is welcome at the School itself when indoors or in other poorly ventilated areas. We can provide a few high-quality KN95 masks for people who would like them and have not brought their own. We encourage everyone to consider outdoor dining options when reasonable \u2014 not just for reducing risk, but also because Madison is beautiful in the summer! While in Madison, if you feel unwell, stay home or at the hotel. When you can, let School staff know why you are absent \u2014 by email or Slack \u2014 and if you would like to keep up with exercises and lectures, we will help support you remotely (see below). If you experience possible symptoms of COVID-19 , or test positive for COVID-19, follow CDC guidelines for what to do when sick.","title":"While in Madison"},{"location":"health/#remote-attendance","text":"If you are in Madison and are sick or quarantined, or if you are not able to travel to Madison, we will do our best to support you via remote attendance. We learned a lot about remote events during the pandemic! We can: Stream lectures live over Zoom Post all slides and exercises on the website Be active on Slack and email Conduct one-on-one consultations over Zoom As long as you feel up to it, we will do our best to support you during the School.","title":"Remote Attendance"},{"location":"schedule/","text":"June 22 (Sunday) \u00b6 Welcome Dinner for Participants and Staff All School participants and staff are encourage to attend! Time: Starting at 6:30 p.m. Location : Union South , 1308 West Dayton Street, Industry room (3rd floor) June 23 (Monday) \u00b6 Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:15 Welcome to the OSG School Tim C. 9:15 9:30 Lecture: Introduction to High Throughput Computing Christina 9:30 9:45 Exercise: Scaling Out Computing Worksheet - 9:45 10:15 Lecture: Introduction to HTCondor Amber 10:15 10:30 Exercise: Log in - 10:30 10:45 Break - 10:45 12:15 Exercises: HTCondor basics (1.n series) - 12:15 13:15 Lunch Domain groups: TBD - 13:15 14:00 Lecture: More HTCondor Amber 14:15 15:00 Exercises: Many jobs (2.n series) - 15:00 15:15 Break - 15:15 15:30 Lecture: Setting goals for the School and beyond Christina 15:30 17:00 Exercises: Goals and unfinished exercises Individual consultations - 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Andrew, Tim C. June 24 (Tuesday) \u00b6 Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Introduction to dHTC and the OSPool Tim C. 9:45 10:30 Exercises: Using the OSPool - 10:30 10:45 Break Also: Collect travel documents as needed - 10:45 11:30 Lecture: Troubleshooting jobs Showmic 11:30 12:15 Exercises: Basic troubleshooting tools - 12:15 13:30 Lunch Software groups Also: Return travel documents (~13:20) - 13:30 14:45 Interactive: High Throughput Computing in action staff 14:45 15:00 Break - 15:00 15:45 Lecture: Software portability Andrew 15:45 17:00 Exercises: Software and unfinished exercises Individual consultations staff 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Christina, Showmic, Tim C. June 25 (Wednesday) \u00b6 Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Working with data Showmic 9:45 10:45 Exercises: Data - 10:45 11:00 Break - 11:00 12:00 HTC Showcase Part 1 \u25b6 Spencer Ericksen ; Small Molecule Facility \u201cScaling Virtual Screening to Ultra-Large Virtual Chemical Libraries\u201d \u25b6 Michael Martinez ; Physics \u201cDark Matter, Radio Astronomy, and High Throughput Computing\u201d - 12:00 12:30 Open Q&A and discussion time staff 12:30 13:45 Lunch - 13:45 17:00 Afternoon off or optional work time Or, individual consultations staff 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Amber, Showmic June 26 (Thursday) \u00b6 Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Independence in Research Computing Danny 9:45 10:45 Exercises: Scaling up - 10:45 11:00 Break - 11:00 12:00 Lecture: DAGMan Andrew 12:00 13:15 Lunch - 13:15 14:30 Exercises: DAGMan Work Time: Apply HTC to own research Individual consultations Bioinformatics work session (optional) staff 14:30 14:45 Break - 14:45 15:45 Work Time: Apply HTC to own research Individual consultations Bioinformatics work session (optional) staff 15:45 16:30 Lecture: Machine Learning Ian 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Danny, Tim C. June 27 (Friday) \u00b6 Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Optional Lecture: Self-Checkpointing Work Time: Apply HTC to own research Showmic 9:45 10:30 Work time: Apply HTC to own research Individual consultations Optional Introduction to Research Computing Facilitation staff 10:30 10:45 Break - 10:45 11:30 Work time: Apply HTC to own research - 11:30 11:50 Group photo (details TBD) - 11:50 13:00 Lunch staff 13:00 14:00 HTC Showcase, Part 2 \u25b6 Daniel Morales ; Biological Sciences \u25b6 Blaine Hoak ; Computer Sciences - 14:00 14:20 Open Q&A Break - 14:20 15:20 Lightning talks by volunteer participants Attendees 15:20 15:45 Open Q&A break staff 15:45 16:40 HTC and HTCondor Philosophy Dr. Miron Livny; Greg Thain 16:40 17:10 Lecture: Forward Tim Closing Dinner for Participants and Staff Location: Fluno Center , 601 University Avenue; Skyview Room, 8th floor Time: Starting at 6:30 p.m.","title":"Schedule"},{"location":"schedule/#june-22-sunday","text":"Welcome Dinner for Participants and Staff All School participants and staff are encourage to attend! Time: Starting at 6:30 p.m. Location : Union South , 1308 West Dayton Street, Industry room (3rd floor)","title":"June 22 (Sunday)"},{"location":"schedule/#june-23-monday","text":"Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:15 Welcome to the OSG School Tim C. 9:15 9:30 Lecture: Introduction to High Throughput Computing Christina 9:30 9:45 Exercise: Scaling Out Computing Worksheet - 9:45 10:15 Lecture: Introduction to HTCondor Amber 10:15 10:30 Exercise: Log in - 10:30 10:45 Break - 10:45 12:15 Exercises: HTCondor basics (1.n series) - 12:15 13:15 Lunch Domain groups: TBD - 13:15 14:00 Lecture: More HTCondor Amber 14:15 15:00 Exercises: Many jobs (2.n series) - 15:00 15:15 Break - 15:15 15:30 Lecture: Setting goals for the School and beyond Christina 15:30 17:00 Exercises: Goals and unfinished exercises Individual consultations - 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Andrew, Tim C.","title":"June 23 (Monday)"},{"location":"schedule/#june-24-tuesday","text":"Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Introduction to dHTC and the OSPool Tim C. 9:45 10:30 Exercises: Using the OSPool - 10:30 10:45 Break Also: Collect travel documents as needed - 10:45 11:30 Lecture: Troubleshooting jobs Showmic 11:30 12:15 Exercises: Basic troubleshooting tools - 12:15 13:30 Lunch Software groups Also: Return travel documents (~13:20) - 13:30 14:45 Interactive: High Throughput Computing in action staff 14:45 15:00 Break - 15:00 15:45 Lecture: Software portability Andrew 15:45 17:00 Exercises: Software and unfinished exercises Individual consultations staff 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Christina, Showmic, Tim C.","title":"June 24 (Tuesday)"},{"location":"schedule/#june-25-wednesday","text":"Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Working with data Showmic 9:45 10:45 Exercises: Data - 10:45 11:00 Break - 11:00 12:00 HTC Showcase Part 1 \u25b6 Spencer Ericksen ; Small Molecule Facility \u201cScaling Virtual Screening to Ultra-Large Virtual Chemical Libraries\u201d \u25b6 Michael Martinez ; Physics \u201cDark Matter, Radio Astronomy, and High Throughput Computing\u201d - 12:00 12:30 Open Q&A and discussion time staff 12:30 13:45 Lunch - 13:45 17:00 Afternoon off or optional work time Or, individual consultations staff 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Amber, Showmic","title":"June 25 (Wednesday)"},{"location":"schedule/#june-26-thursday","text":"Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Lecture: Independence in Research Computing Danny 9:45 10:45 Exercises: Scaling up - 10:45 11:00 Break - 11:00 12:00 Lecture: DAGMan Andrew 12:00 13:15 Lunch - 13:15 14:30 Exercises: DAGMan Work Time: Apply HTC to own research Individual consultations Bioinformatics work session (optional) staff 14:30 14:45 Break - 14:45 15:45 Work Time: Apply HTC to own research Individual consultations Bioinformatics work session (optional) staff 15:45 16:30 Lecture: Machine Learning Ian 19:00 20:30 Evening work session (optional) Memorial Union \u2013 Board Room (3rd Floor East) Danny, Tim C.","title":"June 26 (Thursday)"},{"location":"schedule/#june-27-friday","text":"Start End Event Instructor 8:00 8:45 Breakfast - 9:00 9:45 Optional Lecture: Self-Checkpointing Work Time: Apply HTC to own research Showmic 9:45 10:30 Work time: Apply HTC to own research Individual consultations Optional Introduction to Research Computing Facilitation staff 10:30 10:45 Break - 10:45 11:30 Work time: Apply HTC to own research - 11:30 11:50 Group photo (details TBD) - 11:50 13:00 Lunch staff 13:00 14:00 HTC Showcase, Part 2 \u25b6 Daniel Morales ; Biological Sciences \u25b6 Blaine Hoak ; Computer Sciences - 14:00 14:20 Open Q&A Break - 14:20 15:20 Lightning talks by volunteer participants Attendees 15:20 15:45 Open Q&A break staff 15:45 16:40 HTC and HTCondor Philosophy Dr. Miron Livny; Greg Thain 16:40 17:10 Lecture: Forward Tim Closing Dinner for Participants and Staff Location: Fluno Center , 601 University Avenue; Skyview Room, 8th floor Time: Starting at 6:30 p.m.","title":"June 27 (Friday)"},{"location":"logistics/","text":"OSG School 2025 Logistics \u00b6 The following pages describe some of the important information about your visit to Madison for the OSG School. Please read them carefully. There will be other pages with local details soon. Visa requirements for non-residents Travel planning to and from Madison Hotel information As always: If you have questions, email us at school@osg-htc.org . Use that email address for all emails about the organization of the School. General Information About the School Schedule \u00b6 Travel Schedule \u00b6 Most participants should plan to travel as follows: Arrive on Sunday, June 22, 2025, by about 5:00\u20135:30pm (if possible). There is a welcome dinner on Sunday evening for all participants and staff, and then classes begin on Monday morning. The dinner is a nice way to learn more about the School and begin the week. Depart on Saturday, June 28, 2025, any time. The School ends with a closing dinner on Friday evening, so it is best to stay that night. If we offered to pay for your hotel room, we will pay for the 6 nights of this schedule. Note: If we suggested other travel dates to you in an email, then use those dates instead! School Hours \u00b6 The School is generally Monday through Friday, 9:00 a.m. to about 5:00 p.m., except Wednesday afternoon. There will be optional work sessions on Monday, Tuesday, Wednesday, and Thursday evenings. The detailed schedule is available. Contact Information \u00b6 If you have questions, do not wait to contact us! school@osg-htc.org","title":"General information"},{"location":"logistics/#osg-school-2025-logistics","text":"The following pages describe some of the important information about your visit to Madison for the OSG School. Please read them carefully. There will be other pages with local details soon. Visa requirements for non-residents Travel planning to and from Madison Hotel information As always: If you have questions, email us at school@osg-htc.org . Use that email address for all emails about the organization of the School.","title":"OSG School 2025 Logistics"},{"location":"logistics/#general-information-about-the-school-schedule","text":"","title":"General Information About the School Schedule"},{"location":"logistics/#travel-schedule","text":"Most participants should plan to travel as follows: Arrive on Sunday, June 22, 2025, by about 5:00\u20135:30pm (if possible). There is a welcome dinner on Sunday evening for all participants and staff, and then classes begin on Monday morning. The dinner is a nice way to learn more about the School and begin the week. Depart on Saturday, June 28, 2025, any time. The School ends with a closing dinner on Friday evening, so it is best to stay that night. If we offered to pay for your hotel room, we will pay for the 6 nights of this schedule. Note: If we suggested other travel dates to you in an email, then use those dates instead!","title":"Travel Schedule"},{"location":"logistics/#school-hours","text":"The School is generally Monday through Friday, 9:00 a.m. to about 5:00 p.m., except Wednesday afternoon. There will be optional work sessions on Monday, Tuesday, Wednesday, and Thursday evenings. The detailed schedule is available.","title":"School Hours"},{"location":"logistics/#contact-information","text":"If you have questions, do not wait to contact us! school@osg-htc.org","title":"Contact Information"},{"location":"logistics/account-setup/","text":".hi { font-weight: bold; color: #FF6600; } Apply for Computing Access \u00b6 We will be using two different Access Points during the OSG School - ap40.uw.osg-htc.org and ap1.facility.path-cc.io . As soon as possible please request your account access using this link: OSG School Account Registration Instructions on setting up your account can be found using this guide: Log in to uw.osg-htc.org Access Points We strongly recommend going through the registration process and trying to log in before the School, ideally before your OSG orientation session. If you run into problems contact us at support@osg-htc.org .","title":"Account setup"},{"location":"logistics/account-setup/#apply-for-computing-access","text":"We will be using two different Access Points during the OSG School - ap40.uw.osg-htc.org and ap1.facility.path-cc.io . As soon as possible please request your account access using this link: OSG School Account Registration Instructions on setting up your account can be found using this guide: Log in to uw.osg-htc.org Access Points We strongly recommend going through the registration process and trying to log in before the School, ideally before your OSG orientation session. If you run into problems contact us at support@osg-htc.org .","title":"Apply for Computing Access"},{"location":"logistics/assignment/","text":"OSG School 2025 Assignment \u00b6 The School focused on using high-throughput computing (HTC) to support and transform research. Your final assignment lets you show what you learned and how you might apply your new abilities. We ask for it to: Reinforce and consolidate what you learned at the School Prepare you to take real action on your large-scale computational challenge(s) Demonstrate the value of the School to our funding agencies and to your advisor, colleagues, etc. Guide us as we try to improve the School Option 1: Lightning Talk on Last Day of School \u00b6 If you did a lightning talk on Friday, June 27, it was your final assignment and you are done. Option 2: Short Write-Up \u00b6 For most people, the assignment is a short write-up. This is not a formal paper, report, essay, or anything like that. We will not publish your write-up \u2014 it is just between you and us. Content guidelines \u00b6 For the content, think of a written version of a lightning talk: Describe your science for a broad audience in no more than 1 page (this is a good life skill!) Describe one computing challenge that you want to work on (1 paragraph to 1 page) Explain your plan to work on that challenge with the new things you learned at the School (probably a whole page) For the last part, your audience is the School staff, so you do not have to re-explain concepts and methods that we explained here. Instead, write about how you will apply those things! Try to be detailed about your planned work, including things like resource needs and so on; but be sure to start with the high-level plan so we understand the context. A simple diagram may help. Be sure to put most of your effort into the third part, the plan. If you paste in three pages of intro from another paper, and then write one paragraph about your plan, we will probably ask for more details on the plan! Format guidelines \u00b6 Write at least a full page and no more than four pages of text (single-spaced) Add images, tables, or figures if you like; they may cause your total page count to go over four pages Do not take time to add citations, references, etc. Email your write-up as a PDF (only!) to the school@osg-htc.org list Deadline \u00b6 The paper is due 1 August 2025. Contact us early if you need an extension. Questions? \u00b6 If you have any questions or comments about the assignment, please contact us at the school@osg-htc.org mailing list.","title":"Final assignment (Aug 1)"},{"location":"logistics/assignment/#osg-school-2025-assignment","text":"The School focused on using high-throughput computing (HTC) to support and transform research. Your final assignment lets you show what you learned and how you might apply your new abilities. We ask for it to: Reinforce and consolidate what you learned at the School Prepare you to take real action on your large-scale computational challenge(s) Demonstrate the value of the School to our funding agencies and to your advisor, colleagues, etc. Guide us as we try to improve the School","title":"OSG School 2025 Assignment"},{"location":"logistics/assignment/#option-1-lightning-talk-on-last-day-of-school","text":"If you did a lightning talk on Friday, June 27, it was your final assignment and you are done.","title":"Option 1: Lightning Talk on Last Day of School"},{"location":"logistics/assignment/#option-2-short-write-up","text":"For most people, the assignment is a short write-up. This is not a formal paper, report, essay, or anything like that. We will not publish your write-up \u2014 it is just between you and us.","title":"Option 2: Short Write-Up"},{"location":"logistics/assignment/#content-guidelines","text":"For the content, think of a written version of a lightning talk: Describe your science for a broad audience in no more than 1 page (this is a good life skill!) Describe one computing challenge that you want to work on (1 paragraph to 1 page) Explain your plan to work on that challenge with the new things you learned at the School (probably a whole page) For the last part, your audience is the School staff, so you do not have to re-explain concepts and methods that we explained here. Instead, write about how you will apply those things! Try to be detailed about your planned work, including things like resource needs and so on; but be sure to start with the high-level plan so we understand the context. A simple diagram may help. Be sure to put most of your effort into the third part, the plan. If you paste in three pages of intro from another paper, and then write one paragraph about your plan, we will probably ask for more details on the plan!","title":"Content guidelines"},{"location":"logistics/assignment/#format-guidelines","text":"Write at least a full page and no more than four pages of text (single-spaced) Add images, tables, or figures if you like; they may cause your total page count to go over four pages Do not take time to add citations, references, etc. Email your write-up as a PDF (only!) to the school@osg-htc.org list","title":"Format guidelines"},{"location":"logistics/assignment/#deadline","text":"The paper is due 1 August 2025. Contact us early if you need an extension.","title":"Deadline"},{"location":"logistics/assignment/#questions","text":"If you have any questions or comments about the assignment, please contact us at the school@osg-htc.org mailing list.","title":"Questions?"},{"location":"logistics/dining/","text":"Dining \u00b6 The School provides some catered meals as a group, and you are on your own for others. When on your own, there are many dining options in Madison between the School and your hotel, especially on State Street which is only blocks away from both locations. Restaurants right on and very near to the Capitol Square, onto which the hotel faces, tend to be a little more expensive. As you go toward campus on State Street or neighboring streets, prices tend to go down. But of course, there are exceptions in both directions! It is reasonable to ask to see a menu before ordering or being seated and decide whether to stay. Food Options Near the Hotel \u00b6 Use a mapping app or rating services like Yelp to look at food options. For example: Food Options Near the School \u00b6 There are not a lot of great food options very close to the School, but feel free to ask School staff for suggestions.","title":"Dining options"},{"location":"logistics/dining/#dining","text":"The School provides some catered meals as a group, and you are on your own for others. When on your own, there are many dining options in Madison between the School and your hotel, especially on State Street which is only blocks away from both locations. Restaurants right on and very near to the Capitol Square, onto which the hotel faces, tend to be a little more expensive. As you go toward campus on State Street or neighboring streets, prices tend to go down. But of course, there are exceptions in both directions! It is reasonable to ask to see a menu before ordering or being seated and decide whether to stay.","title":"Dining"},{"location":"logistics/dining/#food-options-near-the-hotel","text":"Use a mapping app or rating services like Yelp to look at food options. For example:","title":"Food Options Near the Hotel"},{"location":"logistics/dining/#food-options-near-the-school","text":"There are not a lot of great food options very close to the School, but feel free to ask School staff for suggestions.","title":"Food Options Near the School"},{"location":"logistics/fun-day/","text":"Fun Activity Ideas While in Madison \u00b6 Free \u00b6 Chazen Museum of Art : Free art museum at 750 University Ave. (~12 minute walk from School) UW\u2013Madison Geology Museum : Large collection of geological specimens. Across Dayton Street from the School building. 1215 Dayton Street (~2 minute walk from School) L.R. Ingersoll Physics Museum : Small museum of Physics objects and demonstrations. Very short walk from the School building: Chamberlin Hall, 1150 University Avenue. (~6 minute walk from School) Terrace Open Mic Night : Enjoy a night out where all styles of music, comedy, spoken word, poetry, and more take the stage. Performances start at 7 PM on Wednesday. 800 Langdon Street (~15 minute walk from School) Tour of Wisconsin State Capitol : Tours start at 1, 2, 3, and 4 p.m. and last about 45 minutes. 2 E Main Street (~29 minute walk from School and across from the Park Hotel) Henry Vilas Zoo : One-mile walk south of Computer Sciences: 702 South Randall Avenue. (~18 minute walk from School) Take a stroll or a ride on The Lakeshore Path : Reach the infamous Picnic Point or take your trip to the Arboretum! Cost \u00b6 Rent a Bcycle : Take advantage of Madison's many bike paths . Camp Randall Guided Tour : 1440 Monroe St; Tour starts promptly at 2:30 PM on Wednesday and will last approximately one hour; $10 per person (~8 minute walk from School) Paddling rentals on Lake Mendota : Paddling rentals, including paddleboards, kayak, and canoes. Memorial Union Terrace, $18 per hour. 800 Langdon Street (~15 minute walk from School) Tour of First Unitarian Society\u2019s Meeting House : The Landmark Auditorium was designed by Frank Lloyd Wright. $15 per person ($12.50 if booked online in advance), up to 10 people. 900 University Bay Drive (~38 minute walk; Bus accessible, with close stop) Olbrich Botanical Gardens : 16 acres outdoor (FREE); indoor: $6 conservatory; $8 butterfly house. 3330 Atwood Avenue (~15 minute drive from School; Bus accessible, with close stop) Disclaimer \u00b6 Madison Museum of Contemporary Art (MMoCA) is closed on Mondays and Tuesdays.","title":"Madison Fun Day"},{"location":"logistics/fun-day/#fun-activity-ideas-while-in-madison","text":"","title":"Fun Activity Ideas While in Madison"},{"location":"logistics/fun-day/#free","text":"Chazen Museum of Art : Free art museum at 750 University Ave. (~12 minute walk from School) UW\u2013Madison Geology Museum : Large collection of geological specimens. Across Dayton Street from the School building. 1215 Dayton Street (~2 minute walk from School) L.R. Ingersoll Physics Museum : Small museum of Physics objects and demonstrations. Very short walk from the School building: Chamberlin Hall, 1150 University Avenue. (~6 minute walk from School) Terrace Open Mic Night : Enjoy a night out where all styles of music, comedy, spoken word, poetry, and more take the stage. Performances start at 7 PM on Wednesday. 800 Langdon Street (~15 minute walk from School) Tour of Wisconsin State Capitol : Tours start at 1, 2, 3, and 4 p.m. and last about 45 minutes. 2 E Main Street (~29 minute walk from School and across from the Park Hotel) Henry Vilas Zoo : One-mile walk south of Computer Sciences: 702 South Randall Avenue. (~18 minute walk from School) Take a stroll or a ride on The Lakeshore Path : Reach the infamous Picnic Point or take your trip to the Arboretum!","title":"Free"},{"location":"logistics/fun-day/#cost","text":"Rent a Bcycle : Take advantage of Madison's many bike paths . Camp Randall Guided Tour : 1440 Monroe St; Tour starts promptly at 2:30 PM on Wednesday and will last approximately one hour; $10 per person (~8 minute walk from School) Paddling rentals on Lake Mendota : Paddling rentals, including paddleboards, kayak, and canoes. Memorial Union Terrace, $18 per hour. 800 Langdon Street (~15 minute walk from School) Tour of First Unitarian Society\u2019s Meeting House : The Landmark Auditorium was designed by Frank Lloyd Wright. $15 per person ($12.50 if booked online in advance), up to 10 people. 900 University Bay Drive (~38 minute walk; Bus accessible, with close stop) Olbrich Botanical Gardens : 16 acres outdoor (FREE); indoor: $6 conservatory; $8 butterfly house. 3330 Atwood Avenue (~15 minute drive from School; Bus accessible, with close stop)","title":"Cost"},{"location":"logistics/fun-day/#disclaimer","text":"Madison Museum of Contemporary Art (MMoCA) is closed on Mondays and Tuesdays.","title":"Disclaimer"},{"location":"logistics/hotel/","text":".hi { font-weight: bold; color: #FF6600; } Hotel Information \u00b6 We reserved a block of rooms at an area hotel for participants from outside Madison. Best Western Premier Park Hotel 22 South Carroll Street, Madison, WI +1 (608) 285\u20118000 Please note: We will reserve your room for you, so do not contact the hotel yourself to reserve a room. Exceptions to this rule are rare and clearly communicated. Other important hotel information: Before the School, we will send you an email with your hotel confirmation number We pay only for basic room costs \u2014 you must provide a credit card to cover extra costs There is one School participant per room; to have friends or family stay with you, please ask us now Check-In Time \u00b6 The (earliest) check-in time at the hotel is 4 p.m. on your day of arrival. If you are arriving earlier, you have options: Ask the hotel if it is possible to check in earlier than 4 p.m. It is up to the hotel to decide if they can meet your request. If there is any additional expense required, you must pay that yourself. Ask the hotel to put your bags in a safe spot and enjoy Madison until 4 p.m. or later. Keep your bags with you and enjoy Madison until 4 p.m. or later. Check-Out Time \u00b6 The (latest) check-out time from the hotel is 11 a.m. on your day of departure. If you are leaving later, you have options: Ask the hotel to put your bags in a safe spot and enjoy Madison until it is time to leave. Keep your bags with you and enjoy Madison until it is time to leave. You are not required to travel directly from the hotel to the airport, but if you do, we may be able to help you arrange to use the free hotel shuttle.","title":"Hotel information"},{"location":"logistics/hotel/#hotel-information","text":"We reserved a block of rooms at an area hotel for participants from outside Madison. Best Western Premier Park Hotel 22 South Carroll Street, Madison, WI +1 (608) 285\u20118000 Please note: We will reserve your room for you, so do not contact the hotel yourself to reserve a room. Exceptions to this rule are rare and clearly communicated. Other important hotel information: Before the School, we will send you an email with your hotel confirmation number We pay only for basic room costs \u2014 you must provide a credit card to cover extra costs There is one School participant per room; to have friends or family stay with you, please ask us now","title":"Hotel Information"},{"location":"logistics/hotel/#check-in-time","text":"The (earliest) check-in time at the hotel is 4 p.m. on your day of arrival. If you are arriving earlier, you have options: Ask the hotel if it is possible to check in earlier than 4 p.m. It is up to the hotel to decide if they can meet your request. If there is any additional expense required, you must pay that yourself. Ask the hotel to put your bags in a safe spot and enjoy Madison until 4 p.m. or later. Keep your bags with you and enjoy Madison until 4 p.m. or later.","title":"Check-In Time"},{"location":"logistics/hotel/#check-out-time","text":"The (latest) check-out time from the hotel is 11 a.m. on your day of departure. If you are leaving later, you have options: Ask the hotel to put your bags in a safe spot and enjoy Madison until it is time to leave. Keep your bags with you and enjoy Madison until it is time to leave. You are not required to travel directly from the hotel to the airport, but if you do, we may be able to help you arrange to use the free hotel shuttle.","title":"Check-Out Time"},{"location":"logistics/local-transportation/","text":"Local Transportation \u00b6 You are responsible for your own transportation within Madison, but we will help coordinate and can reimburse costs between the airport and your hotel. Travel Between the Madison Airport and Your Hotel \u00b6 For travel between the Madison airport (Dane County Regional Airport) and the School hotel , the best option is the hotel shuttle service, when available. Otherwise, you may use a ride-sharing service or taxi, or take the Madison Metro Bus. See below for details. We will help organize groups to take shuttles and taxis, based on arrival and departure times. Shuttle/taxi groups will be formed and emailed shortly before the School itself. Travel Between the Hotel and Campus \u00b6 For travel between the School hotel and the Computer Sciences building on campus, walking is a great option. Also, the hotel shuttle service may be available, especially if organized in advance. See below for details. Options for Getting Around \u00b6 Hotel Shuttle \u00b6 The Park Hotel operates a free shuttle service. The shuttle may not be available at all times, though, and it is best to plan ahead. Work with the hotel staff, individually or even better in groups, to use the shuttle. As noted above, we will help organize groups for the shuttle for airport arrivals on Sunday and departures on Saturday. To ask about the shuttle, either stop by the front desk of the hotel, or call +1 (608) 285-8000 and press 0 for the front desk. Explain that you are a guest at the hotel and ask if the shuttle is available for the number of people in your group; be clear about where you want to go from and to and at what time. We will send the hotel our list of groups who would like the shuttle for airport trips, but it is still best for the leader of each group to check with the hotel anyway. Walking \u00b6 It is easy to walk in and around the University of Wisconsin\u2013Madison campus, with many Madison landmarks within a mile of the School and your hotel. Use a mapping app or ask us or your hotel for a map. In particular, State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great restaurants and shops and is worth walking along while you are here. City of Madison Metro Bus Service \u00b6 Many Madison Metro buses stop near the hotel and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00, and if using a transfer ask the driver for a free transfer pass upon boarding. You can pay with cash, or, if you prefer, you can download the Madison Metro Transit Fast Fares mobile app, which allows you to add funds and scan a QR code to board the bus. This route does require you to make an account. Google Maps is a great resource for finding the best bus routes to use in Madison, giving multiple route options for each trip. Additionally, the Madison Metro Website provides a web interface to plan your trip. Note Bus routes stop running around ~11pm each day. Taxis and Ride-Sharing Services \u00b6 Both Lyft and Uber are active in Madison, or you can choose from our local taxi companies, such as Madison Taxi and Union Cab . We cannot recommend any particular option, but those are some options we know about. Note We cannot reimburse for any taxi or rideshare service beyond the ride to and from the airport. Note We will need receipts for any ride-share or taxi fare over $25. Madison BCycle \u00b6 Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle . Bcycles are available throughout the city , including near the hotel and around campus. Pricing for Bcycles can be found on their website and consist of several tiers. Note Unfortunately, we are not able to reimburse BCycle costs.","title":"Local Transportation"},{"location":"logistics/local-transportation/#local-transportation","text":"You are responsible for your own transportation within Madison, but we will help coordinate and can reimburse costs between the airport and your hotel.","title":"Local Transportation"},{"location":"logistics/local-transportation/#travel-between-the-madison-airport-and-your-hotel","text":"For travel between the Madison airport (Dane County Regional Airport) and the School hotel , the best option is the hotel shuttle service, when available. Otherwise, you may use a ride-sharing service or taxi, or take the Madison Metro Bus. See below for details. We will help organize groups to take shuttles and taxis, based on arrival and departure times. Shuttle/taxi groups will be formed and emailed shortly before the School itself.","title":"Travel Between the Madison Airport and Your Hotel"},{"location":"logistics/local-transportation/#travel-between-the-hotel-and-campus","text":"For travel between the School hotel and the Computer Sciences building on campus, walking is a great option. Also, the hotel shuttle service may be available, especially if organized in advance. See below for details.","title":"Travel Between the Hotel and Campus"},{"location":"logistics/local-transportation/#options-for-getting-around","text":"","title":"Options for Getting Around"},{"location":"logistics/local-transportation/#hotel-shuttle","text":"The Park Hotel operates a free shuttle service. The shuttle may not be available at all times, though, and it is best to plan ahead. Work with the hotel staff, individually or even better in groups, to use the shuttle. As noted above, we will help organize groups for the shuttle for airport arrivals on Sunday and departures on Saturday. To ask about the shuttle, either stop by the front desk of the hotel, or call +1 (608) 285-8000 and press 0 for the front desk. Explain that you are a guest at the hotel and ask if the shuttle is available for the number of people in your group; be clear about where you want to go from and to and at what time. We will send the hotel our list of groups who would like the shuttle for airport trips, but it is still best for the leader of each group to check with the hotel anyway.","title":"Hotel Shuttle"},{"location":"logistics/local-transportation/#walking","text":"It is easy to walk in and around the University of Wisconsin\u2013Madison campus, with many Madison landmarks within a mile of the School and your hotel. Use a mapping app or ask us or your hotel for a map. In particular, State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great restaurants and shops and is worth walking along while you are here.","title":"Walking"},{"location":"logistics/local-transportation/#city-of-madison-metro-bus-service","text":"Many Madison Metro buses stop near the hotel and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00, and if using a transfer ask the driver for a free transfer pass upon boarding. You can pay with cash, or, if you prefer, you can download the Madison Metro Transit Fast Fares mobile app, which allows you to add funds and scan a QR code to board the bus. This route does require you to make an account. Google Maps is a great resource for finding the best bus routes to use in Madison, giving multiple route options for each trip. Additionally, the Madison Metro Website provides a web interface to plan your trip. Note Bus routes stop running around ~11pm each day.","title":"City of Madison Metro Bus Service"},{"location":"logistics/local-transportation/#taxis-and-ride-sharing-services","text":"Both Lyft and Uber are active in Madison, or you can choose from our local taxi companies, such as Madison Taxi and Union Cab . We cannot recommend any particular option, but those are some options we know about. Note We cannot reimburse for any taxi or rideshare service beyond the ride to and from the airport. Note We will need receipts for any ride-share or taxi fare over $25.","title":"Taxis and Ride-Sharing Services"},{"location":"logistics/local-transportation/#madison-bcycle","text":"Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle . Bcycles are available throughout the city , including near the hotel and around campus. Pricing for Bcycles can be found on their website and consist of several tiers. Note Unfortunately, we are not able to reimburse BCycle costs.","title":"Madison BCycle"},{"location":"logistics/location/","text":"Location and transportation \u00b6 The School is held at the University of Wisconsin\u2013Madison in the Computer Sciences Building , located at 1210 West Dayton Street, Madison, WI, 53706 . The main classroom is Room 1240. Jump to local transportation below for suggestions about getting around Madison. Computer Sciences Building, Room 1240 \u00b6 Most School sessions are held in Room 1240 . If you enter the building from Dayton Street: Enter straight into the building from the street Immediately turn left and go through two sets of doors Pass the elevator (on your right) and walk down the hallway Room 1240 is on your right up the few steps There are signs around the building directing you to room 1240. Restrooms \u00b6 Restrooms are across the hallway and a bit to the right of 1240. Gender-neutral bathrooms are on office floors upstairs in a separate wing: Rooms 4329, 5329, 6329, 7329. Labeled as \"toilet\". For those, or other options, just ask staff! Local Transportation \u00b6 You are responsible for your own transportation within Madison, but we will help coordinate and can reimburse costs between the airport and your hotel. Travel Between the Madison Airport and Your Hotel \u00b6 The best option for travel between the Madison airport (Dane County Regional Airport) and the School hotel is the hotel shuttle service. Otherwise, you may use a ride-sharing service or taxi. We will help organize groups to take shuttles and taxis, based on arrival and departure times. Shuttle/taxi groups will be formed and emailed shortly before the School itself. Travel Between the Hotel and Campus \u00b6 The two options we recommend are: Walking. The hotel is 1.3 miles away from the School. Weather is generally great during summer months, and the route is pedestrian-friendly. Hotel shuttle service may be available, especially if organized in advance. Options for Getting Around \u00b6 Hotel Shuttle \u00b6 The Park Hotel operates a free shuttle service. The shuttle may not be available at all times, though, and it is best to plan ahead. Work with the hotel staff, individually or even better in groups, to use the shuttle. We will help organize groups for the shuttle for airport arrivals on Sunday and departures on Saturday. To ask about the shuttle: Stop by the front desk of the hotel Call +1 (608) 285-8000 and press 0 for the front desk. Explain that you are a guest at the hotel and ask if the shuttle is available for the number of people in your group; be clear about where you want to go from and to and at what time. We will send the hotel our list of groups who would like the shuttle for airport trips, but it is still best for the leader of each group to check with the hotel anyway. Walking \u00b6 It is easy to walk in and around the University of Wisconsin\u2013Madison campus, with many Madison landmarks within a mile of the School and your hotel. Use an app or ask us or your hotel for a map. State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great restaurants, shops, and attractions, and is worth walking along while you are here. City of Madison Metro Bus Service \u00b6 Many Madison Metro buses stop near the hotel and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00. There are several ways to pay. On any bus, you can use the Madison Metro app. On regular routes, you can pay with cash. On the rapid routes (A, B, F), you usually need to buy a ticket from the ticket machine at the bus station. To set up the app: Go to this page to download the app: https://madisonwi.justride.tickets/ Once the app is downloaded, create an account and log in. Add funds to the \"Tap & Ride Balance.\" Create a barcode: From the first screen, click on \"Tap and Ride\" Scroll down to the \"Your Active Tokens\" section Choose \"Create a barcode.\" Go through the steps to create a barcode. To use the app: When the bus is arriving, access your barcode (it should be on the first screen when you open the app.) Scan the barcode on the middle part of the bus scanner. See this video . If paying with cash and using a transfer, ask the driver for a free transfer pass upon boarding. There's a useful fare FAQ here . Apps \u00b6 Google Maps is a great resource for finding the best bus routes to use in Madison, giving multiple route options for each trip. Transit ( Android , Apple App Store ) is great app for finding and real-time tracking of buses. Additionally, the Madison Metro Website provides a web interface to plan your trip. Note Bus routes stop running around ~11pm each day. Taxis and Ride-Sharing Services \u00b6 We cannot recommend any particular option, but here are some options: Rideshare services active in Madison Lyft Uber Local taxi companies Madison Taxi Union Cab Note We cannot reimburse for any taxi or rideshare service beyond the ride to and from the airport. Note We will need receipts for any ride-share or taxi fare over $25. Madison BCycle \u00b6 Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle . Bcycles are available throughout the city , including near the hotel and around campus. The Bcycle app is required to use their service. Pricing for Bcycles can be found on their website and consist of several tiers. Note Unfortunately, we are not able to reimburse BCycle costs.","title":"Location and transportation"},{"location":"logistics/location/#location-and-transportation","text":"The School is held at the University of Wisconsin\u2013Madison in the Computer Sciences Building , located at 1210 West Dayton Street, Madison, WI, 53706 . The main classroom is Room 1240. Jump to local transportation below for suggestions about getting around Madison.","title":"Location and transportation"},{"location":"logistics/location/#computer-sciences-building-room-1240","text":"Most School sessions are held in Room 1240 . If you enter the building from Dayton Street: Enter straight into the building from the street Immediately turn left and go through two sets of doors Pass the elevator (on your right) and walk down the hallway Room 1240 is on your right up the few steps There are signs around the building directing you to room 1240.","title":"Computer Sciences Building, Room 1240"},{"location":"logistics/location/#restrooms","text":"Restrooms are across the hallway and a bit to the right of 1240. Gender-neutral bathrooms are on office floors upstairs in a separate wing: Rooms 4329, 5329, 6329, 7329. Labeled as \"toilet\". For those, or other options, just ask staff!","title":"Restrooms"},{"location":"logistics/location/#local-transportation","text":"You are responsible for your own transportation within Madison, but we will help coordinate and can reimburse costs between the airport and your hotel.","title":"Local Transportation"},{"location":"logistics/location/#travel-between-the-madison-airport-and-your-hotel","text":"The best option for travel between the Madison airport (Dane County Regional Airport) and the School hotel is the hotel shuttle service. Otherwise, you may use a ride-sharing service or taxi. We will help organize groups to take shuttles and taxis, based on arrival and departure times. Shuttle/taxi groups will be formed and emailed shortly before the School itself.","title":"Travel Between the Madison Airport and Your Hotel"},{"location":"logistics/location/#travel-between-the-hotel-and-campus","text":"The two options we recommend are: Walking. The hotel is 1.3 miles away from the School. Weather is generally great during summer months, and the route is pedestrian-friendly. Hotel shuttle service may be available, especially if organized in advance.","title":"Travel Between the Hotel and Campus"},{"location":"logistics/location/#options-for-getting-around","text":"","title":"Options for Getting Around"},{"location":"logistics/location/#hotel-shuttle","text":"The Park Hotel operates a free shuttle service. The shuttle may not be available at all times, though, and it is best to plan ahead. Work with the hotel staff, individually or even better in groups, to use the shuttle. We will help organize groups for the shuttle for airport arrivals on Sunday and departures on Saturday. To ask about the shuttle: Stop by the front desk of the hotel Call +1 (608) 285-8000 and press 0 for the front desk. Explain that you are a guest at the hotel and ask if the shuttle is available for the number of people in your group; be clear about where you want to go from and to and at what time. We will send the hotel our list of groups who would like the shuttle for airport trips, but it is still best for the leader of each group to check with the hotel anyway.","title":"Hotel Shuttle"},{"location":"logistics/location/#walking","text":"It is easy to walk in and around the University of Wisconsin\u2013Madison campus, with many Madison landmarks within a mile of the School and your hotel. Use an app or ask us or your hotel for a map. State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great restaurants, shops, and attractions, and is worth walking along while you are here.","title":"Walking"},{"location":"logistics/location/#city-of-madison-metro-bus-service","text":"Many Madison Metro buses stop near the hotel and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00. There are several ways to pay. On any bus, you can use the Madison Metro app. On regular routes, you can pay with cash. On the rapid routes (A, B, F), you usually need to buy a ticket from the ticket machine at the bus station. To set up the app: Go to this page to download the app: https://madisonwi.justride.tickets/ Once the app is downloaded, create an account and log in. Add funds to the \"Tap & Ride Balance.\" Create a barcode: From the first screen, click on \"Tap and Ride\" Scroll down to the \"Your Active Tokens\" section Choose \"Create a barcode.\" Go through the steps to create a barcode. To use the app: When the bus is arriving, access your barcode (it should be on the first screen when you open the app.) Scan the barcode on the middle part of the bus scanner. See this video . If paying with cash and using a transfer, ask the driver for a free transfer pass upon boarding. There's a useful fare FAQ here .","title":"City of Madison Metro Bus Service"},{"location":"logistics/location/#apps","text":"Google Maps is a great resource for finding the best bus routes to use in Madison, giving multiple route options for each trip. Transit ( Android , Apple App Store ) is great app for finding and real-time tracking of buses. Additionally, the Madison Metro Website provides a web interface to plan your trip. Note Bus routes stop running around ~11pm each day.","title":"Apps"},{"location":"logistics/location/#taxis-and-ride-sharing-services","text":"We cannot recommend any particular option, but here are some options: Rideshare services active in Madison Lyft Uber Local taxi companies Madison Taxi Union Cab Note We cannot reimburse for any taxi or rideshare service beyond the ride to and from the airport. Note We will need receipts for any ride-share or taxi fare over $25.","title":"Taxis and Ride-Sharing Services"},{"location":"logistics/location/#madison-bcycle","text":"Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle . Bcycles are available throughout the city , including near the hotel and around campus. The Bcycle app is required to use their service. Pricing for Bcycles can be found on their website and consist of several tiers. Note Unfortunately, we are not able to reimburse BCycle costs.","title":"Madison BCycle"},{"location":"logistics/meals/","text":"Meal Information \u00b6 The School includes some group catered meals for all participants: Sunday (Jun. 22) \u2014 welcome dinner Monday (Jun. 23) \u2013 Friday (Jun. 27) \u2014 breakfast and lunch each day Friday (Jun. 27) \u2014 closing dinner Other meals not listed above are on your own. If you are not a member of the UW\u2013Madison community, we will reimburse you for the on-your-own meals, Monday through Thursday dinners; see below for details. Sorry, UW\u2013Madison folks: The rules say that we cannot reimburse you for meals here. For the meals on your own, you are welcome to join other participants and even staff! We can help with ideas and groups, if you like. There is another page with suggestions for finding dining options near the School and hotel. Catered Meals \u00b6 The catered breakfasts and lunches during the School (see above) will be served in the Computer Sciences Building. Breakfast is in the main auditorium, room 1240 , and lunch is nearby (staff will lead the way on Monday). There is nearby seating both inside and outside. Menus \u00b6 The catered meals should take into account all dietary needs that you told us about in the questionnaire. Check for labels! If you have questions, ask the catering staff (if present) or School staff. Some items, like gluten-free items, are provided in low quantities that are meant just for those people who requested them. Please do not take them unless they are for you. If you have dietary restrictions and have questions about the menu, please feel free to ask staff! Sunday, June 22, 2025 \u00b6 Opening Dinner (6:30 PM - 8:30 PM) \u00b6 Location: Union South - 1308 W. Dayton St. Global Buffet Vegetables, Dips, Spreads, and Pita Chips Sesame, Strawberry Spinach Salad Sake Salmon Chicken Tikka Masala Jerk Fried Tofu, Chimichurri Basmati Rice Naan Monday, June 23, 2025 \u00b6 Breakfast (8:00 AM - 9:00 AM) \u00b6 Badger Breakfast Coffee Cake Plain Scrambled Eggs Hash Browns Fresh Fruit Salad Granola Bars Coffee Hot Tea Bottled Juice Lunch (12:15 PM - 1:15 PM) \u00b6 Linden Citrus Seasoned Chicken Asada Mixed Greens Avocado Ranch Flour Tortillas Brazilian Black Beans Cilantro Lime Rice Shredded Cheddar Shredded Lettuce Pico de Gallo, Salsa Verde, and Sour Cream Corn Tortilla Chips Cookies PM Break (12:30 PM - 4:00 PM) \u00b6 Regular Coffee Tuesday, June 24, 2025 \u00b6 Breakfast (8:00 AM - 9:00 AM) \u00b6 Griddle Bar Blueberry Pancakes Bacon Chicken Sausage Fruit Salad Maple Syrup and Butter Assorted Egg Bites Greek Yogurt Granola Bars Coffee Lunch (12:15 PM - 1:15 PM) \u00b6 Breese Mixed Greens, Balsamic, and Ranch Halal Chicken Shawarma Saffron Rice Falafel Naan Pickled Vegetables, Tzatziki, Tahini Sauce Tomato Basil Bisque Curry Tomato Lentil Soup Cookies Assorted Cold Beverages PM Break (12:30 PM - 4:00 PM) \u00b6 Regular Coffee Wednesday, June 25, 2025 \u00b6 Breakfast (8:00 AM - 9:00 AM) \u00b6 Badger Breakfast Bagels Cream Cheese Plain Scrambled Eggs Baby Red Potatoes Assorted Greek Yogurts Granola Bars Fruit Salad Coffee Tea Bottled Juices Lunch (12:30 PM - 1:45 PM) \u00b6 Boxed Lunches Chicken, White Cheddar, Garlic Aioli, Cranberry Wild Rice Bread Chimichurri Tofu, Mixed Greens, Cuban Bread Italian Meats and Cheeses on Hoagie Red Potato Salad, Fresh Herbs, Vinaigrette Grilled Vegetable Pasta Salad Kettle Chips Cookies Cold Beverages Thursday, August June 26, 2025 \u00b6 Breakfast (8:00 AM - 9:00 AM) \u00b6 Griddle Bar French Toast Bacon Chicken Sausage Hash Browns Assorted Egg Bites Yogurt Granola Bars Coffee Tea Lunch (12:00 PM - 1:15 PM) \u00b6 Frances Mixed Greens, Balsamic, and Ranch Cucumber Salad Vegetarian Potstickers Mandarin Chicken Chimichurri Tofu Garlic Green Beans White Rice Cookies Cold Beverages PM Break (12:30 PM - 4:30 PM) \u00b6 Coffee Pita Chips, Hummus, Vegetables with Dill Dip Soda Bottled Water Friday, June 27, 2025 \u00b6 Breakfast (8:00 AM - 9:00 AM) \u00b6 Badger Breakfast Plain Scrambled Eggs Hash Browns Donuts Granola Bars Fruit Salad Coffee Tea Bottled Juice Lunch (12:00 PM - 1:00 PM) \u00b6 Linden Burgers Beyond Brats Mixed Greens, Balsamic, and Ranch Grilled Vegetable Pasta Salad Fruit Salad Seasoned House Fried Chips Buns and Condiments Cookies Assorted Cold Beverages PM Break (12:30 PM - 4:00 PM) \u00b6 Coffee Closing Dinner (6:30 PM - 8:00 PM) \u00b6 Location: Fluno Center, 601 University Avenue; Skyview Room, 8th Floor Pastaciutta Buffeteria Cavatappi Pasta Cheese Lasagna Grilled Chicken Breast Meatballs Italian Vegetable Blend Breadsticks Marinara Sauce and Alfredo Sauce Caesar Salad Tiramisu Cannoli Meal Reimbursement Tips \u00b6 Again, if you are not part of the UW\u2013Madison community, we can reimburse you for dinners Monday through Thursday. We have curated a page of some possible dining options to use as inspiration. Some tips for successful reimbursements: Keep receipts for your meals \u2013 if anything so that you remember how much meals cost! We can reimburse up to $35 for dinner, including tax and tip. If it is not on the receipt, be sure to write the tip amount yourself, so you do not forget. We cannot pay for any alcohol, although non-alcoholic drinks are OK \u2014 ideally, pay for alcohol separately. We will explain the reimbursement process in detail after the School, but the tips above will help.","title":"Meal information"},{"location":"logistics/meals/#meal-information","text":"The School includes some group catered meals for all participants: Sunday (Jun. 22) \u2014 welcome dinner Monday (Jun. 23) \u2013 Friday (Jun. 27) \u2014 breakfast and lunch each day Friday (Jun. 27) \u2014 closing dinner Other meals not listed above are on your own. If you are not a member of the UW\u2013Madison community, we will reimburse you for the on-your-own meals, Monday through Thursday dinners; see below for details. Sorry, UW\u2013Madison folks: The rules say that we cannot reimburse you for meals here. For the meals on your own, you are welcome to join other participants and even staff! We can help with ideas and groups, if you like. There is another page with suggestions for finding dining options near the School and hotel.","title":"Meal Information"},{"location":"logistics/meals/#catered-meals","text":"The catered breakfasts and lunches during the School (see above) will be served in the Computer Sciences Building. Breakfast is in the main auditorium, room 1240 , and lunch is nearby (staff will lead the way on Monday). There is nearby seating both inside and outside.","title":"Catered Meals"},{"location":"logistics/meals/#menus","text":"The catered meals should take into account all dietary needs that you told us about in the questionnaire. Check for labels! If you have questions, ask the catering staff (if present) or School staff. Some items, like gluten-free items, are provided in low quantities that are meant just for those people who requested them. Please do not take them unless they are for you. If you have dietary restrictions and have questions about the menu, please feel free to ask staff!","title":"Menus"},{"location":"logistics/meals/#sunday-june-22-2025","text":"","title":"Sunday, June 22, 2025"},{"location":"logistics/meals/#opening-dinner-630-pm-830-pm","text":"Location: Union South - 1308 W. Dayton St. Global Buffet Vegetables, Dips, Spreads, and Pita Chips Sesame, Strawberry Spinach Salad Sake Salmon Chicken Tikka Masala Jerk Fried Tofu, Chimichurri Basmati Rice Naan","title":"Opening Dinner (6:30 PM - 8:30 PM)"},{"location":"logistics/meals/#monday-june-23-2025","text":"","title":"Monday, June 23, 2025"},{"location":"logistics/meals/#breakfast-800-am-900-am","text":"Badger Breakfast Coffee Cake Plain Scrambled Eggs Hash Browns Fresh Fruit Salad Granola Bars Coffee Hot Tea Bottled Juice","title":"Breakfast (8:00 AM - 9:00 AM)"},{"location":"logistics/meals/#lunch-1215-pm-115-pm","text":"Linden Citrus Seasoned Chicken Asada Mixed Greens Avocado Ranch Flour Tortillas Brazilian Black Beans Cilantro Lime Rice Shredded Cheddar Shredded Lettuce Pico de Gallo, Salsa Verde, and Sour Cream Corn Tortilla Chips Cookies","title":"Lunch (12:15 PM - 1:15 PM)"},{"location":"logistics/meals/#pm-break-1230-pm-400-pm","text":"Regular Coffee","title":"PM Break (12:30 PM - 4:00 PM)"},{"location":"logistics/meals/#tuesday-june-24-2025","text":"","title":"Tuesday, June 24, 2025"},{"location":"logistics/meals/#breakfast-800-am-900-am_1","text":"Griddle Bar Blueberry Pancakes Bacon Chicken Sausage Fruit Salad Maple Syrup and Butter Assorted Egg Bites Greek Yogurt Granola Bars Coffee","title":"Breakfast (8:00 AM - 9:00 AM)"},{"location":"logistics/meals/#lunch-1215-pm-115-pm_1","text":"Breese Mixed Greens, Balsamic, and Ranch Halal Chicken Shawarma Saffron Rice Falafel Naan Pickled Vegetables, Tzatziki, Tahini Sauce Tomato Basil Bisque Curry Tomato Lentil Soup Cookies Assorted Cold Beverages","title":"Lunch (12:15 PM - 1:15 PM)"},{"location":"logistics/meals/#pm-break-1230-pm-400-pm_1","text":"Regular Coffee","title":"PM Break (12:30 PM - 4:00 PM)"},{"location":"logistics/meals/#wednesday-june-25-2025","text":"","title":"Wednesday, June 25, 2025"},{"location":"logistics/meals/#breakfast-800-am-900-am_2","text":"Badger Breakfast Bagels Cream Cheese Plain Scrambled Eggs Baby Red Potatoes Assorted Greek Yogurts Granola Bars Fruit Salad Coffee Tea Bottled Juices","title":"Breakfast (8:00 AM - 9:00 AM)"},{"location":"logistics/meals/#lunch-1230-pm-145-pm","text":"Boxed Lunches Chicken, White Cheddar, Garlic Aioli, Cranberry Wild Rice Bread Chimichurri Tofu, Mixed Greens, Cuban Bread Italian Meats and Cheeses on Hoagie Red Potato Salad, Fresh Herbs, Vinaigrette Grilled Vegetable Pasta Salad Kettle Chips Cookies Cold Beverages","title":"Lunch (12:30 PM - 1:45 PM)"},{"location":"logistics/meals/#thursday-august-june-26-2025","text":"","title":"Thursday, August June 26, 2025"},{"location":"logistics/meals/#breakfast-800-am-900-am_3","text":"Griddle Bar French Toast Bacon Chicken Sausage Hash Browns Assorted Egg Bites Yogurt Granola Bars Coffee Tea","title":"Breakfast (8:00 AM - 9:00 AM)"},{"location":"logistics/meals/#lunch-1200-pm-115-pm","text":"Frances Mixed Greens, Balsamic, and Ranch Cucumber Salad Vegetarian Potstickers Mandarin Chicken Chimichurri Tofu Garlic Green Beans White Rice Cookies Cold Beverages","title":"Lunch (12:00 PM - 1:15 PM)"},{"location":"logistics/meals/#pm-break-1230-pm-430-pm","text":"Coffee Pita Chips, Hummus, Vegetables with Dill Dip Soda Bottled Water","title":"PM Break (12:30 PM - 4:30 PM)"},{"location":"logistics/meals/#friday-june-27-2025","text":"","title":"Friday, June 27, 2025"},{"location":"logistics/meals/#breakfast-800-am-900-am_4","text":"Badger Breakfast Plain Scrambled Eggs Hash Browns Donuts Granola Bars Fruit Salad Coffee Tea Bottled Juice","title":"Breakfast (8:00 AM - 9:00 AM)"},{"location":"logistics/meals/#lunch-1200-pm-100-pm","text":"Linden Burgers Beyond Brats Mixed Greens, Balsamic, and Ranch Grilled Vegetable Pasta Salad Fruit Salad Seasoned House Fried Chips Buns and Condiments Cookies Assorted Cold Beverages","title":"Lunch (12:00 PM - 1:00 PM)"},{"location":"logistics/meals/#pm-break-1230-pm-400-pm_2","text":"Coffee","title":"PM Break (12:30 PM - 4:00 PM)"},{"location":"logistics/meals/#closing-dinner-630-pm-800-pm","text":"Location: Fluno Center, 601 University Avenue; Skyview Room, 8th Floor Pastaciutta Buffeteria Cavatappi Pasta Cheese Lasagna Grilled Chicken Breast Meatballs Italian Vegetable Blend Breadsticks Marinara Sauce and Alfredo Sauce Caesar Salad Tiramisu Cannoli","title":"Closing Dinner (6:30 PM - 8:00 PM)"},{"location":"logistics/meals/#meal-reimbursement-tips","text":"Again, if you are not part of the UW\u2013Madison community, we can reimburse you for dinners Monday through Thursday. We have curated a page of some possible dining options to use as inspiration. Some tips for successful reimbursements: Keep receipts for your meals \u2013 if anything so that you remember how much meals cost! We can reimburse up to $35 for dinner, including tax and tip. If it is not on the receipt, be sure to write the tip amount yourself, so you do not forget. We cannot pay for any alcohol, although non-alcoholic drinks are OK \u2014 ideally, pay for alcohol separately. We will explain the reimbursement process in detail after the School, but the tips above will help.","title":"Meal Reimbursement Tips"},{"location":"logistics/projects/","text":"Continue Your Access to the OSPool \u00b6 Any U.S.-based researcher with an academic affiliation can use the OSPool indefinitely, with no allocation or cost to you. Your access to the OSPool is temporary\u2014if you'd like to continue your access to the OSPool, follow the instructions below! Option 1: Schedule a consultation at the School (preferred) \u00b6 To continue using your account beyond the School, please schedule a consultation with one of the facilitators during the School! Information on how to schedule a consultation will be provided at the School or via Slack. In this consultation, we'll chat with you one-on-one about your research and help set up a project to ensure you can use the OSPool for your research computing needs! Option 2: Submit an application after the School \u00b6 If you are unable to schedule a consultation during the School or decide to continue your access at a later date, complete the following steps. Please complete the following steps by July 31: Go to this link: Project Creation Form Fill out the form. Tips for Filling Out the Form If the Project Name doesn't auto-populate, use OSG_School_2025 . For your project description, please include a summary of what your lab works on. Include what the overarching research question is. For your Field of Science ID and Field of Science (Legacy) responses, please choose the entry that best fits your lab's research area. After completing the form, click \"Submit manually\": Send an email to support@osg-htc.org with the following: Include the text of the project creation form (see image below). Please email the YAML Output to the OSPool Team You do not need to go to GitHub and upload these yourself, the team will do this for you. Please email the outputted YAML file to support@osg-htc.org with the subject: OSG School 2025 - Account Conversion - <Your First and Last Name> . CC the PI of your research group, asking them to confirm your membership. in their group. If you are the PI, that should be indicated in the email. If you are not the PI of your research group, make sure your PI replies to your email to confirm that you are working with them!! Once the project PI has confirmed your membership, OSG Facilitators will create a new project of the form Institution_PIName . You will receive a notification when you are added to the new project, and a follow up email from the OSG facilitation team.","title":"Long-term OSPool access"},{"location":"logistics/projects/#continue-your-access-to-the-ospool","text":"Any U.S.-based researcher with an academic affiliation can use the OSPool indefinitely, with no allocation or cost to you. Your access to the OSPool is temporary\u2014if you'd like to continue your access to the OSPool, follow the instructions below!","title":"Continue Your Access to the OSPool"},{"location":"logistics/projects/#option-1-schedule-a-consultation-at-the-school-preferred","text":"To continue using your account beyond the School, please schedule a consultation with one of the facilitators during the School! Information on how to schedule a consultation will be provided at the School or via Slack. In this consultation, we'll chat with you one-on-one about your research and help set up a project to ensure you can use the OSPool for your research computing needs!","title":"Option 1: Schedule a consultation at the School (preferred)"},{"location":"logistics/projects/#option-2-submit-an-application-after-the-school","text":"If you are unable to schedule a consultation during the School or decide to continue your access at a later date, complete the following steps. Please complete the following steps by July 31: Go to this link: Project Creation Form Fill out the form. Tips for Filling Out the Form If the Project Name doesn't auto-populate, use OSG_School_2025 . For your project description, please include a summary of what your lab works on. Include what the overarching research question is. For your Field of Science ID and Field of Science (Legacy) responses, please choose the entry that best fits your lab's research area. After completing the form, click \"Submit manually\": Send an email to support@osg-htc.org with the following: Include the text of the project creation form (see image below). Please email the YAML Output to the OSPool Team You do not need to go to GitHub and upload these yourself, the team will do this for you. Please email the outputted YAML file to support@osg-htc.org with the subject: OSG School 2025 - Account Conversion - <Your First and Last Name> . CC the PI of your research group, asking them to confirm your membership. in their group. If you are the PI, that should be indicated in the email. If you are not the PI of your research group, make sure your PI replies to your email to confirm that you are working with them!! Once the project PI has confirmed your membership, OSG Facilitators will create a new project of the form Institution_PIName . You will receive a notification when you are added to the new project, and a follow up email from the OSG facilitation team.","title":"Option 2: Submit an application after the School"},{"location":"logistics/reimbursements/","text":"Getting Reimbursed for Expenses \u00b6 If you paid for certain expenses to attend the OSG School 2025, then we will reimburse you for them. This page explains what can be reimbursed and how to request reimbursement. Please read this whole page carefully before submitting your request. The deadline for all reimbursement submissions is Friday, 8 August 2025 . What Can Be Reimbursed? \u00b6 We offered to reimburse certain expenses that were part of attending the OSG School: Bus fare to/from Madison Taxi or ride-share to/from Madison airport Driving mileage (if prearranged) Dinners Monday through Thursday ($35 per dinner) Generally, we are not able to reimburse you for any other expenses (e.g., checked baggage fees, transportation to/from your home and local airport). However, if we made specific arrangements with you before the School, those arrangements are still valid for reimbursement. When in doubt, contact us well before the deadline. What Information Is Needed? \u00b6 The University of Wisconsin\u2013Madison has strict requirements about reimbursements. See below for each type of expense and its rules. Expense Rules Bus fare For bus trips between Madison and a nearby airport, as planned with us in advance. The original receipt is required, including at least the total amount paid. Ride fare For taxi and ride-sharing trips between the Madison airport and your hotel. Rides in your home town are not covered. If you paid for other School participants, list their names. If the total (fare + tip) is more than $25, the original receipt is required, ideally including: Date of trip Fare and tip Driving mileage If we agreed in advance to cover your driving costs to/from the School, simply list the agreed-upon reimbursement amount ($ or miles) as a reminder. Meals For dinners Monday through Thursday of the School, as noted on the meals page , we will reimburse you $35 per dinner. If you were gone for any evening(s), please remind us; otherwise simply write that you had dinner all four evenings. For other expense types that were arranged before the School, contact us to learn what documentation is required. Note: If you must give us receipts (see above), electronic files are fine. You may scan or photograph any paper receipts and email them to us. How to Request Reimbursement \u00b6 Please follow these instructions carefully for fastest processing of your reimbursement request. Gather your reimbursement information (see above for what you need). Email your reimbursement request by Friday, 8 August 2025. Write a clear email with your name, the exact expenses and amounts to be reimbursed, reimbursement total, and any scanned receipts or other documentation. Send the email to school@osg-htc.org . Use the sample format below to get started, but update the subtotals and remove the sections that do not apply to you : I attended the OSG School 2025 in Madison, Wisconsin, and I request reimbursement for the following School-related expenses: STUDENT INFORMATION Name: Cartwright, Tim SSN: 1234 [LAST FOUR DIGITS ONLY] Mail: 1210 W Dayton St, Madison, WI 53706 [see below] Email: cat@cs.wisc.edu Phone: +1 (608) 262-4002 EXPENSES Bus Fare => $55.55 * Roundtrip on Van Galder bus between Chicago O\u2019Hare and UW-Madison * Original electronic receipt is attached Taxi Fare => $33.33 * One-way from Madison airport to the Park Hotel * Other OSG School passengers: Miron Livny, Christina Koch * Scanned receipt is attached Driving Mileage => $222.22 [if arranged in advance] * Roundtrip between home and Park Hotel in Madison * Estimated total miles (from Google Maps): 432 Dinners => $140 * I was at the School for all four evenings (Mon-Thu) TOTAL => $123.45 For receipts, just attach the electronic files (PDF) to your request email; or, email separately to school@osg-htc.org . All receipts must be emailed by Friday, 8 August 2025, or the corresponding expenses will not be reimbursed. OSG School staff will review your request and either approve it or send it back for changes. Once approved, UW\u2013Madison administrative staff will process your reimbursement request. Your reimbursement check will be mailed to the postal address that you provided, so choose a reliable one (home, office, other). Note that it may take months after you submit before your check is mailed, so plan ahead. When you receive your check, please email school@osg-htc.org to say that the check arrived and the exact amount of the check (dollars and cents). Maybe surprisingly, this is the only way that we can make sure that you got the reimbursement and that the amount was correct.","title":"Reimbursements (Aug 8)"},{"location":"logistics/reimbursements/#getting-reimbursed-for-expenses","text":"If you paid for certain expenses to attend the OSG School 2025, then we will reimburse you for them. This page explains what can be reimbursed and how to request reimbursement. Please read this whole page carefully before submitting your request. The deadline for all reimbursement submissions is Friday, 8 August 2025 .","title":"Getting Reimbursed for Expenses"},{"location":"logistics/reimbursements/#what-can-be-reimbursed","text":"We offered to reimburse certain expenses that were part of attending the OSG School: Bus fare to/from Madison Taxi or ride-share to/from Madison airport Driving mileage (if prearranged) Dinners Monday through Thursday ($35 per dinner) Generally, we are not able to reimburse you for any other expenses (e.g., checked baggage fees, transportation to/from your home and local airport). However, if we made specific arrangements with you before the School, those arrangements are still valid for reimbursement. When in doubt, contact us well before the deadline.","title":"What Can Be Reimbursed?"},{"location":"logistics/reimbursements/#what-information-is-needed","text":"The University of Wisconsin\u2013Madison has strict requirements about reimbursements. See below for each type of expense and its rules. Expense Rules Bus fare For bus trips between Madison and a nearby airport, as planned with us in advance. The original receipt is required, including at least the total amount paid. Ride fare For taxi and ride-sharing trips between the Madison airport and your hotel. Rides in your home town are not covered. If you paid for other School participants, list their names. If the total (fare + tip) is more than $25, the original receipt is required, ideally including: Date of trip Fare and tip Driving mileage If we agreed in advance to cover your driving costs to/from the School, simply list the agreed-upon reimbursement amount ($ or miles) as a reminder. Meals For dinners Monday through Thursday of the School, as noted on the meals page , we will reimburse you $35 per dinner. If you were gone for any evening(s), please remind us; otherwise simply write that you had dinner all four evenings. For other expense types that were arranged before the School, contact us to learn what documentation is required. Note: If you must give us receipts (see above), electronic files are fine. You may scan or photograph any paper receipts and email them to us.","title":"What Information Is Needed?"},{"location":"logistics/reimbursements/#how-to-request-reimbursement","text":"Please follow these instructions carefully for fastest processing of your reimbursement request. Gather your reimbursement information (see above for what you need). Email your reimbursement request by Friday, 8 August 2025. Write a clear email with your name, the exact expenses and amounts to be reimbursed, reimbursement total, and any scanned receipts or other documentation. Send the email to school@osg-htc.org . Use the sample format below to get started, but update the subtotals and remove the sections that do not apply to you : I attended the OSG School 2025 in Madison, Wisconsin, and I request reimbursement for the following School-related expenses: STUDENT INFORMATION Name: Cartwright, Tim SSN: 1234 [LAST FOUR DIGITS ONLY] Mail: 1210 W Dayton St, Madison, WI 53706 [see below] Email: cat@cs.wisc.edu Phone: +1 (608) 262-4002 EXPENSES Bus Fare => $55.55 * Roundtrip on Van Galder bus between Chicago O\u2019Hare and UW-Madison * Original electronic receipt is attached Taxi Fare => $33.33 * One-way from Madison airport to the Park Hotel * Other OSG School passengers: Miron Livny, Christina Koch * Scanned receipt is attached Driving Mileage => $222.22 [if arranged in advance] * Roundtrip between home and Park Hotel in Madison * Estimated total miles (from Google Maps): 432 Dinners => $140 * I was at the School for all four evenings (Mon-Thu) TOTAL => $123.45 For receipts, just attach the electronic files (PDF) to your request email; or, email separately to school@osg-htc.org . All receipts must be emailed by Friday, 8 August 2025, or the corresponding expenses will not be reimbursed. OSG School staff will review your request and either approve it or send it back for changes. Once approved, UW\u2013Madison administrative staff will process your reimbursement request. Your reimbursement check will be mailed to the postal address that you provided, so choose a reliable one (home, office, other). Note that it may take months after you submit before your check is mailed, so plan ahead. When you receive your check, please email school@osg-htc.org to say that the check arrived and the exact amount of the check (dollars and cents). Maybe surprisingly, this is the only way that we can make sure that you got the reimbursement and that the amount was correct.","title":"How to Request Reimbursement"},{"location":"logistics/travel-advice/","text":"Travel Advice \u00b6 This page offers some tips for traveling to and from the OSG School. When travelling, you may experience delays, changes, or cancellations due to weather, mechanical issues, and so on. It is good to be prepared for last-minute changes. Below are some tips and ideas for dealing with travel. For health guidelines, before or during the event, please see our health guidelines page . Checking In Early \u00b6 Airlines generally allow you to check in for your flights the day before. Doing so may save you time and hassle at the airport. Go to your airline website and look for the \u201cCheck In\u201d section, then follow the steps. Finding Flight Status \u00b6 Be sure to check your flight status often, starting the day before travel begins. While you can check the status of each flight individually on the airline website (or a third-party site), you may be able to view your entire trip at once. Go to your airline website, find their section for \u201cMy Trips\u201d or something similar, and use the six-character \u201cConfirmation Number\u201d on your itinerary plus your last name to access your full itinerary, including flight status for each segment. Definitely check your flight status before leaving for the airport! If Your Arrival in Madison Is Delayed \u00b6 If your flights change and you will arrive in Madison later than planned, think about what effect that will have: If you will arrive before Sunday, 6 p.m. (or so), you should be fine. If there is time, you can still go to the hotel first; if it is after 5:30 p.m. (or so), it may be best to go straight to Union South for the welcome dinner at 6:30 p.m. If you will arrive on Sunday but after 6 p.m. (or so), you will miss the welcome dinner. Go straight to the hotel and check in, then find dinner on your own; we can reimburse you in this case. Try to let us know about the situation, when you can. If you will arrive later than Sunday, just do your best to get here. Try to let us know about your situation as soon as you can. We can help deal things like the hotel and may be able to suggest travel options. If you need to make flight changes, see below. If Your Arrival Back Home Is Delayed \u00b6 If you flights back home are delayed, there is not as much that we can do. For example, it is not clear whether we can pay for changes on return flights. Contact your airline to find out how they will get you home. If You Must Make Flight Changes \u00b6 If one or more flights are cancelled, or if we approve flight changes and their fees in advance , you will need to make new plans with your airline. If you are at an airport, it is a good idea to get in line at your airline\u2019s service counter right away. Also, you can try calling their service number while waiting in line! For any change that requires extra payment, you must get our approval and make the change through Fox World Travel , UW\u2013Madison\u2019s only approved travel agency. If you pay for a change any other way, we cannot reimburse you. Fox World Travel phone number: +1 (844) 630-3853 Note: If you call Fox World Travel on the weekend or outside of 7am\u20137:30pm (Central), they will charge us $20 just for calling. So please use this option only when you must pay for approved flight changes. If there are significant changes to your travel plans, when you have time, please email us with your news or reach out to us on Slack.","title":"Travel advice"},{"location":"logistics/travel-advice/#travel-advice","text":"This page offers some tips for traveling to and from the OSG School. When travelling, you may experience delays, changes, or cancellations due to weather, mechanical issues, and so on. It is good to be prepared for last-minute changes. Below are some tips and ideas for dealing with travel. For health guidelines, before or during the event, please see our health guidelines page .","title":"Travel Advice"},{"location":"logistics/travel-advice/#checking-in-early","text":"Airlines generally allow you to check in for your flights the day before. Doing so may save you time and hassle at the airport. Go to your airline website and look for the \u201cCheck In\u201d section, then follow the steps.","title":"Checking In Early"},{"location":"logistics/travel-advice/#finding-flight-status","text":"Be sure to check your flight status often, starting the day before travel begins. While you can check the status of each flight individually on the airline website (or a third-party site), you may be able to view your entire trip at once. Go to your airline website, find their section for \u201cMy Trips\u201d or something similar, and use the six-character \u201cConfirmation Number\u201d on your itinerary plus your last name to access your full itinerary, including flight status for each segment. Definitely check your flight status before leaving for the airport!","title":"Finding Flight Status"},{"location":"logistics/travel-advice/#if-your-arrival-in-madison-is-delayed","text":"If your flights change and you will arrive in Madison later than planned, think about what effect that will have: If you will arrive before Sunday, 6 p.m. (or so), you should be fine. If there is time, you can still go to the hotel first; if it is after 5:30 p.m. (or so), it may be best to go straight to Union South for the welcome dinner at 6:30 p.m. If you will arrive on Sunday but after 6 p.m. (or so), you will miss the welcome dinner. Go straight to the hotel and check in, then find dinner on your own; we can reimburse you in this case. Try to let us know about the situation, when you can. If you will arrive later than Sunday, just do your best to get here. Try to let us know about your situation as soon as you can. We can help deal things like the hotel and may be able to suggest travel options. If you need to make flight changes, see below.","title":"If Your Arrival in Madison Is Delayed"},{"location":"logistics/travel-advice/#if-your-arrival-back-home-is-delayed","text":"If you flights back home are delayed, there is not as much that we can do. For example, it is not clear whether we can pay for changes on return flights. Contact your airline to find out how they will get you home.","title":"If Your Arrival Back Home Is Delayed"},{"location":"logistics/travel-advice/#if-you-must-make-flight-changes","text":"If one or more flights are cancelled, or if we approve flight changes and their fees in advance , you will need to make new plans with your airline. If you are at an airport, it is a good idea to get in line at your airline\u2019s service counter right away. Also, you can try calling their service number while waiting in line! For any change that requires extra payment, you must get our approval and make the change through Fox World Travel , UW\u2013Madison\u2019s only approved travel agency. If you pay for a change any other way, we cannot reimburse you. Fox World Travel phone number: +1 (844) 630-3853 Note: If you call Fox World Travel on the weekend or outside of 7am\u20137:30pm (Central), they will charge us $20 just for calling. So please use this option only when you must pay for approved flight changes. If there are significant changes to your travel plans, when you have time, please email us with your news or reach out to us on Slack.","title":"If You Must Make Flight Changes"},{"location":"logistics/travel-planning/","text":"Travel To and From Madison \u00b6 Please wait to begin making travel arrangements until we email you about it. We plan to email everyone about travel soon, but are starting with a small group to find and fix issues. Whether we offered to pay your travel costs or not, please we must hear about your travel plans so that we know when to expect you here. (And if we offered to pay for your hotel room, we will reserve your hotel room for you.) Find the numbered section below that applies to you: 1. We Offered to Pay for Your Travel \u00b6 We want to find reasonable and comfortable travel options for you. At the same time, we must stay within budget and follow University rules about arranging and paying for your travel costs. Let\u2019s work together to find something that makes sense for everyone. Here are ideas that have helped some School travelers in past years: If you are near Madison, consider driving; we can reimburse mileage and tolls up to a point, plus parking. Or look into bus routes, especially from larger cities like Chicago. The buses are very comfortable, have wi-fi, and run frequently. If you fly, try to get flights to and from Madison (MSN) itself . In some cases, we may ask you to consider flying to Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away), then taking a direct bus to Madison; we do this only when the costs or itinerary options to Madison are terrible. If you fly, be flexible about departure times \u2014 early and late flights are often the least expensive. We do not like very early or very late flights any more than you do, so we will work hard to find reasonable flight times. Note: Please try to complete your travel plans by about May 30, when rates may go up. Travel by Airplane \u00b6 Do NOT buy your own airline tickets . University rules say that our travel agency, Fox World Travel, must buy your tickets. Please try to complete air travel arrangements by Thursday, 29 May 2025; earlier is better ! Use the following information to get air travel tickets: Start by emailing Fox World Travel (FWT): Send the email to \u201cuwtravel@foxworldtravel.com\u201d As always, CC the School: \u201cschool@osg-htc.org\u201d For the email subject, put something like \u201cOSG School travel request (01UWS037) - \u201d, then your first initial and last name(s) For the email body, repeat that you are requesting flights for the OSG School 2025 (01UWS037), and include your full name, where you are traveling from (city or airport), and where you are returning to If you have special constraints or situations (e.g., other dates, travel companion), explain clearly A FWT agent will email you back with options and/or questions. Watch for this email and respond as soon as reasonable. It may take a few emails back and forth. Please CC the School on all emails. If you prefer, you may call FWT instead: +1 (844) 630-3853. If your itinerary must be approved (see below), then we will use email for that and remaining steps. Make sure FWT has your email address! We must review and approve some itineraries. FWT can purchase tickets directly in many cases, but if the agent says that your trip must be reviewed, do not worry! It just means that we need to check the budget, options, and UW rules. We hope to approve your first choice, or we will work with you and FWT to find another reasonable one. Common reasons for a trip needing review are: total trip cost over $800, leaving too early or arriving too late, travel starting and ending at different locations, and travel on dates other than June 22 and 28. Approval takes time, so it may take 1\u20132 days to get confirmation. Airplane tickets cannot be held without purchase over a weekend, so avoid contacting FWT late on Fridays. Please be considerate of the FWT agent(s) you work with. They work hard to find good options for you, but they must also follow our rules. If you feel that they are not providing the options that you want, you should email us . We will help resolve any issues. Do not argue with the FWT agents, especially about options you find online \u2014 there are many reasons why that option might not be available to us. Travel by Bus \u00b6 For some nearby locations, or in addition to air travel to Chicago or Milwaukee, it may be helpful to take a bus to Madison. Bus companies that School travelers have used often in the past are: Van Galder Bus , especially from Chicago Badger Bus , especially from Milwaukee To get bus tickets, pick one method: Ask us to buy bus tickets for you in advance. This is the easiest option all around. Just email us at school@osg-htc.org ; include your desired travel dates and times, and start and end bus stations or stops. Buy bus tickets for yourself. You may purchase bus tickets yourself before or on the day of travel. If you purchase your own tickets, you must get approval from the School for the estimated cost first, then request reimbursement from us after the School. If you purchase your own tickets, save the original receipt (even if by email). It is best to have a detailed receipt (including your name, itinerary, date of purchase, and total amount paid), but a regular ticket stub (e.g., without your name or date) should work fine. Just get what you can! Be sure to email us with your bus plans, including: Transportation provider(s) (e.g., Van Galder bus) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison Actual or estimated cost (indicate which) Travel by Personal Car \u00b6 If you are driving to Madison, you will be reimbursed the mileage rate of $0.700 per mile for the shortest round-trip distance (as calculated by Google Maps), plus tolls. Also, we will pay for parking costs for the week at the hotel in Madison (but not elsewhere). We recommend keeping your receipts for tolls. Note: Due to the high mileage reimbursement rate, driving can be an expensive option! We reserve the right to limit your total driving reimbursement, so work with us on the details. To travel by personal car, please check with us first. We may search for comparable flight options to figure out a reasonable maximum cost. Be sure to email us with your travel plans as soon as possible. Try to include: Departure date from home, location (for mileage calculation), and approximate time of arrival in Madison Departure date and approximate time from Madison, and return location (for mileage calculation) if different than above 2. We Are Not Paying for Your Travel \u00b6 If you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements now! Just remember to arrive on Sunday, June 22, before about 5:00 pm and depart on Saturday, June 28, or whatever dates we suggested directly to you. For other travel dates, check with us first, please! Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., airline) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison (e.g., airport, bus station, etc.)","title":"Travel planning"},{"location":"logistics/travel-planning/#travel-to-and-from-madison","text":"Please wait to begin making travel arrangements until we email you about it. We plan to email everyone about travel soon, but are starting with a small group to find and fix issues. Whether we offered to pay your travel costs or not, please we must hear about your travel plans so that we know when to expect you here. (And if we offered to pay for your hotel room, we will reserve your hotel room for you.) Find the numbered section below that applies to you:","title":"Travel To and From Madison"},{"location":"logistics/travel-planning/#1-we-offered-to-pay-for-your-travel","text":"We want to find reasonable and comfortable travel options for you. At the same time, we must stay within budget and follow University rules about arranging and paying for your travel costs. Let\u2019s work together to find something that makes sense for everyone. Here are ideas that have helped some School travelers in past years: If you are near Madison, consider driving; we can reimburse mileage and tolls up to a point, plus parking. Or look into bus routes, especially from larger cities like Chicago. The buses are very comfortable, have wi-fi, and run frequently. If you fly, try to get flights to and from Madison (MSN) itself . In some cases, we may ask you to consider flying to Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away), then taking a direct bus to Madison; we do this only when the costs or itinerary options to Madison are terrible. If you fly, be flexible about departure times \u2014 early and late flights are often the least expensive. We do not like very early or very late flights any more than you do, so we will work hard to find reasonable flight times. Note: Please try to complete your travel plans by about May 30, when rates may go up.","title":"1. We Offered to Pay for Your Travel"},{"location":"logistics/travel-planning/#travel-by-airplane","text":"Do NOT buy your own airline tickets . University rules say that our travel agency, Fox World Travel, must buy your tickets. Please try to complete air travel arrangements by Thursday, 29 May 2025; earlier is better ! Use the following information to get air travel tickets: Start by emailing Fox World Travel (FWT): Send the email to \u201cuwtravel@foxworldtravel.com\u201d As always, CC the School: \u201cschool@osg-htc.org\u201d For the email subject, put something like \u201cOSG School travel request (01UWS037) - \u201d, then your first initial and last name(s) For the email body, repeat that you are requesting flights for the OSG School 2025 (01UWS037), and include your full name, where you are traveling from (city or airport), and where you are returning to If you have special constraints or situations (e.g., other dates, travel companion), explain clearly A FWT agent will email you back with options and/or questions. Watch for this email and respond as soon as reasonable. It may take a few emails back and forth. Please CC the School on all emails. If you prefer, you may call FWT instead: +1 (844) 630-3853. If your itinerary must be approved (see below), then we will use email for that and remaining steps. Make sure FWT has your email address! We must review and approve some itineraries. FWT can purchase tickets directly in many cases, but if the agent says that your trip must be reviewed, do not worry! It just means that we need to check the budget, options, and UW rules. We hope to approve your first choice, or we will work with you and FWT to find another reasonable one. Common reasons for a trip needing review are: total trip cost over $800, leaving too early or arriving too late, travel starting and ending at different locations, and travel on dates other than June 22 and 28. Approval takes time, so it may take 1\u20132 days to get confirmation. Airplane tickets cannot be held without purchase over a weekend, so avoid contacting FWT late on Fridays. Please be considerate of the FWT agent(s) you work with. They work hard to find good options for you, but they must also follow our rules. If you feel that they are not providing the options that you want, you should email us . We will help resolve any issues. Do not argue with the FWT agents, especially about options you find online \u2014 there are many reasons why that option might not be available to us.","title":"Travel by Airplane"},{"location":"logistics/travel-planning/#travel-by-bus","text":"For some nearby locations, or in addition to air travel to Chicago or Milwaukee, it may be helpful to take a bus to Madison. Bus companies that School travelers have used often in the past are: Van Galder Bus , especially from Chicago Badger Bus , especially from Milwaukee To get bus tickets, pick one method: Ask us to buy bus tickets for you in advance. This is the easiest option all around. Just email us at school@osg-htc.org ; include your desired travel dates and times, and start and end bus stations or stops. Buy bus tickets for yourself. You may purchase bus tickets yourself before or on the day of travel. If you purchase your own tickets, you must get approval from the School for the estimated cost first, then request reimbursement from us after the School. If you purchase your own tickets, save the original receipt (even if by email). It is best to have a detailed receipt (including your name, itinerary, date of purchase, and total amount paid), but a regular ticket stub (e.g., without your name or date) should work fine. Just get what you can! Be sure to email us with your bus plans, including: Transportation provider(s) (e.g., Van Galder bus) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison Actual or estimated cost (indicate which)","title":"Travel by Bus"},{"location":"logistics/travel-planning/#travel-by-personal-car","text":"If you are driving to Madison, you will be reimbursed the mileage rate of $0.700 per mile for the shortest round-trip distance (as calculated by Google Maps), plus tolls. Also, we will pay for parking costs for the week at the hotel in Madison (but not elsewhere). We recommend keeping your receipts for tolls. Note: Due to the high mileage reimbursement rate, driving can be an expensive option! We reserve the right to limit your total driving reimbursement, so work with us on the details. To travel by personal car, please check with us first. We may search for comparable flight options to figure out a reasonable maximum cost. Be sure to email us with your travel plans as soon as possible. Try to include: Departure date from home, location (for mileage calculation), and approximate time of arrival in Madison Departure date and approximate time from Madison, and return location (for mileage calculation) if different than above","title":"Travel by Personal Car"},{"location":"logistics/travel-planning/#2-we-are-not-paying-for-your-travel","text":"If you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements now! Just remember to arrive on Sunday, June 22, before about 5:00 pm and depart on Saturday, June 28, or whatever dates we suggested directly to you. For other travel dates, check with us first, please! Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., airline) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison (e.g., airport, bus station, etc.)","title":"2. We Are Not Paying for Your Travel"},{"location":"logistics/visas/","text":"Documentation Requirements for Non-Resident Aliens \u00b6 This page is for Non-Resident Aliens only. If you are a United States citizen or permanent resident or member of the UW\u2013Madison community, this page does not apply to you. For the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal information from you. We collect as little information as possible and do not share it except with University staff who need it. Most of what we need comes from the online form you completed after accepting our invitation to attend. When you come to the School in Madison, we will need to look at and verify your travel documents. Please bring all travel documents to the School! See below for details. Tasks To Do Now \u00b6 Please check your passport and visa for travel in the United States now. Make sure that all documents are valid from now and until after the School ends. If any documents are expired or will expire before the end of the School: Tell us immediately, so that we can help you Begin the process for updating your documents immediately Do whatever you can to expedite the update process The University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents. We have no control over this policy and there are no exceptions. If you are in the United States on a J-1 Scholar visa, there are extra steps needed to make the University and Federal government happy. If you have a J-1 visa and have not heard from us about it already, please email us immediately so that we can help. Documents to Bring to the School \u00b6 When you come to Madison, you must bring: Passport U.S. visa U.S. Customs and Border Protection form I-94 If you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it! If you entered the U.S. after 30 April 2013, the I-94 is stored electronically; you can request a copy to print from CBP If you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94. Additional forms specified in the table below: If you have this visa We will also need F-1 (Student) Form I-20 (original document, not a copy) J-1 (Visitor) Form DS-2019 (original document, not a copy) Visa Waiver Program Paper copy of ESTA Authorization Please bring all required information and documents to the School, especially on Tuesday, June 24. School staff will make copies of the documents and return them to you as quickly as possible. We will announce further details in class.","title":"Visa requirements"},{"location":"logistics/visas/#documentation-requirements-for-non-resident-aliens","text":"This page is for Non-Resident Aliens only. If you are a United States citizen or permanent resident or member of the UW\u2013Madison community, this page does not apply to you. For the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal information from you. We collect as little information as possible and do not share it except with University staff who need it. Most of what we need comes from the online form you completed after accepting our invitation to attend. When you come to the School in Madison, we will need to look at and verify your travel documents. Please bring all travel documents to the School! See below for details.","title":"Documentation Requirements for Non-Resident Aliens"},{"location":"logistics/visas/#tasks-to-do-now","text":"Please check your passport and visa for travel in the United States now. Make sure that all documents are valid from now and until after the School ends. If any documents are expired or will expire before the end of the School: Tell us immediately, so that we can help you Begin the process for updating your documents immediately Do whatever you can to expedite the update process The University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents. We have no control over this policy and there are no exceptions. If you are in the United States on a J-1 Scholar visa, there are extra steps needed to make the University and Federal government happy. If you have a J-1 visa and have not heard from us about it already, please email us immediately so that we can help.","title":"Tasks To Do Now"},{"location":"logistics/visas/#documents-to-bring-to-the-school","text":"When you come to Madison, you must bring: Passport U.S. visa U.S. Customs and Border Protection form I-94 If you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it! If you entered the U.S. after 30 April 2013, the I-94 is stored electronically; you can request a copy to print from CBP If you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94. Additional forms specified in the table below: If you have this visa We will also need F-1 (Student) Form I-20 (original document, not a copy) J-1 (Visitor) Form DS-2019 (original document, not a copy) Visa Waiver Program Paper copy of ESTA Authorization Please bring all required information and documents to the School, especially on Tuesday, June 24. School staff will make copies of the documents and return them to you as quickly as possible. We will announce further details in class.","title":"Documents to Bring to the School"},{"location":"materials/","text":"OSG School Materials \u00b6 School Overview and Intro \u00b6 View the slides: pdf Intro to HTC and HTCondor Job Execution \u00b6 Intro to HTC Slides \u00b6 Intro to HTC: pdf Worksheet: pdf or Google Drive Intro to HTCondor Slides \u00b6 View the slides: pdf Intro Exercises 1: Running and Viewing Simple Jobs (Strongly Recommended) \u00b6 Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue Bonus Exercises: Job Attributes and Handling \u00b6 Bonus Exercise 1.7: Explore condor_q Bonus Exercise 1.8: Explore condor_status Intro to HTCondor Multiple Job Execution \u00b6 View the Slides: pdf Intro Exercises 2: Running Many HTC Jobs (Strongly Recommended) \u00b6 Exercise 2.1: Work with input and output files Exercise 2.2: Use queue N , $(Cluster) , and $(Process) Exercise 2.3: Use queue from with custom variables Bonus Exercise 2.4: Use queue matching with a custom variable The Open Science Pool (OSPool) \u00b6 View the slides: pdf OSPool Exercises: Researching the OSPool (Strongly Recommended) \u00b6 Exercise 1.1: Where Do Jobs Run? Exercise 1.2: How Much Can I Get? Exercise 1.3: How Does Capacity Change? Exercise 1.4: What Is In an Execution Point? Bonus Exercise 1.5: Viewing OSPool Information Troubleshooting \u00b6 View the Slides: pdf ppt Troubleshooting Exercises: \u00b6 Exercise 1.1: Troubleshooting Jobs Exercise 1.2: Job Retry Software \u00b6 View the Slides: pdf , pptx Software Exercises 1: Exploring Containers \u00b6 Exercise 1.1: Run and Explore Apptainer Containers Exercise 1.2: Use Apptainer Containers in OSPool Jobs Exercise 1.3: Use Docker Containers in OSPool Jobs Exercise 1.4: Build, Test, and Deploy an Apptainer Container Exercise 1.5: Choose Software Options Software Exercises 2: Preparing Scripts \u00b6 Exercise 2.1: Build an HTC-Friendly Executable Software Exercises 3: Container Examples (Optional) \u00b6 Exercise 3.1: Create an Apptainer Definition Files Exercise 3.2: Build Your Own Docker Container Software Exercises 4: Exploring Compiled Software (Optional) \u00b6 Exercise 4.1: Download and Use Compiled Software Exercise 4.2: Use a Wrapper Script To Run Software Exercise 4.3: Using Arguments With Wrapper Scripts Software Exercises 5: Compiled Software Examples (Optional) \u00b6 Exercise 5.1: Compiling a Research Software Exercise 5.2: Compiling Python and Running Jobs Exercise 5.3: Using Conda Environments Exercise 5.4: Compiling and Running a Simple Code Data \u00b6 View the Slides: pdf Data Exercises 1: HTCondor File Transfer (Strongly Recommended) \u00b6 Exercise 1.1: Understanding a job's data needs Exercise 1.2: transfer_input_files, transfer_output_files, and remaps Exercise 1.3: Splitting input Data Exercises 2: Using OSDF (Strongly Recommended) \u00b6 Exercise 2.1: OSDF for inputs Exercise 2.2: OSDF for outputs Scaling Up \u00b6 View the Slides: pdf Scaling Up Exercises \u00b6 Exercise 1.1: Organizing HTC workloads Exercise 1.2: Composing Your Jobs Exercise 2.1: Log Into a Local Pool Exercise 2.2: Hardware Differences Between OSPool and the CHTC Pool Exercise 2.3: Software Differences Between OSPool and the CHTC Pool Workflows with DAGMan \u00b6 View the slides: pptx , pdf DAGMan Exercises 1 \u00b6 Exercise 1.1: Coordinating set of jobs: A simple DAG Exercise 1.2: A brief detour through the Mandelbrot set Exercise 1.3: A more complex DAG Exercise 1.4: Handling jobs that fail with DAGMan Exercise 1.5: Workflow Challenges Extra Topics \u00b6 Machine Learning \u00b6 View the Slides: pdf Self-checkpointing for long-running jobs \u00b6 View the Slides: pdf ppt Exercise 1.1: Trying out self-checkpointing Special Environments \u00b6 View the slides: [Coming soon] Special Environments Exercises 1 \u00b6 Exercise 1.1: GPUs Introduction to Research Computing Facilitation \u00b6 View the slides: [Coming soon] Final Talks \u00b6 Philosophy: [Slides coming soon] Final thoughts: [Slides coming soon] Forward (Tim\u2019s final talk): PDF","title":"Overview"},{"location":"materials/#osg-school-materials","text":"","title":"OSG School Materials"},{"location":"materials/#school-overview-and-intro","text":"View the slides: pdf","title":"School Overview and Intro"},{"location":"materials/#intro-to-htc-and-htcondor-job-execution","text":"","title":"Intro to HTC and HTCondor Job Execution"},{"location":"materials/#intro-to-htc-slides","text":"Intro to HTC: pdf Worksheet: pdf or Google Drive","title":"Intro to HTC Slides"},{"location":"materials/#intro-to-htcondor-slides","text":"View the slides: pdf","title":"Intro to HTCondor Slides"},{"location":"materials/#intro-exercises-1-running-and-viewing-simple-jobs-strongly-recommended","text":"Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue","title":"Intro Exercises 1: Running and Viewing Simple Jobs (Strongly Recommended)"},{"location":"materials/#bonus-exercises-job-attributes-and-handling","text":"Bonus Exercise 1.7: Explore condor_q Bonus Exercise 1.8: Explore condor_status","title":"Bonus Exercises: Job Attributes and Handling"},{"location":"materials/#intro-to-htcondor-multiple-job-execution","text":"View the Slides: pdf","title":"Intro to HTCondor Multiple Job Execution"},{"location":"materials/#intro-exercises-2-running-many-htc-jobs-strongly-recommended","text":"Exercise 2.1: Work with input and output files Exercise 2.2: Use queue N , $(Cluster) , and $(Process) Exercise 2.3: Use queue from with custom variables Bonus Exercise 2.4: Use queue matching with a custom variable","title":"Intro Exercises 2: Running Many HTC Jobs (Strongly Recommended)"},{"location":"materials/#the-open-science-pool-ospool","text":"View the slides: pdf","title":"The Open Science Pool (OSPool)"},{"location":"materials/#ospool-exercises-researching-the-ospool-strongly-recommended","text":"Exercise 1.1: Where Do Jobs Run? Exercise 1.2: How Much Can I Get? Exercise 1.3: How Does Capacity Change? Exercise 1.4: What Is In an Execution Point? Bonus Exercise 1.5: Viewing OSPool Information","title":"OSPool Exercises: Researching the OSPool (Strongly Recommended)"},{"location":"materials/#troubleshooting","text":"View the Slides: pdf ppt","title":"Troubleshooting"},{"location":"materials/#troubleshooting-exercises","text":"Exercise 1.1: Troubleshooting Jobs Exercise 1.2: Job Retry","title":"Troubleshooting Exercises:"},{"location":"materials/#software","text":"View the Slides: pdf , pptx","title":"Software"},{"location":"materials/#software-exercises-1-exploring-containers","text":"Exercise 1.1: Run and Explore Apptainer Containers Exercise 1.2: Use Apptainer Containers in OSPool Jobs Exercise 1.3: Use Docker Containers in OSPool Jobs Exercise 1.4: Build, Test, and Deploy an Apptainer Container Exercise 1.5: Choose Software Options","title":"Software Exercises 1: Exploring Containers"},{"location":"materials/#software-exercises-2-preparing-scripts","text":"Exercise 2.1: Build an HTC-Friendly Executable","title":"Software Exercises 2: Preparing Scripts"},{"location":"materials/#software-exercises-3-container-examples-optional","text":"Exercise 3.1: Create an Apptainer Definition Files Exercise 3.2: Build Your Own Docker Container","title":"Software Exercises 3: Container Examples (Optional)"},{"location":"materials/#software-exercises-4-exploring-compiled-software-optional","text":"Exercise 4.1: Download and Use Compiled Software Exercise 4.2: Use a Wrapper Script To Run Software Exercise 4.3: Using Arguments With Wrapper Scripts","title":"Software Exercises 4: Exploring Compiled Software (Optional)"},{"location":"materials/#software-exercises-5-compiled-software-examples-optional","text":"Exercise 5.1: Compiling a Research Software Exercise 5.2: Compiling Python and Running Jobs Exercise 5.3: Using Conda Environments Exercise 5.4: Compiling and Running a Simple Code","title":"Software Exercises 5: Compiled Software Examples (Optional)"},{"location":"materials/#data","text":"View the Slides: pdf","title":"Data"},{"location":"materials/#data-exercises-1-htcondor-file-transfer-strongly-recommended","text":"Exercise 1.1: Understanding a job's data needs Exercise 1.2: transfer_input_files, transfer_output_files, and remaps Exercise 1.3: Splitting input","title":"Data Exercises 1: HTCondor File Transfer (Strongly Recommended)"},{"location":"materials/#data-exercises-2-using-osdf-strongly-recommended","text":"Exercise 2.1: OSDF for inputs Exercise 2.2: OSDF for outputs","title":"Data Exercises 2: Using OSDF (Strongly Recommended)"},{"location":"materials/#scaling-up","text":"View the Slides: pdf","title":"Scaling Up"},{"location":"materials/#scaling-up-exercises","text":"Exercise 1.1: Organizing HTC workloads Exercise 1.2: Composing Your Jobs Exercise 2.1: Log Into a Local Pool Exercise 2.2: Hardware Differences Between OSPool and the CHTC Pool Exercise 2.3: Software Differences Between OSPool and the CHTC Pool","title":"Scaling Up Exercises"},{"location":"materials/#workflows-with-dagman","text":"View the slides: pptx , pdf","title":"Workflows with DAGMan"},{"location":"materials/#dagman-exercises-1","text":"Exercise 1.1: Coordinating set of jobs: A simple DAG Exercise 1.2: A brief detour through the Mandelbrot set Exercise 1.3: A more complex DAG Exercise 1.4: Handling jobs that fail with DAGMan Exercise 1.5: Workflow Challenges","title":"DAGMan Exercises 1"},{"location":"materials/#extra-topics","text":"","title":"Extra Topics"},{"location":"materials/#machine-learning","text":"View the Slides: pdf","title":"Machine Learning"},{"location":"materials/#self-checkpointing-for-long-running-jobs","text":"View the Slides: pdf ppt Exercise 1.1: Trying out self-checkpointing","title":"Self-checkpointing for long-running jobs"},{"location":"materials/#special-environments","text":"View the slides: [Coming soon]","title":"Special Environments"},{"location":"materials/#special-environments-exercises-1","text":"Exercise 1.1: GPUs","title":"Special Environments Exercises 1"},{"location":"materials/#introduction-to-research-computing-facilitation","text":"View the slides: [Coming soon]","title":"Introduction to Research Computing Facilitation"},{"location":"materials/#final-talks","text":"Philosophy: [Slides coming soon] Final thoughts: [Slides coming soon] Forward (Tim\u2019s final talk): PDF","title":"Final Talks"},{"location":"materials/checkpoint/part1-ex1-checkpointing/","text":"Self-Checkpointing Exercise 1.1: Trying It Out \u00b6 The goal of this exercise is to practice writing a submit file for self-checkpointing, and to see the process in action. Calculating Fibonacci numbers \u2026 slowly \u00b6 The sample code for this exercise calculates the Fibonacci number resulting from a given set of iterations. Because this is a trival computation, the code includes a delay in each iteration through the main loop; this simulates a more intensive computation. To get set up: Log in to ap40.uw.osg-htc.org ( ap1 is fine, too) Create and change into a new directory for this exercise Download the Python script that is the main executable for this exercise: user@server $ osdf object get /ospool/uc-shared/public/school/2025/fibonacci.py ./ If you want to run the script directly, make it executable first: user@server $ chmod 0755 fibonacci.py Take a look at the code, if you like. It is not very elegant, but it gets the job done. A few notes: The script takes a single argument, the number of iterations to run. To minimize computing time while leaving time to explore, 10 is a good number of iterations. The script checkpoints every other iteration through the main loop. The exit status code for a checkpoint is 85. It prints some output to standard out along the way, to let you know what is going on. The final result is written to a separate file named fibonacci.result . This file does not exist until the very end of the complete run. It is safe to run from the command line on an access point: user@server $ ./fibonacci.py 10 If you run it, what happens? (Due to the 30-second delay, be patient.) Can you explain its behavior? What happens if you run it again, without changing any files in between? Why? Preparing to run \u00b6 Now you have an executable and you know how to run it. It is time to prepare it for submission to HTCondor! Using what you know about the script (above), and using information in the slides from today, try writing a submit file that runs this software and implements exit-driven self-checkpointing. The Python code itself is ready and should not need any changes. Just use a plain queue statement, one job is enough to experiment on. Before you submit, read the next section first! Running and monitoring \u00b6 With the 30-second delay per iteration in the code and the suggested 10 iterations, once the script starts running you have about 5 minutes of runtime in which to see what is going on. So it may help to read through this section and then return here and submit your job. If your job has problems or finishes before you have the chance to do all the steps below, just remove the extra files (besides the Python script and your submit file) and try again! Submission and first checkpoint \u00b6 Submit the job Look at the contents of the submit directory \u2014 what changed? Start watching the log file: tail -n 100 -f YOUR-LOG-FILENAME.log Be patient! As HTCondor adds more lines to the end of your log file, they will appear automatically. Thus, nothing much will happen until HTCondor starts running your job. When it does, you will see three sets of messages in the log file quickly: Started transferring input files Finished transferring input files Job executing on host: (Of course, each message will contain a lot of other characters!) Now wait about 1 minute, and you should see two more messages appear: Started transferring output files Finished transferring output files That is the first checkpoint happening! Forcing your job to stop running \u00b6 Now, assuming that your job is still running (check condor_q again), you can force HTCondor to remove ( evict ) your job before it finishes: Run condor_q to get the job ID of the running job Run condor_vacate_job JOB_ID , where you replace JOB_ID with your job ID from above Monitor the action again by running tail -n 100 -f YOUR-LOG-FILENAME.log Finishing the job and wrap-up \u00b6 Be patient again! You removed your running job, and so HTCondor put it back in the queue as idle. If you wait a minute or two, you should see that HTCondor starts running the job again. In the log file, look carefully for the two Job executing on host: messages. Does it seem like you ran on the same computer again or on a different one? Both are possible! Let your job finish running this time. There should be a Job terminated of its own accord message near the end. Did you get results? Go through all the files and see what they contain. The log and output files are probably the most interesting. But did you get a result file, too? Did the output file \u2014 that is, whatever file you named in the output line of your submit file \u2014 contain everything that you expected it to? Conclusion \u00b6 This has been a brief and simple tour of self-checkpointing. If you would like to learn more, please read the Self-Checkpointing Applications section of the HTCondor Manual. Or talk to School staff about it. Or contact support@osg-htc.org for further help at any time.","title":"1.1 - Trying out self-checkpointing"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#self-checkpointing-exercise-11-trying-it-out","text":"The goal of this exercise is to practice writing a submit file for self-checkpointing, and to see the process in action.","title":"Self-Checkpointing Exercise 1.1: Trying It Out"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#calculating-fibonacci-numbers-slowly","text":"The sample code for this exercise calculates the Fibonacci number resulting from a given set of iterations. Because this is a trival computation, the code includes a delay in each iteration through the main loop; this simulates a more intensive computation. To get set up: Log in to ap40.uw.osg-htc.org ( ap1 is fine, too) Create and change into a new directory for this exercise Download the Python script that is the main executable for this exercise: user@server $ osdf object get /ospool/uc-shared/public/school/2025/fibonacci.py ./ If you want to run the script directly, make it executable first: user@server $ chmod 0755 fibonacci.py Take a look at the code, if you like. It is not very elegant, but it gets the job done. A few notes: The script takes a single argument, the number of iterations to run. To minimize computing time while leaving time to explore, 10 is a good number of iterations. The script checkpoints every other iteration through the main loop. The exit status code for a checkpoint is 85. It prints some output to standard out along the way, to let you know what is going on. The final result is written to a separate file named fibonacci.result . This file does not exist until the very end of the complete run. It is safe to run from the command line on an access point: user@server $ ./fibonacci.py 10 If you run it, what happens? (Due to the 30-second delay, be patient.) Can you explain its behavior? What happens if you run it again, without changing any files in between? Why?","title":"Calculating Fibonacci numbers &hellip; slowly"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#preparing-to-run","text":"Now you have an executable and you know how to run it. It is time to prepare it for submission to HTCondor! Using what you know about the script (above), and using information in the slides from today, try writing a submit file that runs this software and implements exit-driven self-checkpointing. The Python code itself is ready and should not need any changes. Just use a plain queue statement, one job is enough to experiment on. Before you submit, read the next section first!","title":"Preparing to run"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#running-and-monitoring","text":"With the 30-second delay per iteration in the code and the suggested 10 iterations, once the script starts running you have about 5 minutes of runtime in which to see what is going on. So it may help to read through this section and then return here and submit your job. If your job has problems or finishes before you have the chance to do all the steps below, just remove the extra files (besides the Python script and your submit file) and try again!","title":"Running and monitoring"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#submission-and-first-checkpoint","text":"Submit the job Look at the contents of the submit directory \u2014 what changed? Start watching the log file: tail -n 100 -f YOUR-LOG-FILENAME.log Be patient! As HTCondor adds more lines to the end of your log file, they will appear automatically. Thus, nothing much will happen until HTCondor starts running your job. When it does, you will see three sets of messages in the log file quickly: Started transferring input files Finished transferring input files Job executing on host: (Of course, each message will contain a lot of other characters!) Now wait about 1 minute, and you should see two more messages appear: Started transferring output files Finished transferring output files That is the first checkpoint happening!","title":"Submission and first checkpoint"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#forcing-your-job-to-stop-running","text":"Now, assuming that your job is still running (check condor_q again), you can force HTCondor to remove ( evict ) your job before it finishes: Run condor_q to get the job ID of the running job Run condor_vacate_job JOB_ID , where you replace JOB_ID with your job ID from above Monitor the action again by running tail -n 100 -f YOUR-LOG-FILENAME.log","title":"Forcing your job to stop running"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#finishing-the-job-and-wrap-up","text":"Be patient again! You removed your running job, and so HTCondor put it back in the queue as idle. If you wait a minute or two, you should see that HTCondor starts running the job again. In the log file, look carefully for the two Job executing on host: messages. Does it seem like you ran on the same computer again or on a different one? Both are possible! Let your job finish running this time. There should be a Job terminated of its own accord message near the end. Did you get results? Go through all the files and see what they contain. The log and output files are probably the most interesting. But did you get a result file, too? Did the output file \u2014 that is, whatever file you named in the output line of your submit file \u2014 contain everything that you expected it to?","title":"Finishing the job and wrap-up"},{"location":"materials/checkpoint/part1-ex1-checkpointing/#conclusion","text":"This has been a brief and simple tour of self-checkpointing. If you would like to learn more, please read the Self-Checkpointing Applications section of the HTCondor Manual. Or talk to School staff about it. Or contact support@osg-htc.org for further help at any time.","title":"Conclusion"},{"location":"materials/data/part1-ex1-data-needs/","text":"Data Exercise 1.1: Understanding Data Requirements \u00b6 Exercise Goal \u00b6 This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST . Setup \u00b6 For this exercise, we will use the ap40.uw.osg-htc.org access point. Log in: $ ssh <USERNAME>@ap40.uw.osg-htc.org Create a directory for this exercise named blast-data and change into it Copy the Input Files \u00b6 To run BLAST, we need the executable, input file, and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Copy the BLAST executables: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/ncbi-blast-2.16.0+-x64-linux.tar.gz ./ user@ap40 $ tar -xzvf ncbi-blast-2.16.0+-x64-linux.tar.gz Download these files to your current directory: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/pdbaa.tar.gz ./ user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/mouse.fa ./ Untar the pdbaa database: user@ap40 $ tar -xzvf pdbaa.tar.gz Understanding BLAST \u00b6 Remember that blastx is executed in a command like the following: user@ap40 $ ./ncbi-blast-2.16.0+/bin/blastx -db <DATABASE ROOTNAME> -query <INPUT FILE> -out <RESULTS FILE> In the above, the <INPUT FILE> is the name of a file containing a number of genetic sequences (e.g. mouse.fa ), and the database that these are compared against is made up of several files that begin with the same <DATABASE ROOTNAME> , (e.g. pdbaa/pdbaa ). The output from this analysis will be printed to <RESULTS FILE> which is also indicated in the command. Calculating Data Needs \u00b6 Using the files that you prepared in blast-data , we will calculate how much disk space is needed if we were to run a hypothetical BLAST job with a wrapper script, where the job: Transfers all of its input files (including the executable) as tarballs Untars the input files tarballs on the execute host Runs blastx using the untarred input files Here are some commands that will be useful for calculating your job's storage needs: List the size of a specific file: user@ap40 $ ls -lh <FILE NAME> List the sizes of all files in the current directory: user@ap40 $ ls -lh Sum the size of all files in a specific directory: user@ap40 $ du -sh <DIRECTORY> Input requirements \u00b6 Using the commands that you have learned so far, you should be able to get an idea about the input file size. Now, do the following before moving on to the next exercise. Total up the amount of data in all of the files necessary to run the blastx wrapper job, including the executable itself. Write down this number. Also take note of how much total data is in the pdbaa directory. Compressed Files Remember, blastx reads the un-compressed pdbaa files. Output requirements \u00b6 You also need to get an idea about the output file sizes as well to estimate the total disk requirement.To sum the output file sizes, estimate the file sizes of the following files The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . Also, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well. Up next! \u00b6 Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"1.1 - Understanding a job's data needs"},{"location":"materials/data/part1-ex1-data-needs/#data-exercise-11-understanding-data-requirements","text":"","title":"Data Exercise 1.1: Understanding Data Requirements"},{"location":"materials/data/part1-ex1-data-needs/#exercise-goal","text":"This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST .","title":"Exercise Goal"},{"location":"materials/data/part1-ex1-data-needs/#setup","text":"For this exercise, we will use the ap40.uw.osg-htc.org access point. Log in: $ ssh <USERNAME>@ap40.uw.osg-htc.org Create a directory for this exercise named blast-data and change into it","title":"Setup"},{"location":"materials/data/part1-ex1-data-needs/#copy-the-input-files","text":"To run BLAST, we need the executable, input file, and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Copy the BLAST executables: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/ncbi-blast-2.16.0+-x64-linux.tar.gz ./ user@ap40 $ tar -xzvf ncbi-blast-2.16.0+-x64-linux.tar.gz Download these files to your current directory: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/pdbaa.tar.gz ./ user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/mouse.fa ./ Untar the pdbaa database: user@ap40 $ tar -xzvf pdbaa.tar.gz","title":"Copy the Input Files"},{"location":"materials/data/part1-ex1-data-needs/#understanding-blast","text":"Remember that blastx is executed in a command like the following: user@ap40 $ ./ncbi-blast-2.16.0+/bin/blastx -db <DATABASE ROOTNAME> -query <INPUT FILE> -out <RESULTS FILE> In the above, the <INPUT FILE> is the name of a file containing a number of genetic sequences (e.g. mouse.fa ), and the database that these are compared against is made up of several files that begin with the same <DATABASE ROOTNAME> , (e.g. pdbaa/pdbaa ). The output from this analysis will be printed to <RESULTS FILE> which is also indicated in the command.","title":"Understanding BLAST"},{"location":"materials/data/part1-ex1-data-needs/#calculating-data-needs","text":"Using the files that you prepared in blast-data , we will calculate how much disk space is needed if we were to run a hypothetical BLAST job with a wrapper script, where the job: Transfers all of its input files (including the executable) as tarballs Untars the input files tarballs on the execute host Runs blastx using the untarred input files Here are some commands that will be useful for calculating your job's storage needs: List the size of a specific file: user@ap40 $ ls -lh <FILE NAME> List the sizes of all files in the current directory: user@ap40 $ ls -lh Sum the size of all files in a specific directory: user@ap40 $ du -sh <DIRECTORY>","title":"Calculating Data Needs"},{"location":"materials/data/part1-ex1-data-needs/#input-requirements","text":"Using the commands that you have learned so far, you should be able to get an idea about the input file size. Now, do the following before moving on to the next exercise. Total up the amount of data in all of the files necessary to run the blastx wrapper job, including the executable itself. Write down this number. Also take note of how much total data is in the pdbaa directory. Compressed Files Remember, blastx reads the un-compressed pdbaa files.","title":"Input requirements"},{"location":"materials/data/part1-ex1-data-needs/#output-requirements","text":"You also need to get an idea about the output file sizes as well to estimate the total disk requirement.To sum the output file sizes, estimate the file sizes of the following files The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . Also, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well.","title":"Output requirements"},{"location":"materials/data/part1-ex1-data-needs/#up-next","text":"Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"Up next!"},{"location":"materials/data/part1-ex2-file-transfer/","text":"Data Exercise 1.2: transfer_input_files, transfer_output_files, and remaps \u00b6 Exercise Goal \u00b6 The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, use the knowledge of previous exercise and begin examining the memory and disk space used by your jobs in order to plan larger batches. We will also explore ways to deal with output data. Setup \u00b6 The executable we'll use in this exercise and later today is the same blastx executable from previous exercises. Log in to ap40: $ ssh <USERNAME>@ap40.uw.osg-htc.org Then change into the blast-data folder that you created in the previous exercise. Review: HTCondor File Transfer \u00b6 Recall that OSG does NOT have a shared filesystem! Instead, HTCondor transfers your executable and input files (specified with the executable and transfer_input_files submit file directives, respectively) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node. Start with a test submit file \u00b6 We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.err log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OSGVO_OS_STRING == \"RHEL 9\") queue Implement file compression \u00b6 In our first blast job from the Software exercises ( 1.1 ), the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For this version of the job, let's compress our blast database files to send them to the submit node as a single tar.gz file (otherwise known as a tarball), by following the below steps: Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. Note that this file will be different from the pdbaa.tar.gz file that you used earlier, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) Remember, a typical command for creating a tar file is: user@ap40 $ tar -cvzf <COMPRESSED FILENAME> <LIST OF FILES OR DIRECTORIES> Replacing <COMPRESSED FILENAME> with the name of the tarball that you would like to create and <LIST OF FILES OR DIRECTORIES> with a space-separated list of files and/or directories that you want inside pdbaa_files.tar.gz. Move the resulting tarball to the blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our executable in the submit file, we'll also end up transferring the blastx executable with transfer_input_files . In the blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar -xzvf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* Also remember to make the script executable: chmod +x blast_wrapper.sh Extra Files! The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output since they're \"new\" files that HTCondor didn't transfer over as input. List the executable and input files \u00b6 Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) In transfer_input_files , list the blastx binary, the pdbaa_files.tar.gz file, and the input query file. Commas, commas everywhere! Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script. Predict memory and disk requests from your data \u00b6 Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: How much memory blastx would use if it loaded all of the database files and the query input file into memory. How much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). Whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from Software exercise ( 1.1 ), and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests. You may still want to request slightly more than the job actually used. Run the test job \u00b6 Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from Software exercise ( 1.1 ), and why? transfer_output_files \u00b6 So far, we have used HTCondor's new file detection to transfer back the newly created files. An alternative is to be explicit, using the transfer_output_files attribute in the submit file. The upside to this approach is that you can pick to only transfer back a subset of the created files. The downside is that you have to know which files are created. The first exercise is to modify the submit file from the previous example, and add a line like (remember, before the queue ): transfer_output_files = mouse.fa.result You may also remove the last line in the blast_wrapper.sh , the rm pdbaa.* as extra files are no longer an issue - those files will be ignored because we used transfer_output_files . Submit the job, and make sure everything works. Did you get any pdbaa.* files back? The next thing we should try is to see what happens if the file we specify does not exist. Modify your submit file, and change the transfer_output_files to: transfer_output_files = elephant.fa.result Submit the job and see how it behaves. Did it finish successfully? transfer_output_remaps \u00b6 Related to transfer_output_files is transfer_output_remaps , which allows us to rename outputs, or map the outputs to a different storage system (will be explored in the next module). The format of the transfer_output_remaps attribute is a list of remaps, each remap taking the form of src=dst . The destination can be a local path, or a URL. For example: transfer_output_remaps = \"myresults.dat = s3://destination-server.com/myresults.dat\" If you have more than one remap, you can separate them with ; By now, your blast-data directory is probably starting to look messy with a mix of submit files, input data, log file and output data all intermingled. One improvement could be to map our outputs to a separate directory. Create a new directory named science-results . Add a transfer_output_remaps line to the submit file. It is common to place this line right after the transfer_output_files line. Change the transfer_output_files back to mouse.fa.result . Example: transfer_output_files = mouse.fa.result transfer_output_remaps = Fill out the remap line, mapping mouse.fa.result to the destination science-results/mouse.fa.result . Remember that the transfer_output_remaps value requires double quotes around it. Submit the job, and wait for it to complete. Are there any errors? Can you find mouse.fa.result? Conclusions \u00b6 In this exercise, you: Used your data requirements knowledge from the previous exercise to write a job. Executed the job on a remote worker node and took note of the data usage. Used transfer_input_files to transfer inputs Used transfer_output_files to transfer outputs Used transfer_output_remaps to map outputs to a different destination When you've completed the above, continue with the next exercise .","title":"1.2 - transfer_input_files, transfer_output_files, and remaps"},{"location":"materials/data/part1-ex2-file-transfer/#data-exercise-12-transfer_input_files-transfer_output_files-and-remaps","text":"","title":"Data Exercise 1.2: transfer_input_files, transfer_output_files, and remaps"},{"location":"materials/data/part1-ex2-file-transfer/#exercise-goal","text":"The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, use the knowledge of previous exercise and begin examining the memory and disk space used by your jobs in order to plan larger batches. We will also explore ways to deal with output data.","title":"Exercise Goal"},{"location":"materials/data/part1-ex2-file-transfer/#setup","text":"The executable we'll use in this exercise and later today is the same blastx executable from previous exercises. Log in to ap40: $ ssh <USERNAME>@ap40.uw.osg-htc.org Then change into the blast-data folder that you created in the previous exercise.","title":"Setup"},{"location":"materials/data/part1-ex2-file-transfer/#review-htcondor-file-transfer","text":"Recall that OSG does NOT have a shared filesystem! Instead, HTCondor transfers your executable and input files (specified with the executable and transfer_input_files submit file directives, respectively) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node.","title":"Review: HTCondor File Transfer"},{"location":"materials/data/part1-ex2-file-transfer/#start-with-a-test-submit-file","text":"We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.err log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OSGVO_OS_STRING == \"RHEL 9\") queue","title":"Start with a test submit file"},{"location":"materials/data/part1-ex2-file-transfer/#implement-file-compression","text":"In our first blast job from the Software exercises ( 1.1 ), the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For this version of the job, let's compress our blast database files to send them to the submit node as a single tar.gz file (otherwise known as a tarball), by following the below steps: Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. Note that this file will be different from the pdbaa.tar.gz file that you used earlier, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) Remember, a typical command for creating a tar file is: user@ap40 $ tar -cvzf <COMPRESSED FILENAME> <LIST OF FILES OR DIRECTORIES> Replacing <COMPRESSED FILENAME> with the name of the tarball that you would like to create and <LIST OF FILES OR DIRECTORIES> with a space-separated list of files and/or directories that you want inside pdbaa_files.tar.gz. Move the resulting tarball to the blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our executable in the submit file, we'll also end up transferring the blastx executable with transfer_input_files . In the blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar -xzvf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* Also remember to make the script executable: chmod +x blast_wrapper.sh Extra Files! The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output since they're \"new\" files that HTCondor didn't transfer over as input.","title":"Implement file compression"},{"location":"materials/data/part1-ex2-file-transfer/#list-the-executable-and-input-files","text":"Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) In transfer_input_files , list the blastx binary, the pdbaa_files.tar.gz file, and the input query file. Commas, commas everywhere! Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script.","title":"List the executable and input files"},{"location":"materials/data/part1-ex2-file-transfer/#predict-memory-and-disk-requests-from-your-data","text":"Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: How much memory blastx would use if it loaded all of the database files and the query input file into memory. How much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). Whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from Software exercise ( 1.1 ), and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests. You may still want to request slightly more than the job actually used.","title":"Predict memory and disk requests from your data"},{"location":"materials/data/part1-ex2-file-transfer/#run-the-test-job","text":"Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from Software exercise ( 1.1 ), and why?","title":"Run the test job"},{"location":"materials/data/part1-ex2-file-transfer/#transfer_output_files","text":"So far, we have used HTCondor's new file detection to transfer back the newly created files. An alternative is to be explicit, using the transfer_output_files attribute in the submit file. The upside to this approach is that you can pick to only transfer back a subset of the created files. The downside is that you have to know which files are created. The first exercise is to modify the submit file from the previous example, and add a line like (remember, before the queue ): transfer_output_files = mouse.fa.result You may also remove the last line in the blast_wrapper.sh , the rm pdbaa.* as extra files are no longer an issue - those files will be ignored because we used transfer_output_files . Submit the job, and make sure everything works. Did you get any pdbaa.* files back? The next thing we should try is to see what happens if the file we specify does not exist. Modify your submit file, and change the transfer_output_files to: transfer_output_files = elephant.fa.result Submit the job and see how it behaves. Did it finish successfully?","title":"transfer_output_files"},{"location":"materials/data/part1-ex2-file-transfer/#transfer_output_remaps","text":"Related to transfer_output_files is transfer_output_remaps , which allows us to rename outputs, or map the outputs to a different storage system (will be explored in the next module). The format of the transfer_output_remaps attribute is a list of remaps, each remap taking the form of src=dst . The destination can be a local path, or a URL. For example: transfer_output_remaps = \"myresults.dat = s3://destination-server.com/myresults.dat\" If you have more than one remap, you can separate them with ; By now, your blast-data directory is probably starting to look messy with a mix of submit files, input data, log file and output data all intermingled. One improvement could be to map our outputs to a separate directory. Create a new directory named science-results . Add a transfer_output_remaps line to the submit file. It is common to place this line right after the transfer_output_files line. Change the transfer_output_files back to mouse.fa.result . Example: transfer_output_files = mouse.fa.result transfer_output_remaps = Fill out the remap line, mapping mouse.fa.result to the destination science-results/mouse.fa.result . Remember that the transfer_output_remaps value requires double quotes around it. Submit the job, and wait for it to complete. Are there any errors? Can you find mouse.fa.result?","title":"transfer_output_remaps"},{"location":"materials/data/part1-ex2-file-transfer/#conclusions","text":"In this exercise, you: Used your data requirements knowledge from the previous exercise to write a job. Executed the job on a remote worker node and took note of the data usage. Used transfer_input_files to transfer inputs Used transfer_output_files to transfer outputs Used transfer_output_remaps to map outputs to a different destination When you've completed the above, continue with the next exercise .","title":"Conclusions"},{"location":"materials/data/part1-ex3-blast-split/","text":"Data Exercise 1.3: Splitting Large Input for Better Throughput \u00b6 The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files. Setup \u00b6 Log in to ap40.uw.osg-htc.org Create a directory for this exercise named blast-split and change into it. Copy over the following files from the previous exercise : Your submit file blastx pdbaa_files.tar.gz blast_wrapper.sh Remember to modify the submit file for the new locations of the above files. Obtain the large input \u00b6 We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/mouse_rna.tar.gz ./ After un-tar'ing ( tar xzf mouse_rna.tar.gz ) the file, you should be able to confirm that it's size is roughly 154 MB. It would take hours to complete a single blastx analysis for it, and the resulting output file would be huge. Split the input file \u00b6 For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Because genetic sequence data is used heavily across the life sciences, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like BLAST) using the following command: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/gt-1.6.5-Linux_x86_64-64bit-complete.tar.gz ./ Un-tar the gt package ( tar -xzvf ... ), then run its sequence file splitter as follows, with the target file size of 1MB: user@ap40 $ ./gt-1.6.5-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa You'll notice that the result is a set of 154 files, all about the size of 1 MB, and numbered 1 through 154. Run a Jobs on Split Input \u00b6 Now, you'll submit jobs on the split input files, where each job will use a different piece of the large original input file. Modify the submit file \u00b6 First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file from the previous exercise to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.* Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Add the $(inputfile) to the end of your list of transfer_input_files : transfer_input_files = ... , $(inputfile) Remove or comment out transfer_output_files and transfer_output_remaps . Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. Modify the wrapper file \u00b6 Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result Note Bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc. Submit the jobs \u00b6 This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job. In our tests, the jobs ran for ~15 minutes. Jobs on jobs! Be careful to not submit the job again. Why? Our queue statement says ... matching mouse_rna.fa.* , and look at the current directory. There are new files named mouse_rna.fa.X.log and other files. Submitting again, the queue statement would see these new files, and try to run blast on them! If you want to remove all of the extra files, you can try: user@ap40 $ rm *.err *.log *.out *.result Update the resource requests \u00b6 After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file.","title":"1.3- Splitting input"},{"location":"materials/data/part1-ex3-blast-split/#data-exercise-13-splitting-large-input-for-better-throughput","text":"The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files.","title":"Data Exercise 1.3: Splitting Large Input for Better Throughput"},{"location":"materials/data/part1-ex3-blast-split/#setup","text":"Log in to ap40.uw.osg-htc.org Create a directory for this exercise named blast-split and change into it. Copy over the following files from the previous exercise : Your submit file blastx pdbaa_files.tar.gz blast_wrapper.sh Remember to modify the submit file for the new locations of the above files.","title":"Setup"},{"location":"materials/data/part1-ex3-blast-split/#obtain-the-large-input","text":"We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/mouse_rna.tar.gz ./ After un-tar'ing ( tar xzf mouse_rna.tar.gz ) the file, you should be able to confirm that it's size is roughly 154 MB. It would take hours to complete a single blastx analysis for it, and the resulting output file would be huge.","title":"Obtain the large input"},{"location":"materials/data/part1-ex3-blast-split/#split-the-input-file","text":"For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! On the other hand, BLAST databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database. Because genetic sequence data is used heavily across the life sciences, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like BLAST) using the following command: user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/gt-1.6.5-Linux_x86_64-64bit-complete.tar.gz ./ Un-tar the gt package ( tar -xzvf ... ), then run its sequence file splitter as follows, with the target file size of 1MB: user@ap40 $ ./gt-1.6.5-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa You'll notice that the result is a set of 154 files, all about the size of 1 MB, and numbered 1 through 154.","title":"Split the input file"},{"location":"materials/data/part1-ex3-blast-split/#run-a-jobs-on-split-input","text":"Now, you'll submit jobs on the split input files, where each job will use a different piece of the large original input file.","title":"Run a Jobs on Split Input"},{"location":"materials/data/part1-ex3-blast-split/#modify-the-submit-file","text":"First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file from the previous exercise to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.* Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Add the $(inputfile) to the end of your list of transfer_input_files : transfer_input_files = ... , $(inputfile) Remove or comment out transfer_output_files and transfer_output_remaps . Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each.","title":"Modify the submit file"},{"location":"materials/data/part1-ex3-blast-split/#modify-the-wrapper-file","text":"Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result Note Bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc.","title":"Modify the wrapper file"},{"location":"materials/data/part1-ex3-blast-split/#submit-the-jobs","text":"This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job. In our tests, the jobs ran for ~15 minutes. Jobs on jobs! Be careful to not submit the job again. Why? Our queue statement says ... matching mouse_rna.fa.* , and look at the current directory. There are new files named mouse_rna.fa.X.log and other files. Submitting again, the queue statement would see these new files, and try to run blast on them! If you want to remove all of the extra files, you can try: user@ap40 $ rm *.err *.log *.out *.result","title":"Submit the jobs"},{"location":"materials/data/part1-ex3-blast-split/#update-the-resource-requests","text":"After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file.","title":"Update the resource requests"},{"location":"materials/data/part2-ex1-osdf-inputs/","text":"Data Exercise 2.1: Using OSDF for Large Shared Data \u00b6 This exercise will use a BLAST workflow to demonstrate the functionality of OSDF for transferring input files to jobs on OSG. Because our individual blast jobs from previous exercises would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for transfer_input_files (larger than ~1 GB). For this exercise, we will use the same inputs, but instead of using transfer_input_files for the pdbaa database, we will place it in OSDF and have the jobs download from there. OSDF is connected to a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed. Setup \u00b6 Make sure you're logged in to ap40.uw.osg-htc.org Copy the following files from the previous Blast exercises to a new directory in /home/<username> called osdf-shared : blast_wrapper.sh mouse_rna.fa.1 mouse_rna.fa.2 mouse_rna.fa.3 Your most recent submit file (probably named blast_split.sub ) For this exercise, we will use a container of the blastx named blastx.sif . Download the container using the following command user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/blast.sif . Place the Database and Container in OSDF \u00b6 The image of the software ( blast.sif ) will be used frequently and it needs to be copied to the OSDF directory. Details about the OSDF directory is given below. Copy to your data to the OSDF space \u00b6 OSDF provides a directory for you to store data which can be accessed through the caching servers. First, you need to move your BLAST database ( pdbaa_files.tar.gz ) and BLAST container ( blast.sif ) into this directory. For ap40.uw.osg-htc.org , the directory to use is /ospool/ap40/data/[USERNAME]/ Note that files placed in the /ospool/ap40/data/[USERNAME]/ directory will only be accessible by your own jobs. Modify the Submit File and Wrapper \u00b6 You will have to modify the wrapper and submit file to use OSDF: For using the container, add the following line to your submit file container_image = osdf:///ospool/ap40/data/[USERNAME]/blast.sif HTCondor knows how to do OSDF transfers, so you just have to provide the correct URL in transfer_input_files . Note there is no servername (3 slashes in :///) and instead it is just based on namespace ( /ospool/ap40 in this case): transfer_input_files = $(inputfile), osdf:///ospool/ap40/data/[USERNAME]/pdbaa_files.tar.gz Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied a few them from the previous exercise directory). Submit the Job \u00b6 Now submit and monitor the job! If your 154 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise! Considerations \u00b6 Why did we not place all files in OSDF? What do you think will happen if you make changes to pdbaa_files.tar.gz ? Will the caches be updated automatically, or is there a possiblility that the old version of pdbaa_files.tar.gz will be served up to jobs? What is the solution to this problem? (Hint: OSDF only considers the filename when caching data) Note: Keeping OSDF 'Clean' \u00b6 Just as for any data directory, it is VERY important to remove old files from OSDF when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from /ospool/ap40/data/[USERNAME]/ on when you don't need them there anymore, but only after all jobs have finished. The next time you use OSDF after the school, remember to first check for old files that you can delete.","title":"2.1 - OSDF for inputs"},{"location":"materials/data/part2-ex1-osdf-inputs/#data-exercise-21-using-osdf-for-large-shared-data","text":"This exercise will use a BLAST workflow to demonstrate the functionality of OSDF for transferring input files to jobs on OSG. Because our individual blast jobs from previous exercises would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for transfer_input_files (larger than ~1 GB). For this exercise, we will use the same inputs, but instead of using transfer_input_files for the pdbaa database, we will place it in OSDF and have the jobs download from there. OSDF is connected to a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed.","title":"Data Exercise 2.1: Using OSDF for Large Shared Data"},{"location":"materials/data/part2-ex1-osdf-inputs/#setup","text":"Make sure you're logged in to ap40.uw.osg-htc.org Copy the following files from the previous Blast exercises to a new directory in /home/<username> called osdf-shared : blast_wrapper.sh mouse_rna.fa.1 mouse_rna.fa.2 mouse_rna.fa.3 Your most recent submit file (probably named blast_split.sub ) For this exercise, we will use a container of the blastx named blastx.sif . Download the container using the following command user@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/blast.sif .","title":"Setup"},{"location":"materials/data/part2-ex1-osdf-inputs/#place-the-database-and-container-in-osdf","text":"The image of the software ( blast.sif ) will be used frequently and it needs to be copied to the OSDF directory. Details about the OSDF directory is given below.","title":"Place the Database and Container in OSDF"},{"location":"materials/data/part2-ex1-osdf-inputs/#copy-to-your-data-to-the-osdf-space","text":"OSDF provides a directory for you to store data which can be accessed through the caching servers. First, you need to move your BLAST database ( pdbaa_files.tar.gz ) and BLAST container ( blast.sif ) into this directory. For ap40.uw.osg-htc.org , the directory to use is /ospool/ap40/data/[USERNAME]/ Note that files placed in the /ospool/ap40/data/[USERNAME]/ directory will only be accessible by your own jobs.","title":"Copy to your data to the OSDF space"},{"location":"materials/data/part2-ex1-osdf-inputs/#modify-the-submit-file-and-wrapper","text":"You will have to modify the wrapper and submit file to use OSDF: For using the container, add the following line to your submit file container_image = osdf:///ospool/ap40/data/[USERNAME]/blast.sif HTCondor knows how to do OSDF transfers, so you just have to provide the correct URL in transfer_input_files . Note there is no servername (3 slashes in :///) and instead it is just based on namespace ( /ospool/ap40 in this case): transfer_input_files = $(inputfile), osdf:///ospool/ap40/data/[USERNAME]/pdbaa_files.tar.gz Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied a few them from the previous exercise directory).","title":"Modify the Submit File and Wrapper"},{"location":"materials/data/part2-ex1-osdf-inputs/#submit-the-job","text":"Now submit and monitor the job! If your 154 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise!","title":"Submit the Job"},{"location":"materials/data/part2-ex1-osdf-inputs/#considerations","text":"Why did we not place all files in OSDF? What do you think will happen if you make changes to pdbaa_files.tar.gz ? Will the caches be updated automatically, or is there a possiblility that the old version of pdbaa_files.tar.gz will be served up to jobs? What is the solution to this problem? (Hint: OSDF only considers the filename when caching data)","title":"Considerations"},{"location":"materials/data/part2-ex1-osdf-inputs/#note-keeping-osdf-clean","text":"Just as for any data directory, it is VERY important to remove old files from OSDF when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from /ospool/ap40/data/[USERNAME]/ on when you don't need them there anymore, but only after all jobs have finished. The next time you use OSDF after the school, remember to first check for old files that you can delete.","title":"Note: Keeping OSDF 'Clean'"},{"location":"materials/data/part2-ex2-osdf-outputs/","text":"Data Exercise 2.2: Using OSDF for outputs \u00b6 This exercise will use a Minimap2 workflow to demonstrate the functionality of OSDF for transferring input files to jobs on OSG. As presented in the Data lecture , for submitting multiple jobs that require larger files, OSDF not only reduces the load on the OSPool network but also keeps a copy of the files in different caching sites, which results in faster transfer of files to the Execution Point. Setup \u00b6 Please make sure that you are logged into ap40.uw.osg-htc.org . Create a folder named minimap2-read and cd into the directory Download the required files for this lesson using the following command osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif . osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.fa . File Sizes What are the sizes of the container and the reference genome file? Prepare files for the job \u00b6 Minimap2 compares and aligns large pieces of genetic information, like DNA sequences. This is useful for tasks like identifying similarities between genomes, verifying sequences, or assembling genetic data from scratch. For this exercise, we will perform indexing using the reference genome. Place files in OSDF \u00b6 There are some files we will be using frequently that do not change often. One example of this is the apptainer/singularity container image we will be using for run our minimap2 mappings. The Open Science Data Federation is a data lake accessible to the OSPool with built in caching. The OSDF can significantly improve throughput for jobs by caching files closer to the execution points. [!WARNING] The OSDF caches files aggressively. Using files on the OSDF with names that are not unique from previous versions can cause your job to download an incorrect previous version of the data file. We recommend using unique version-controlled names for your files, such as data_file_04JAN2025_version4.txt with the data of last update and a version identifier. This ensures your files are correctly called by HTCondor from the OSDF. Copy your data to the OSDF space \u00b6 OSDF provides a directory for you to store data which can be accessed through the caching servers. First, you need to move your minimap2.sif image. For ap40.uw.osg-htc.org , the directory to use is /ospool/ap40/data/[USERNAME]/ Note that files placed in the /ospool/ap40/data/[USERNAME]/ directory will only be accessible by your own jobs. Second, the exercise will use a reference genome file named Celegans_ref.mmi to map against the FASTQ files. Copy the Celegans_ref.mmi file to your OSDF directory as well. Considerations \u00b6 Why minimap2.sif is placed in the OSDF directory? Is it a big container? Create a wrapper script and submit jobs \u00b6 We will need our wrapper script and submit file to use OSDF. Perform the following steps. Copy the following contents and save that as minimap2_index.sh 1 2 #!/bin/bash minimap2 -x map-ont -d Celegans_ref.mmi Celegans_ref.fa Create minimap2_index.sub using either vim or nano container_image = osdf:///ospool/ap40/data/[USERNAME]/minimap2.sif executable = ./minimap2_index.sh transfer_input_files = Celegans_ref.fa transfer_output_files = Celegans_ref.mmi transfer_output_remaps = \"Celegans_ref.mmi = osdf:///ospool/ap40/data/[USERNAME]/Celegans_ref.mmi\" output = indexing_step1_$(Cluster)_$(Process).out error = indexing_step1_$(Cluster)_$(Process).err log = indexing_step1_$(Cluster)_$(Process).log request_cpus = 4 request_disk = 5 GB request_memory = 5 GB queue 1 [!IMPORTANT] Notice that we are using the transfer_output_remaps attribute in our submit file. By default, HTCondor will transfer outputs to the directory where we submitted our job from. Since we want to transfer the indexed reference genome file Celegans_ref.mmi to a specific directory, we can use the transfer_output_remaps attribute on our submission script. The syntax of this attribute is: transfer_output_remaps = \"<file_on_execution_point>=<desired_path_to_file_on_access_point> It is also important to note that we are transferring our Celegans_ref.mmi to the OSDF directory /ospool/<ap##>/data/<user.name>/ . We will be reusing our indexed reference genome file for each mapping job in the ScalingUp exercises, we benefit from the caching feature of the OSDF. Therefore, we can direct transfer_output_remaps to redirect the Celegans_ref.mmi file to our OSDF directory. Submit your minimap2_index.sub job to the OSPool condor_submit minimap2_index.sub","title":"2.2 - OSDF for outputs"},{"location":"materials/data/part2-ex2-osdf-outputs/#data-exercise-22-using-osdf-for-outputs","text":"This exercise will use a Minimap2 workflow to demonstrate the functionality of OSDF for transferring input files to jobs on OSG. As presented in the Data lecture , for submitting multiple jobs that require larger files, OSDF not only reduces the load on the OSPool network but also keeps a copy of the files in different caching sites, which results in faster transfer of files to the Execution Point.","title":"Data Exercise 2.2: Using OSDF for outputs"},{"location":"materials/data/part2-ex2-osdf-outputs/#setup","text":"Please make sure that you are logged into ap40.uw.osg-htc.org . Create a folder named minimap2-read and cd into the directory Download the required files for this lesson using the following command osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif . osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.fa . File Sizes What are the sizes of the container and the reference genome file?","title":"Setup"},{"location":"materials/data/part2-ex2-osdf-outputs/#prepare-files-for-the-job","text":"Minimap2 compares and aligns large pieces of genetic information, like DNA sequences. This is useful for tasks like identifying similarities between genomes, verifying sequences, or assembling genetic data from scratch. For this exercise, we will perform indexing using the reference genome.","title":"Prepare files for the job"},{"location":"materials/data/part2-ex2-osdf-outputs/#place-files-in-osdf","text":"There are some files we will be using frequently that do not change often. One example of this is the apptainer/singularity container image we will be using for run our minimap2 mappings. The Open Science Data Federation is a data lake accessible to the OSPool with built in caching. The OSDF can significantly improve throughput for jobs by caching files closer to the execution points. [!WARNING] The OSDF caches files aggressively. Using files on the OSDF with names that are not unique from previous versions can cause your job to download an incorrect previous version of the data file. We recommend using unique version-controlled names for your files, such as data_file_04JAN2025_version4.txt with the data of last update and a version identifier. This ensures your files are correctly called by HTCondor from the OSDF.","title":"Place files in OSDF"},{"location":"materials/data/part2-ex2-osdf-outputs/#copy-your-data-to-the-osdf-space","text":"OSDF provides a directory for you to store data which can be accessed through the caching servers. First, you need to move your minimap2.sif image. For ap40.uw.osg-htc.org , the directory to use is /ospool/ap40/data/[USERNAME]/ Note that files placed in the /ospool/ap40/data/[USERNAME]/ directory will only be accessible by your own jobs. Second, the exercise will use a reference genome file named Celegans_ref.mmi to map against the FASTQ files. Copy the Celegans_ref.mmi file to your OSDF directory as well.","title":"Copy your data to the OSDF space"},{"location":"materials/data/part2-ex2-osdf-outputs/#considerations","text":"Why minimap2.sif is placed in the OSDF directory? Is it a big container?","title":"Considerations"},{"location":"materials/data/part2-ex2-osdf-outputs/#create-a-wrapper-script-and-submit-jobs","text":"We will need our wrapper script and submit file to use OSDF. Perform the following steps. Copy the following contents and save that as minimap2_index.sh 1 2 #!/bin/bash minimap2 -x map-ont -d Celegans_ref.mmi Celegans_ref.fa Create minimap2_index.sub using either vim or nano container_image = osdf:///ospool/ap40/data/[USERNAME]/minimap2.sif executable = ./minimap2_index.sh transfer_input_files = Celegans_ref.fa transfer_output_files = Celegans_ref.mmi transfer_output_remaps = \"Celegans_ref.mmi = osdf:///ospool/ap40/data/[USERNAME]/Celegans_ref.mmi\" output = indexing_step1_$(Cluster)_$(Process).out error = indexing_step1_$(Cluster)_$(Process).err log = indexing_step1_$(Cluster)_$(Process).log request_cpus = 4 request_disk = 5 GB request_memory = 5 GB queue 1 [!IMPORTANT] Notice that we are using the transfer_output_remaps attribute in our submit file. By default, HTCondor will transfer outputs to the directory where we submitted our job from. Since we want to transfer the indexed reference genome file Celegans_ref.mmi to a specific directory, we can use the transfer_output_remaps attribute on our submission script. The syntax of this attribute is: transfer_output_remaps = \"<file_on_execution_point>=<desired_path_to_file_on_access_point> It is also important to note that we are transferring our Celegans_ref.mmi to the OSDF directory /ospool/<ap##>/data/<user.name>/ . We will be reusing our indexed reference genome file for each mapping job in the ScalingUp exercises, we benefit from the caching feature of the OSDF. Therefore, we can direct transfer_output_remaps to redirect the Celegans_ref.mmi file to our OSDF directory. Submit your minimap2_index.sub job to the OSPool condor_submit minimap2_index.sub","title":"Create a wrapper script and submit jobs"},{"location":"materials/htcondor/part1-ex1-login/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.1: Log In and Look Around \u00b6 Background \u00b6 There are different High Throughput Computing (HTC) systems at universities, government facilities, and other institutions around the world, and they may have different user experiences. For example, some systems have dedicated resources (which means your job will be guaranteed a certain amount of resources/time to complete), while other systems have opportunistic, backfill resources (which means your job can take advantage of some resources, but those resources could be removed at any time). Other systems have a mix of dedicated and opportunistic resources. During the OSG School, you will mostly practice on the \" OSG's Open Science Pool (OSPool) \". The OSPool provides researchers with opportunistic resources and the ability to run many smaller and shorter jobs simultaneously . The OSPool is composed of approximately 60,000+ cores and dozens of different GPUs. Exercise Goal \u00b6 The goal of this first exercise is to log in to the OSPool Access Point and look around a little bit, which will take only a few minutes. If you have trouble logging in to the Access Point, ask the instructors right away! Gaining access is critical for all remaining exercises. Logging In \u00b6 You will log in to an OSPool Access Point, ap40.uw.osg-htc.org , using the username assigned to you. To log in, use a Secure Shell (SSH) client. On Windows, you can use Terminal (Powershell or WSL), MobaXTerm , PuTTY , VSCode, or any other SSH client. On Mac or Linux, you may use the Terminal application or any other SSH client. To log in, run the ssh command as shown below, replacing <USERNAME> with your username. Do not include the brackets ( <> ). ssh <USERNAME>@ap40.uw.osg-htc.org If you need help finding or using an SSH client, ask the instructors for help right away ! Running Commands \u00b6 In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: [username@ap40 ~]$ hostname ospool-ap2140 Note In the first line of the example above, the [username@ap40 ~]$ part is meant to show the Linux command-line prompt. You do not type this part! So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try (the examples below do not show the output from each command): [username@ap40 ~]$ whoami [username@ap40 ~]$ date [username@ap40 ~]$ uname -a Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, but you will learn more if you take the time to type each command yourself. Organizing Your Workspace \u00b6 You will be doing many different exercises over the next few days, many of them on this Access Point. Each exercise may use or generate many files. To avoid confusion and stay organized, it is useful to create a separate directory for each exercise. Additionally, directories that contain numerous files can be reduce system performance. The exact number varies, depending on the file system type. To create a directory, use the mkdir command. We can change directories with the cd command, as shown below. [username@ap40 ~]$ mkdir intro-1.1-login [username@ap40 ~]$ cd intro-1.1-login To return to the directory above the intro-1.1-login directory, use the cd command with two dots ( .. ). [username@ap40 intro-1.1-login]$ cd .. [username@ap40 ~]$ The meaning of .. , . , and ~ \u00b6 Sometimes, you'll see .. , . , and ~ in commands and paths. These symbols represent different paths in the system. Two dots ( .. ) represent the directory above the current directory. One dot ( . ) represents the current directory. The tilde symbol ( ~ ) represents your home directory ( /home/username ). Showing the Version of HTCondor \u00b6 HTCondor is installed on this server. But what version? You can ask HTCondor itself: [user@ap40 ~]$ condor_version $ CondorVersion: 24 .7.3 2025 -04-17 BuildID: 802763 PackageID: 24 .7.3-0.802763 GitSHA: bb5d9294 RC $ $ CondorPlatform: x86_64_AlmaLinux9 $ Do you see a different version? If so, why might it be different? Reference Materials \u00b6 Here are a few links to reference materials that will be useful on your journey in research computing! Software Carpentry Unix Shell Lesson . Are you new to the command line or need a refresher? This lesson is a good place to start learning about the Unix shell, including how to navigate directories, create files, and write simple scripts. HTCondor manual . We recommend reading the manual corresponding to the version of HTCondor that you use. This link points to the latest version of the manual, but you can switch versions using the toggle in the lower left corner of that page.","title":"1.1 - Log in and look around"},{"location":"materials/htcondor/part1-ex1-login/#htc-exercise-11-log-in-and-look-around","text":"","title":"HTC Exercise 1.1: Log In and Look Around"},{"location":"materials/htcondor/part1-ex1-login/#background","text":"There are different High Throughput Computing (HTC) systems at universities, government facilities, and other institutions around the world, and they may have different user experiences. For example, some systems have dedicated resources (which means your job will be guaranteed a certain amount of resources/time to complete), while other systems have opportunistic, backfill resources (which means your job can take advantage of some resources, but those resources could be removed at any time). Other systems have a mix of dedicated and opportunistic resources. During the OSG School, you will mostly practice on the \" OSG's Open Science Pool (OSPool) \". The OSPool provides researchers with opportunistic resources and the ability to run many smaller and shorter jobs simultaneously . The OSPool is composed of approximately 60,000+ cores and dozens of different GPUs.","title":"Background"},{"location":"materials/htcondor/part1-ex1-login/#exercise-goal","text":"The goal of this first exercise is to log in to the OSPool Access Point and look around a little bit, which will take only a few minutes. If you have trouble logging in to the Access Point, ask the instructors right away! Gaining access is critical for all remaining exercises.","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex1-login/#logging-in","text":"You will log in to an OSPool Access Point, ap40.uw.osg-htc.org , using the username assigned to you. To log in, use a Secure Shell (SSH) client. On Windows, you can use Terminal (Powershell or WSL), MobaXTerm , PuTTY , VSCode, or any other SSH client. On Mac or Linux, you may use the Terminal application or any other SSH client. To log in, run the ssh command as shown below, replacing <USERNAME> with your username. Do not include the brackets ( <> ). ssh <USERNAME>@ap40.uw.osg-htc.org If you need help finding or using an SSH client, ask the instructors for help right away !","title":"Logging In"},{"location":"materials/htcondor/part1-ex1-login/#running-commands","text":"In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: [username@ap40 ~]$ hostname ospool-ap2140 Note In the first line of the example above, the [username@ap40 ~]$ part is meant to show the Linux command-line prompt. You do not type this part! So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try (the examples below do not show the output from each command): [username@ap40 ~]$ whoami [username@ap40 ~]$ date [username@ap40 ~]$ uname -a Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, but you will learn more if you take the time to type each command yourself.","title":"Running Commands"},{"location":"materials/htcondor/part1-ex1-login/#organizing-your-workspace","text":"You will be doing many different exercises over the next few days, many of them on this Access Point. Each exercise may use or generate many files. To avoid confusion and stay organized, it is useful to create a separate directory for each exercise. Additionally, directories that contain numerous files can be reduce system performance. The exact number varies, depending on the file system type. To create a directory, use the mkdir command. We can change directories with the cd command, as shown below. [username@ap40 ~]$ mkdir intro-1.1-login [username@ap40 ~]$ cd intro-1.1-login To return to the directory above the intro-1.1-login directory, use the cd command with two dots ( .. ). [username@ap40 intro-1.1-login]$ cd .. [username@ap40 ~]$","title":"Organizing Your Workspace"},{"location":"materials/htcondor/part1-ex1-login/#the-meaning-of-and","text":"Sometimes, you'll see .. , . , and ~ in commands and paths. These symbols represent different paths in the system. Two dots ( .. ) represent the directory above the current directory. One dot ( . ) represents the current directory. The tilde symbol ( ~ ) represents your home directory ( /home/username ).","title":"The meaning of .., ., and ~"},{"location":"materials/htcondor/part1-ex1-login/#showing-the-version-of-htcondor","text":"HTCondor is installed on this server. But what version? You can ask HTCondor itself: [user@ap40 ~]$ condor_version $ CondorVersion: 24 .7.3 2025 -04-17 BuildID: 802763 PackageID: 24 .7.3-0.802763 GitSHA: bb5d9294 RC $ $ CondorPlatform: x86_64_AlmaLinux9 $ Do you see a different version? If so, why might it be different?","title":"Showing the Version of HTCondor"},{"location":"materials/htcondor/part1-ex1-login/#reference-materials","text":"Here are a few links to reference materials that will be useful on your journey in research computing! Software Carpentry Unix Shell Lesson . Are you new to the command line or need a refresher? This lesson is a good place to start learning about the Unix shell, including how to navigate directories, create files, and write simple scripts. HTCondor manual . We recommend reading the manual corresponding to the version of HTCondor that you use. This link points to the latest version of the manual, but you can switch versions using the toggle in the lower left corner of that page.","title":"Reference Materials"},{"location":"materials/htcondor/part1-ex2-commands/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.2: Experiment With HTCondor Commands \u00b6 Exercise Goal \u00b6 Before you run work on a High Throughput Computing system, it's useful to know what resources are available and how to view jobs. The goal of this exercise is to learn about two HTCondor commands: condor_q and condor_status . These commands are useful for monitoring your jobs and viewing available Execution Point slots throughout the week. Viewing Slots \u00b6 As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is: [username@ap40 ~]$ condor_status When running this command, there is typically a lot of output printed to the screen. Looking at your terminal output, there is one line per Execution Point slot. Tip You can widen your terminal window or zoom out, which may help you to see all details of the output better. Since there are so many entries, you can also limit the number of slots displayed with condor_status -limit 5 . Here is some example output (what you see will be longer): Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1_15@IU-Jetstream2-Backfill.e53d75d92d0e LINUX X86_64 Claimed Busy 0.000 2048 0+00:00:08 slot1_7@IU-Jetstream2-Backfill.f7d8ef98fdd0 LINUX X86_64 Claimed Busy 1.000 8192 0+04:57:31 slot1@glidein_2_813658338@e4014.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 14 0+05:35:29 slot1@glidein_608506_86504717@node063.lawrence LINUX X86_64 Unclaimed Idle 0.000 472 0+15:59:56 slot1_4@glidein_3651085_902238852@uct2-c576.mwt2.org LINUX X86_64 Claimed Busy 4.470 2048 0+05:10:48 This output consists of 8 columns: Column Example Meaning Name slot1_4@glidein_3651085_902238852@uct2-c576.mwt2.org Full slot name (including the hostname) OpSys LINUX Operating system Arch X86_64 Slot architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 4.470 Load average, a measure of CPU activity on the slot Mem 2048 Memory available to the slot, in MB ActvtyTime 0+05:10:48 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Total Owner Claimed Unclaimed Matched Preempting Drain Backfill BkIdle X86_64/LINUX 54466 0 47991 6313 0 162 0 0 0 aarch64/LINUX 1 0 0 1 0 0 0 0 0 Total 54467 0 47991 6314 0 162 0 0 0 There is one row of summary for each machine (i.e., \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool. Questions \u00b6 When you run condor_status , how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) What percent of the total slots are currently claimed by researcher jobs? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.) How have these numbers changed (if at all) when you run the condor_status command again? Viewing Researcher Jobs \u00b6 The condor_q command lists jobs that were submitted from the Access Point and are running or waiting to run. The _q part of the name is an abbreviation of \u201cqueue\u201d, or the list of jobs waiting to finish. Viewing Your Own Jobs \u00b6 The default behavior of the command lists only your jobs: [username@ap40 ~]$ condor_q The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it might look something like the below: -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... @ 04/28/25 13:54:57 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice CMD: run_ffmpeg.sh 4/28 09:58 _ _ 1 1 18801.0 This output consists of 9 columns: Column Example Meaning OWNER alice The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The \"jobbatchname\" specified within the submit file. If not specified, it's given the value of ID: <ClusterID> SUBMITTED 4/28 09:58 The date and time when the job(s) was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch with the format <ClusterID>.<ProcessID> At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell? Viewing Everyone\u2019s Jobs \u00b6 By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: [username@ap40 ~]$ condor_q -all How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now? Viewing Jobs without the Default \"batch\" Mode \u00b6 The condor_q output, by default, groups \"batches\" of jobs together (i.e., if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the -nobatch option to condor_q : [username@ap40 ~]$ condor_q -all -nobatch How has the column information changed? (Below is an example of the top of the output.) -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... @ 04/28/25 13:57:40 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 4/28 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 4/28 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 alice 4/28 09:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 4/28 10:59 0+00:00:32 R 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 4/28 11:06 0+00:09:20 R 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER alice The user ID of the user who submitted the job SUBMITTED 4/28 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs. Extra Information \u00b6 Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in future exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h . Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web As a simple exercise, use one of the above reference methods to learn about the -limit option and implement it in a condor_q command.","title":"1.2 - Experiment with HTCondor commands"},{"location":"materials/htcondor/part1-ex2-commands/#htc-exercise-12-experiment-with-htcondor-commands","text":"","title":"HTC Exercise 1.2: Experiment With HTCondor Commands"},{"location":"materials/htcondor/part1-ex2-commands/#exercise-goal","text":"Before you run work on a High Throughput Computing system, it's useful to know what resources are available and how to view jobs. The goal of this exercise is to learn about two HTCondor commands: condor_q and condor_status . These commands are useful for monitoring your jobs and viewing available Execution Point slots throughout the week.","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-slots","text":"As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is: [username@ap40 ~]$ condor_status When running this command, there is typically a lot of output printed to the screen. Looking at your terminal output, there is one line per Execution Point slot. Tip You can widen your terminal window or zoom out, which may help you to see all details of the output better. Since there are so many entries, you can also limit the number of slots displayed with condor_status -limit 5 . Here is some example output (what you see will be longer): Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1_15@IU-Jetstream2-Backfill.e53d75d92d0e LINUX X86_64 Claimed Busy 0.000 2048 0+00:00:08 slot1_7@IU-Jetstream2-Backfill.f7d8ef98fdd0 LINUX X86_64 Claimed Busy 1.000 8192 0+04:57:31 slot1@glidein_2_813658338@e4014.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 14 0+05:35:29 slot1@glidein_608506_86504717@node063.lawrence LINUX X86_64 Unclaimed Idle 0.000 472 0+15:59:56 slot1_4@glidein_3651085_902238852@uct2-c576.mwt2.org LINUX X86_64 Claimed Busy 4.470 2048 0+05:10:48 This output consists of 8 columns: Column Example Meaning Name slot1_4@glidein_3651085_902238852@uct2-c576.mwt2.org Full slot name (including the hostname) OpSys LINUX Operating system Arch X86_64 Slot architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 4.470 Load average, a measure of CPU activity on the slot Mem 2048 Memory available to the slot, in MB ActvtyTime 0+05:10:48 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Total Owner Claimed Unclaimed Matched Preempting Drain Backfill BkIdle X86_64/LINUX 54466 0 47991 6313 0 162 0 0 0 aarch64/LINUX 1 0 0 1 0 0 0 0 0 Total 54467 0 47991 6314 0 162 0 0 0 There is one row of summary for each machine (i.e., \"slot\") architecture/operating system combination with columns for the number of slots in each state. The final row gives a summary of slot states for the whole pool.","title":"Viewing Slots"},{"location":"materials/htcondor/part1-ex2-commands/#questions","text":"When you run condor_status , how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) What percent of the total slots are currently claimed by researcher jobs? (Note: there is a rapid turnover of slots, which is what allows users with new submission to have jobs start quickly.) How have these numbers changed (if at all) when you run the condor_status command again?","title":"Questions"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-researcher-jobs","text":"The condor_q command lists jobs that were submitted from the Access Point and are running or waiting to run. The _q part of the name is an abbreviation of \u201cqueue\u201d, or the list of jobs waiting to finish.","title":"Viewing Researcher Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-your-own-jobs","text":"The default behavior of the command lists only your jobs: [username@ap40 ~]$ condor_q The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one set (\"batch\") of submitted jobs per line. If you had a single job in the queue, it might look something like the below: -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... @ 04/28/25 13:54:57 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS alice CMD: run_ffmpeg.sh 4/28 09:58 _ _ 1 1 18801.0 This output consists of 9 columns: Column Example Meaning OWNER alice The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The \"jobbatchname\" specified within the submit file. If not specified, it's given the value of ID: <ClusterID> SUBMITTED 4/28 09:58 The date and time when the job(s) was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch with the format <ClusterID>.<ProcessID> At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell?","title":"Viewing Your Own Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-everyones-jobs","text":"By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: [username@ap40 ~]$ condor_q -all How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now?","title":"Viewing Everyone\u2019s Jobs"},{"location":"materials/htcondor/part1-ex2-commands/#viewing-jobs-without-the-default-batch-mode","text":"The condor_q output, by default, groups \"batches\" of jobs together (i.e., if they were submitted with the same submit file or \"jobbatchname\"). To see more information for EVERY job on a separate line of output, use the -nobatch option to condor_q : [username@ap40 ~]$ condor_q -all -nobatch How has the column information changed? (Below is an example of the top of the output.) -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... @ 04/28/25 13:57:40 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 4/28 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 4/28 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 alice 4/28 09:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 4/28 10:59 0+00:00:32 R 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 4/28 11:06 0+00:09:20 R 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER alice The user ID of the user who submitted the job SUBMITTED 4/28 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs.","title":"Viewing Jobs without the Default \"batch\" Mode"},{"location":"materials/htcondor/part1-ex2-commands/#extra-information","text":"Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in future exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h . Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web As a simple exercise, use one of the above reference methods to learn about the -limit option and implement it in a condor_q command.","title":"Extra Information"},{"location":"materials/htcondor/part1-ex3-jobs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.3: Run Jobs! \u00b6 Exercise Goal \u00b6 Now that we've seen what resources are available on the OSPool, we're going to start submitting our own jobs! The goal of this exercise is to transform a list of computational tasks into a format that HTCondor understands. This is done by writing job submit files. You can give (\"submit\") these files to HTCondor, which then has the responsibility for running them on available capacity. Learning to write submit files is a huge step in learning to use an HTC system! This exercise will take longer time than the first two, short ones. If you are having any problems getting the jobs to run, please ask the instructors! It is critical that you know how to run jobs. A List of Computational Tasks \u00b6 Imagine that you're a researcher who wants to collect information about the slots in the OSPool. You decide to do this by writing a script that collects information about the current machine into a CSV file. Below is the script you write: 1 2 3 4 #!/bin/bash echo \"Date,Hostname,System,Directory,OSG Site Name\" echo \" $( date ) , $( hostname ) , $( uname -spo ) , $( pwd ) , ${ OSG_SITE_NAME } \" (You don't need to understand everything the script is doing to complete the exercise.) Since this script is not computationally heavy, we can test this script on the Access Point. If you cannot run your script from the command line, HTCondor probably cannot run it on another machine. It's important to test your code before you submit it as a job, but be aware that you should never run compute-heavy code directly on the Access Point. Create a directory on ap40 for this exercise using the mkdir command. In this new directory, save the script above as slotinfo.sh using your favorite command line text editor (i.e. nano , vim , emacs ). With the chmod command, change the permissions of the script so it's executable. [username@ap40 htc1-3]$ chmod +x slotinfo.sh Execute the script and observe the output printed to the screen. [username@ap40 htc1-3]$ ./slotinfo.sh What information is printed? How is this information organized? Careful observers might note that we're missing the output of ${OSG_SITE_NAME} . This is an environment variable unique to jobs in the OSPool, but is not available on the Access Point. Running Your First List of Jobs \u00b6 Now that we've confirmed that our script works, we're ready to deploy a small-scale test on the OSPool! But to do so, we'll need to translate our task into a language that HTCondor understands - this \"translation\" is given through the job submit file. When you want to run HTCondor jobs, you will first need to write an HTCondor submit file for them. In this exercise, you will run the same slotinfo.sh script as we did above, but this time, the command will run within jobs on Execution Points (aka a slot) in the OSPool, instead of the Access Point. First, create a submit file called slotinfo.sub using your favorite text editor and then transfer the following information to that file: executable = slotinfo.sh output = slotinfo_$(Process).out error = slotinfo_$(Process).err log = slotinfo_$(Cluster).log request_cpus = 1 request_memory = 5 MB request_disk = 300 KB queue 50 Save your submit file using the name slotinfo.sub . Note You can name the HTCondor submit file using any filename. It's a good practice to always include the .sub extension, but it is not required. This is because the submit file is a simple text file that we are using to pass information to HTCondor. The lines of the submit file have the following meanings: Submit Command Explanation executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your jobs (messages printed to the screen). error The filename where HTCondor will write the standard error from your jobs (error messages printed to the screen). This particular jobs is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your jobs. While not required, it is best to have a log file for every job. request_* How many cpus , memory , and disk we want. In this case, we don't need much because the script is small and isn't computationally intensive. queue Tells HTCondor to run your jobs with the settings above. In this example, we will run 50 instances of the job. What are $(Process) and $(Cluster) ? Remember the job IDs we saw in the last exercises with condor_status and condor_q ? The $(Process) and $(Cluster) variables are used by HTCondor that are related to your jobs' IDs. Once you submit jobs, HTCondor will automatically populate these variables with the actual cluster and process numbers. Note that we are not using the arguments or transfer_input_files lines that were mentioned during lecture because we don't need any additional input files or options for our script. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your jobs with the condor_submit command: [username@ap40 ~]$ condor_submit slotinfo.sub Submitting job(s). 50 job(s) submitted to cluster NNNN. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your jobs to run. Instead, as soon as it has finished submitting your jobs into the queue, the submit command finishes. View your jobs in the queue \u00b6 Now, use condor_q and condor_q -nobatch to watch for your jobs in the queue! You may not even catch the jobs in the R running state, because the slotinfo.sh script runs very quickly. When the jobs are finished, it will 'leave' the queue and no longer be listed in the condor_q output. After the jobs finish, take a look at your working directory with ls -lh . You'll see some newly generated files, including .out , .err , and .log files. Let's check the contents of the .out files, which is where job information printed to the terminal screen will be printed for the jobs. [username@ap40 ~]$ cat slotinfo_0.out Date,Hostname,System,Directory,OSG Site Name Tue May 27 21:25:49 UTC 2025,osgvo-docker-pilot-7646558687-fqlp8,Linux x86_64 GNU/Linux,/srv,CHTC Since looking at each .out file individually is not efficient, we can concatenate the results in a .csv file with a few shell commands: [username@ap40 htc1-3]$ head -n 1 slotinfo_0.out > slotinfo_combined.csv [username@ap40 htc1-3]$ tail -q -n +2 slotinfo_*.out >> slotinfo_combined.csv Take a look at slotinfo_combined.csv . What information did we collect? What details stand out? Did the jobs execute around the same time or at different times? The .err file should be empty, unless there were issues running the slotinfo.sh executable after it was transferred to the slot. The .log file is more complex and will be the focus of a later exercise. Running Jobs With Arguments \u00b6 Very often, when you run a command on the command line, it includes arguments (i.e. options) after the program name. In an HTCondor submit file, the program (or 'executable') name goes in the executable statement and all remaining arguments go into an arguments statement. For example, if we have a script that multiplies all following numbers together: [username@ap40 ~]$ ./multipy.sh 1 2 3 6 Then in the submit file, we would put the \"multiply.sh\" program as the job executable , and 1 2 3 as the job arguments : executable = multiply.sh arguments = 1 2 3 Let\u2019s try a job submission with arguments. We will revisit our slotinfo.sh script. If you check the time zones of the outputs of each job, you may have a heterogenous mix of time zones\u2014some may be in UTC, some in CDT, EDT, MDT, PDT, etc. This isn't very helpful for comparison between jobs! Let's have our script take in a time zone as an argument so that all times are listed in that time zone. Create a new script called tz_slotinfo.sh with the following content: 1 2 3 4 5 6 7 8 9 #!/bin/bash if [ \" $# \" -ne 1 ] ; then echo \"Usage: $0 [time zone]\" exit fi echo \"Date,Hostname,System,Directory,OSG Site Name\" echo \" $( TZ = \" ${ 1 } \" date ) , $( hostname ) , $( uname -spo ) , $( pwd ) , ${ OSG_SITE_NAME } \" Make sure the script is executable, and test it using America/Los_Angeles as the argument. [username@ap40 htc1-3]$ ./tz_slotinfo.sh America/Los_Angeles Since we're in Wisconsin, we're going to use the America/Chicago time zone for our jobs. Create a new submit file and save the following text in it. executable = tz_slotinfo.sh arguments = America/Chicago output = tz_slotinfo_$(Process).out error = tz_slotinfo_$(Process).err log = tz_slotinfo_$(Cluster).log request_cpus = 1 request_memory = 5 MB request_disk = 300 KB queue 50 You can save the file using any name, but as a reminder, we recommend it uses the .sub file extension. Except for changing a few filenames, this submit file is nearly identical to the last one, except for the addition of the arguments line. Submit these new jobs to HTCondor. Again, watch for it to run using condor_q and optional arguments; check once every 15 seconds or so. When the jobs finish, it will disappear from the queue and create .out and .err files. After checking that there were no errors, concatenate the files into a new .csv file and view it. Did we get the time zone we expected for all jobs? Extra Challenge \u00b6 Note There are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session. Below is a Python script that does something similar to the shell script above. Write a submit file and submit a job that runs this Python script. #!/usr/bin/env python3 \"\"\"Extra Challenge for OSG School Written by Tim Cartwright \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len ( sys . argv ) > 1 : arguments = '\"' + ' ' . join ( sys . argv [ 1 :]) + '\"' print ( __doc__ , file = sys . stderr ) print ( 'Time :' , time . strftime ( '%Y-%m- %d ( %a ) %H:%M:%S %Z' )) print ( 'Host :' , getpass . getuser (), '@' , socket . gethostname ()) uname = platform . uname () print ( \"System :\" , uname [ 0 ], uname [ 2 ], uname [ 4 ]) print ( \"Version :\" , platform . python_version ()) print ( \"Program :\" , sys . executable ) print ( 'Script :' , os . path . abspath ( __file__ )) print ( 'Args :' , arguments )","title":"1.3 - Run jobs!"},{"location":"materials/htcondor/part1-ex3-jobs/#htc-exercise-13-run-jobs","text":"","title":"HTC Exercise 1.3: Run Jobs!"},{"location":"materials/htcondor/part1-ex3-jobs/#exercise-goal","text":"Now that we've seen what resources are available on the OSPool, we're going to start submitting our own jobs! The goal of this exercise is to transform a list of computational tasks into a format that HTCondor understands. This is done by writing job submit files. You can give (\"submit\") these files to HTCondor, which then has the responsibility for running them on available capacity. Learning to write submit files is a huge step in learning to use an HTC system! This exercise will take longer time than the first two, short ones. If you are having any problems getting the jobs to run, please ask the instructors! It is critical that you know how to run jobs.","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex3-jobs/#a-list-of-computational-tasks","text":"Imagine that you're a researcher who wants to collect information about the slots in the OSPool. You decide to do this by writing a script that collects information about the current machine into a CSV file. Below is the script you write: 1 2 3 4 #!/bin/bash echo \"Date,Hostname,System,Directory,OSG Site Name\" echo \" $( date ) , $( hostname ) , $( uname -spo ) , $( pwd ) , ${ OSG_SITE_NAME } \" (You don't need to understand everything the script is doing to complete the exercise.) Since this script is not computationally heavy, we can test this script on the Access Point. If you cannot run your script from the command line, HTCondor probably cannot run it on another machine. It's important to test your code before you submit it as a job, but be aware that you should never run compute-heavy code directly on the Access Point. Create a directory on ap40 for this exercise using the mkdir command. In this new directory, save the script above as slotinfo.sh using your favorite command line text editor (i.e. nano , vim , emacs ). With the chmod command, change the permissions of the script so it's executable. [username@ap40 htc1-3]$ chmod +x slotinfo.sh Execute the script and observe the output printed to the screen. [username@ap40 htc1-3]$ ./slotinfo.sh What information is printed? How is this information organized? Careful observers might note that we're missing the output of ${OSG_SITE_NAME} . This is an environment variable unique to jobs in the OSPool, but is not available on the Access Point.","title":"A List of Computational Tasks"},{"location":"materials/htcondor/part1-ex3-jobs/#running-your-first-list-of-jobs","text":"Now that we've confirmed that our script works, we're ready to deploy a small-scale test on the OSPool! But to do so, we'll need to translate our task into a language that HTCondor understands - this \"translation\" is given through the job submit file. When you want to run HTCondor jobs, you will first need to write an HTCondor submit file for them. In this exercise, you will run the same slotinfo.sh script as we did above, but this time, the command will run within jobs on Execution Points (aka a slot) in the OSPool, instead of the Access Point. First, create a submit file called slotinfo.sub using your favorite text editor and then transfer the following information to that file: executable = slotinfo.sh output = slotinfo_$(Process).out error = slotinfo_$(Process).err log = slotinfo_$(Cluster).log request_cpus = 1 request_memory = 5 MB request_disk = 300 KB queue 50 Save your submit file using the name slotinfo.sub . Note You can name the HTCondor submit file using any filename. It's a good practice to always include the .sub extension, but it is not required. This is because the submit file is a simple text file that we are using to pass information to HTCondor. The lines of the submit file have the following meanings: Submit Command Explanation executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your jobs (messages printed to the screen). error The filename where HTCondor will write the standard error from your jobs (error messages printed to the screen). This particular jobs is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your jobs. While not required, it is best to have a log file for every job. request_* How many cpus , memory , and disk we want. In this case, we don't need much because the script is small and isn't computationally intensive. queue Tells HTCondor to run your jobs with the settings above. In this example, we will run 50 instances of the job. What are $(Process) and $(Cluster) ? Remember the job IDs we saw in the last exercises with condor_status and condor_q ? The $(Process) and $(Cluster) variables are used by HTCondor that are related to your jobs' IDs. Once you submit jobs, HTCondor will automatically populate these variables with the actual cluster and process numbers. Note that we are not using the arguments or transfer_input_files lines that were mentioned during lecture because we don't need any additional input files or options for our script. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your jobs with the condor_submit command: [username@ap40 ~]$ condor_submit slotinfo.sub Submitting job(s). 50 job(s) submitted to cluster NNNN. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file or ask for help. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your jobs to run. Instead, as soon as it has finished submitting your jobs into the queue, the submit command finishes.","title":"Running Your First List of Jobs"},{"location":"materials/htcondor/part1-ex3-jobs/#view-your-jobs-in-the-queue","text":"Now, use condor_q and condor_q -nobatch to watch for your jobs in the queue! You may not even catch the jobs in the R running state, because the slotinfo.sh script runs very quickly. When the jobs are finished, it will 'leave' the queue and no longer be listed in the condor_q output. After the jobs finish, take a look at your working directory with ls -lh . You'll see some newly generated files, including .out , .err , and .log files. Let's check the contents of the .out files, which is where job information printed to the terminal screen will be printed for the jobs. [username@ap40 ~]$ cat slotinfo_0.out Date,Hostname,System,Directory,OSG Site Name Tue May 27 21:25:49 UTC 2025,osgvo-docker-pilot-7646558687-fqlp8,Linux x86_64 GNU/Linux,/srv,CHTC Since looking at each .out file individually is not efficient, we can concatenate the results in a .csv file with a few shell commands: [username@ap40 htc1-3]$ head -n 1 slotinfo_0.out > slotinfo_combined.csv [username@ap40 htc1-3]$ tail -q -n +2 slotinfo_*.out >> slotinfo_combined.csv Take a look at slotinfo_combined.csv . What information did we collect? What details stand out? Did the jobs execute around the same time or at different times? The .err file should be empty, unless there were issues running the slotinfo.sh executable after it was transferred to the slot. The .log file is more complex and will be the focus of a later exercise.","title":"View your jobs in the queue"},{"location":"materials/htcondor/part1-ex3-jobs/#running-jobs-with-arguments","text":"Very often, when you run a command on the command line, it includes arguments (i.e. options) after the program name. In an HTCondor submit file, the program (or 'executable') name goes in the executable statement and all remaining arguments go into an arguments statement. For example, if we have a script that multiplies all following numbers together: [username@ap40 ~]$ ./multipy.sh 1 2 3 6 Then in the submit file, we would put the \"multiply.sh\" program as the job executable , and 1 2 3 as the job arguments : executable = multiply.sh arguments = 1 2 3 Let\u2019s try a job submission with arguments. We will revisit our slotinfo.sh script. If you check the time zones of the outputs of each job, you may have a heterogenous mix of time zones\u2014some may be in UTC, some in CDT, EDT, MDT, PDT, etc. This isn't very helpful for comparison between jobs! Let's have our script take in a time zone as an argument so that all times are listed in that time zone. Create a new script called tz_slotinfo.sh with the following content: 1 2 3 4 5 6 7 8 9 #!/bin/bash if [ \" $# \" -ne 1 ] ; then echo \"Usage: $0 [time zone]\" exit fi echo \"Date,Hostname,System,Directory,OSG Site Name\" echo \" $( TZ = \" ${ 1 } \" date ) , $( hostname ) , $( uname -spo ) , $( pwd ) , ${ OSG_SITE_NAME } \" Make sure the script is executable, and test it using America/Los_Angeles as the argument. [username@ap40 htc1-3]$ ./tz_slotinfo.sh America/Los_Angeles Since we're in Wisconsin, we're going to use the America/Chicago time zone for our jobs. Create a new submit file and save the following text in it. executable = tz_slotinfo.sh arguments = America/Chicago output = tz_slotinfo_$(Process).out error = tz_slotinfo_$(Process).err log = tz_slotinfo_$(Cluster).log request_cpus = 1 request_memory = 5 MB request_disk = 300 KB queue 50 You can save the file using any name, but as a reminder, we recommend it uses the .sub file extension. Except for changing a few filenames, this submit file is nearly identical to the last one, except for the addition of the arguments line. Submit these new jobs to HTCondor. Again, watch for it to run using condor_q and optional arguments; check once every 15 seconds or so. When the jobs finish, it will disappear from the queue and create .out and .err files. After checking that there were no errors, concatenate the files into a new .csv file and view it. Did we get the time zone we expected for all jobs?","title":"Running Jobs With Arguments"},{"location":"materials/htcondor/part1-ex3-jobs/#extra-challenge","text":"Note There are Extra Challenges throughout the school curriculum. You may be better off coming back to these after you've completed all other exercises for your current working session. Below is a Python script that does something similar to the shell script above. Write a submit file and submit a job that runs this Python script. #!/usr/bin/env python3 \"\"\"Extra Challenge for OSG School Written by Tim Cartwright \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len ( sys . argv ) > 1 : arguments = '\"' + ' ' . join ( sys . argv [ 1 :]) + '\"' print ( __doc__ , file = sys . stderr ) print ( 'Time :' , time . strftime ( '%Y-%m- %d ( %a ) %H:%M:%S %Z' )) print ( 'Host :' , getpass . getuser (), '@' , socket . gethostname ()) uname = platform . uname () print ( \"System :\" , uname [ 0 ], uname [ 2 ], uname [ 4 ]) print ( \"Version :\" , platform . python_version ()) print ( \"Program :\" , sys . executable ) print ( 'Script :' , os . path . abspath ( __file__ )) print ( 'Args :' , arguments )","title":"Extra Challenge"},{"location":"materials/htcondor/part1-ex4-logs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.4: Read and Interpret Log Files \u00b6 Exercise Goal \u00b6 In the previous exercise, we learned how to translate a simple list of computational tasks into HTCondor jobs. What if we want to learn more about our jobs? The goal of this exercise is to learn how to understand the contents of a job's log file, which contains a history of the steps HTCondor took to run your job. The log file is also a great place to look while you are testing your jobs, as it records resource usage. Additionally, if you suspect something has gone wrong with your job, the log is the a great place to start looking for indications of whether things might have gone wrong (in addition to the error file). Reading a Log File \u00b6 In our last exercise, we collected information about the slots and submitted a relatively small batch of jobs as a initial test. What is HTCondor doing behind the scenes? In addition, how many resources are we actually using? Why should we care about our resource usage? There are two undesirable scenarios: Under-requesting resources. If you under-request resources (i.e. memory, disk), your jobs go into the hold state when their usage exceeds the resources allocated to it. This means your jobs stops running, and you have to fix the issue by requesting more resources and resubmit the jobs. Over-requesting resources. The easy solution to avoid the above scenario is to request a lot of resources, right? Unfortunately, no! Over-requesting resources means HTCondor needs to find a slot that has those resources, which can take longer than necessary if your jobs could have run on a slot with fewer resources. This is especially detrimental when you plan to submit many jobs. For this exercise, we can examine a log file for any previous jobs that you have run. The example output below is based on a single job (process) within with the batch of jobs we submitted. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are some of the event headings from the tz_slotinfo job log (detailed output in between headings has been omitted here): 000 (12636880.000.000) 2025-05-27 17:35:28 Job submitted from host: <128... 040 (12636880.000.000) 2025-05-27 17:35:52 Started transferring input files 040 (12636880.000.000) 2025-05-27 17:35:52 Finished transferring input files 021 (12636880.000.000) 2025-05-27 17:35:54 Message from starter on slot1... 001 (12636880.000.000) 2025-05-27 17:35:54 Job executing on host: <10.11... 006 (12636880.000.000) 2025-05-27 17:35:55 Image size of job updated: 1 040 (12636880.000.000) 2025-05-27 17:35:55 Started transferring output files 040 (12636880.000.000) 2025-05-27 17:35:55 Finished transferring output files 005 (12636880.000.000) 2025-05-27 17:35:55 Job terminated. View one of these log files and scroll through, observing what's written. There is a lot of extra information in those lines, but you can see: The job ID: cluster 12636880 , process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Each event ends with a line that contains only 3 dots: ... Note Because we printed a single log file for all 50 jobs in the batch of jobs, all 50 jobs' events are printed in this log file as they happen. If you want an individual log file for each job in the batch, use $(Process) in the log line of the submit file. However, some lines have additional information to help you quickly understand where and how your jobs are running. For example: 001 (12636880.000.000) 2025-05-27 17:35:54 Job executing on host: <10.118.5.219:33393?CCBID=128.105.82.148:9618%3faddrs%3d128.105.82.148-9618+[2607-f388-2200-87-d439-a1c8-2a11-24fc]-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector1#72672458%20192.170.231.11:9618%3faddrs%3d192.170.231.11-9618+[fd85-ee78-d8a6-8607--1-73ab]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector7#31809902&PrivNet=c219.mgmt.hellbender&addrs=10.118.5.219-33393&alias=c219.mgmt.hellbender&noUDP> SlotName: slot1_2@glidein_4082265_33522666@c219.mgmt.hellbender CondorScratchDir = \"/local/scratch/glide_bKAhkg/execute/dir_1368654\" Cpus = 1 Disk = 1049600 GLIDEIN_ResourceName = \"Missouri-Hellbender-CE1\" GPUs = 0 Memory = 1024 ... The SlotName is the name of the execution point slot your job was assigned to by HTCondor, and the name of the execution point resource is provided in GLIDEIN_ResourceName . The CondorScratchDir is the name of the scratch directory that was created by HTCondor for your job to run inside. The Cpu , GPUs , Disk (in KiB), and Memory (in MB) values provide the maximum amount of each resource your job can use while running. Another example of is the periodic update: 006 (12636880.000.000) 2025-05-27 17:35:55 Image size of job updated: 1 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... These updates record the amount of memory that your jobs are using on the Execution Points. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. The job termination event includes a lot of very useful information: 005 (12636880.000.000) 2025-05-27 17:35:55 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 147 - Run Bytes Sent By Job 211 - Run Bytes Received By Job 147 - Total Bytes Sent By Job 211 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 0 1 1 Disk (KB) : 130 1048576 1049600 GPUs : 0 Memory (MB) : 0 1024 1024 TimeExecute (s) : 1 TimeSlotBusy (s) : 3 ... Probably the most interesting information is: The return value or exit code ( 0 here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage, which will inform larger submissions. There are many other kinds of events, but the ones above will occur in almost every job log. Questions to consider Did we under- or over-request resources for our jobs? Why do you think the CPU usage shows 0 instead of 1 ? What might account for the difference between TimeExecute and TimeSlotBusy ? Discuss your answers to these questions with a neighbor or staff member. Understanding How HTCondor Writes Files \u00b6 When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we will use the tz_slotinfo job from earlier. Edit the submit file so it submits 5 jobs instead of 50. Submit the job three separate times in a row. Wait for all the jobs to finish. Examine the output file: Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file carefully: What happened there? Pay close attention to the times and job IDs of the events. How can you modify the submit file so it creates a unique .out and .err file for every condor_submit attempt? For further clarification about how HTCondor handles these files, reach out to your neighbor or one of the other School staff.","title":"1.4 - Read and interpret log files"},{"location":"materials/htcondor/part1-ex4-logs/#htc-exercise-14-read-and-interpret-log-files","text":"","title":"HTC Exercise 1.4: Read and Interpret Log Files"},{"location":"materials/htcondor/part1-ex4-logs/#exercise-goal","text":"In the previous exercise, we learned how to translate a simple list of computational tasks into HTCondor jobs. What if we want to learn more about our jobs? The goal of this exercise is to learn how to understand the contents of a job's log file, which contains a history of the steps HTCondor took to run your job. The log file is also a great place to look while you are testing your jobs, as it records resource usage. Additionally, if you suspect something has gone wrong with your job, the log is the a great place to start looking for indications of whether things might have gone wrong (in addition to the error file).","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex4-logs/#reading-a-log-file","text":"In our last exercise, we collected information about the slots and submitted a relatively small batch of jobs as a initial test. What is HTCondor doing behind the scenes? In addition, how many resources are we actually using? Why should we care about our resource usage? There are two undesirable scenarios: Under-requesting resources. If you under-request resources (i.e. memory, disk), your jobs go into the hold state when their usage exceeds the resources allocated to it. This means your jobs stops running, and you have to fix the issue by requesting more resources and resubmit the jobs. Over-requesting resources. The easy solution to avoid the above scenario is to request a lot of resources, right? Unfortunately, no! Over-requesting resources means HTCondor needs to find a slot that has those resources, which can take longer than necessary if your jobs could have run on a slot with fewer resources. This is especially detrimental when you plan to submit many jobs. For this exercise, we can examine a log file for any previous jobs that you have run. The example output below is based on a single job (process) within with the batch of jobs we submitted. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are some of the event headings from the tz_slotinfo job log (detailed output in between headings has been omitted here): 000 (12636880.000.000) 2025-05-27 17:35:28 Job submitted from host: <128... 040 (12636880.000.000) 2025-05-27 17:35:52 Started transferring input files 040 (12636880.000.000) 2025-05-27 17:35:52 Finished transferring input files 021 (12636880.000.000) 2025-05-27 17:35:54 Message from starter on slot1... 001 (12636880.000.000) 2025-05-27 17:35:54 Job executing on host: <10.11... 006 (12636880.000.000) 2025-05-27 17:35:55 Image size of job updated: 1 040 (12636880.000.000) 2025-05-27 17:35:55 Started transferring output files 040 (12636880.000.000) 2025-05-27 17:35:55 Finished transferring output files 005 (12636880.000.000) 2025-05-27 17:35:55 Job terminated. View one of these log files and scroll through, observing what's written. There is a lot of extra information in those lines, but you can see: The job ID: cluster 12636880 , process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Each event ends with a line that contains only 3 dots: ... Note Because we printed a single log file for all 50 jobs in the batch of jobs, all 50 jobs' events are printed in this log file as they happen. If you want an individual log file for each job in the batch, use $(Process) in the log line of the submit file. However, some lines have additional information to help you quickly understand where and how your jobs are running. For example: 001 (12636880.000.000) 2025-05-27 17:35:54 Job executing on host: <10.118.5.219:33393?CCBID=128.105.82.148:9618%3faddrs%3d128.105.82.148-9618+[2607-f388-2200-87-d439-a1c8-2a11-24fc]-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector1#72672458%20192.170.231.11:9618%3faddrs%3d192.170.231.11-9618+[fd85-ee78-d8a6-8607--1-73ab]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector7#31809902&PrivNet=c219.mgmt.hellbender&addrs=10.118.5.219-33393&alias=c219.mgmt.hellbender&noUDP> SlotName: slot1_2@glidein_4082265_33522666@c219.mgmt.hellbender CondorScratchDir = \"/local/scratch/glide_bKAhkg/execute/dir_1368654\" Cpus = 1 Disk = 1049600 GLIDEIN_ResourceName = \"Missouri-Hellbender-CE1\" GPUs = 0 Memory = 1024 ... The SlotName is the name of the execution point slot your job was assigned to by HTCondor, and the name of the execution point resource is provided in GLIDEIN_ResourceName . The CondorScratchDir is the name of the scratch directory that was created by HTCondor for your job to run inside. The Cpu , GPUs , Disk (in KiB), and Memory (in MB) values provide the maximum amount of each resource your job can use while running. Another example of is the periodic update: 006 (12636880.000.000) 2025-05-27 17:35:55 Image size of job updated: 1 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... These updates record the amount of memory that your jobs are using on the Execution Points. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. The job termination event includes a lot of very useful information: 005 (12636880.000.000) 2025-05-27 17:35:55 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 147 - Run Bytes Sent By Job 211 - Run Bytes Received By Job 147 - Total Bytes Sent By Job 211 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 0 1 1 Disk (KB) : 130 1048576 1049600 GPUs : 0 Memory (MB) : 0 1024 1024 TimeExecute (s) : 1 TimeSlotBusy (s) : 3 ... Probably the most interesting information is: The return value or exit code ( 0 here, means the executable completed and didn't indicate any internal errors; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage, which will inform larger submissions. There are many other kinds of events, but the ones above will occur in almost every job log. Questions to consider Did we under- or over-request resources for our jobs? Why do you think the CPU usage shows 0 instead of 1 ? What might account for the difference between TimeExecute and TimeSlotBusy ? Discuss your answers to these questions with a neighbor or staff member.","title":"Reading a Log File"},{"location":"materials/htcondor/part1-ex4-logs/#understanding-how-htcondor-writes-files","text":"When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we will use the tz_slotinfo job from earlier. Edit the submit file so it submits 5 jobs instead of 50. Submit the job three separate times in a row. Wait for all the jobs to finish. Examine the output file: Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file carefully: What happened there? Pay close attention to the times and job IDs of the events. How can you modify the submit file so it creates a unique .out and .err file for every condor_submit attempt? For further clarification about how HTCondor handles these files, reach out to your neighbor or one of the other School staff.","title":"Understanding How HTCondor Writes Files"},{"location":"materials/htcondor/part1-ex5-request/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.5: Determine Resource Needs \u00b6 Exercise Goal \u00b6 Before we submit full workloads, it's important to not over- or under-request resources, as discussed in the last exercise. Now that we know what the log file is and how to read it, let's apply that knowledge. The goal of this exercise is to demonstrate how to test and tune the request_* statements in a submit file for when you don't know what resources your job needs. There are three resource request statements that you should use in an HTCondor submit file: request_cpus for the number of CPUs your jobs will use. A value of 1 is always a great starting point, but some software can use more. (Most softwares will use an argument to control this number.) request_memory for the maximum amount of run-time memory your jobs may use. request_disk for the maximum amount of disk space your jobs may use. This includes the executable, input files, output files, and any other files generated during the jobs. HTCondor defaults to certain values for these request settings, so you do not need to use them to get small jobs to run. However, it is highly recommended to estimate resource requests before submitting any job, especially before submitting multiple jobs. Check Your Understanding \u00b6 As a quick review, take a moment to think about what happens in the following scenarios, as discussed in the previous exercise. When you have thought of your answer, check your understanding by expanding the boxes below. What happens if you under-request resources? If your jobs goes over the request values, they may be removed from the execution point and held (status H in the condor_q output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or underestimate them. What happens if you over-request resources? If you overestimate your resource requests, your jobs will match to fewer slots and take longer to match to a slot to begin running. Additionally, by hogging up resources that you don't need, other users may be deprived of the resources they require. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running). Determine Resource Needs Before Running Any Jobs \u00b6 Note If you are running short on time, you can skip to Determining Resource Needs By Running Test Jobs below, but try to come back and read over this part at some point. It can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your jobs' maximum memory needs, use that number or a slightly larger number to ensure your jobs have enough memory to complete. If this is your first time running your jobs, you can request a fairly large amount of memory (as high as what's on your laptop or workstation, usually about 4 to 8 GB, if you know your program can run without crashing) for a first test job, OR you can run the program locally and \"watch\" it. Examine a Running Program on a Local Computer \u00b6 When working on a shared Access Point, you should never run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all users. However, you may have access to other computers (e.g. your laptop, workstation, another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage. Estimating Memory \u00b6 On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : [username@ap40]$ ps ux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND alice 24342 0.0 0.0 90224 1864 ? S 13:39 0:00 sshd: alice@pts/0 alice 24343 0.0 0.0 66096 1580 pts/0 Ss 13:39 0:00 -bash alice 25864 0.0 0.0 65624 996 pts/0 R+ 13:52 0:00 ps ux alice 30052 0.0 0.0 90720 2456 ? S Jun22 0:00 sshd: alice@pts/2 alice 30053 0.0 0.0 66096 1624 pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : [username@ap40]$ top -u <USERNAME> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 24342 alice 15 0 90224 1864 1096 S 0.0 0.0 0:00.26 sshd 24343 alice 15 0 66096 1580 1232 S 0.0 0.0 0:00.07 bash 25927 alice 15 0 12760 1196 836 R 0.0 0.0 0:00.01 top 30052 alice 16 0 90720 2456 1112 S 0.0 0.1 0:00.69 sshd 30053 alice 18 0 66096 1624 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage. Estimating Disk \u00b6 Determining disk needs is easier, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage, including the following: The executable itself All \"input\" files (anything else that gets transferred TO the jobs, even if you don't think of it as \"input\") All files created during the jobs (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the access point), you should be able to view files and their sizes with the ls -lh and du commands. Determine Resource Needs By Running Test Jobs (Recommended) \u00b6 Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a Python script that does not do anything useful but consumes some real resources while running: #!/usr/bin/env python3 import time import os size = 1000000 numbers = [] for i in range ( size ): numbers . append ( str ( i )) with open ( 'numbers.txt' , 'w' ) as tempfile : tempfile . write ( ' ' . join ( numbers )) time . sleep ( 60 ) Without trying to figure out what this code does or how many resources it uses, create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the jobs will use far more). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 6739 1048576 8022934 Memory (MB) : 99 1024 1024 So, now we know that HTCondor saw that the job used 6,739 KB of disk (= about 6.5 MB) and 99 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. You should round up your resource requests a little, just in case your job occasionally uses more resources. Set Resource Requirements \u00b6 Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: # rounded up from 99 MB request_memory = 120MB # rounded up from 6.5 MB request_disk = 7MB Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into attributes that become part of the job's requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_* statements instead. If you still have time in this working session, add these requirements to your submit file for the Python script, rerun the jobs, and confirm in the log file that your requests were used. After changing the requirements in your submit file, did your jobs run successfully? If not, why?","title":"1.5 - Determine resource needs"},{"location":"materials/htcondor/part1-ex5-request/#htc-exercise-15-determine-resource-needs","text":"","title":"HTC Exercise 1.5: Determine Resource Needs"},{"location":"materials/htcondor/part1-ex5-request/#exercise-goal","text":"Before we submit full workloads, it's important to not over- or under-request resources, as discussed in the last exercise. Now that we know what the log file is and how to read it, let's apply that knowledge. The goal of this exercise is to demonstrate how to test and tune the request_* statements in a submit file for when you don't know what resources your job needs. There are three resource request statements that you should use in an HTCondor submit file: request_cpus for the number of CPUs your jobs will use. A value of 1 is always a great starting point, but some software can use more. (Most softwares will use an argument to control this number.) request_memory for the maximum amount of run-time memory your jobs may use. request_disk for the maximum amount of disk space your jobs may use. This includes the executable, input files, output files, and any other files generated during the jobs. HTCondor defaults to certain values for these request settings, so you do not need to use them to get small jobs to run. However, it is highly recommended to estimate resource requests before submitting any job, especially before submitting multiple jobs.","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex5-request/#check-your-understanding","text":"As a quick review, take a moment to think about what happens in the following scenarios, as discussed in the previous exercise. When you have thought of your answer, check your understanding by expanding the boxes below. What happens if you under-request resources? If your jobs goes over the request values, they may be removed from the execution point and held (status H in the condor_q output, awaiting action on your part) without saving any partial job output files. So it is a disadvantage to not declare your resource needs or underestimate them. What happens if you over-request resources? If you overestimate your resource requests, your jobs will match to fewer slots and take longer to match to a slot to begin running. Additionally, by hogging up resources that you don't need, other users may be deprived of the resources they require. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running).","title":"Check Your Understanding"},{"location":"materials/htcondor/part1-ex5-request/#determine-resource-needs-before-running-any-jobs","text":"Note If you are running short on time, you can skip to Determining Resource Needs By Running Test Jobs below, but try to come back and read over this part at some point. It can be very difficult to predict the memory needs of your running program without running tests. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your jobs' maximum memory needs, use that number or a slightly larger number to ensure your jobs have enough memory to complete. If this is your first time running your jobs, you can request a fairly large amount of memory (as high as what's on your laptop or workstation, usually about 4 to 8 GB, if you know your program can run without crashing) for a first test job, OR you can run the program locally and \"watch\" it.","title":"Determine Resource Needs Before Running Any Jobs"},{"location":"materials/htcondor/part1-ex5-request/#examine-a-running-program-on-a-local-computer","text":"When working on a shared Access Point, you should never run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all users. However, you may have access to other computers (e.g. your laptop, workstation, another server) where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage.","title":"Examine a Running Program on a Local Computer"},{"location":"materials/htcondor/part1-ex5-request/#estimating-memory","text":"On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : [username@ap40]$ ps ux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND alice 24342 0.0 0.0 90224 1864 ? S 13:39 0:00 sshd: alice@pts/0 alice 24343 0.0 0.0 66096 1580 pts/0 Ss 13:39 0:00 -bash alice 25864 0.0 0.0 65624 996 pts/0 R+ 13:52 0:00 ps ux alice 30052 0.0 0.0 90720 2456 ? S Jun22 0:00 sshd: alice@pts/2 alice 30053 0.0 0.0 66096 1624 pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : [username@ap40]$ top -u <USERNAME> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 24342 alice 15 0 90224 1864 1096 S 0.0 0.0 0:00.26 sshd 24343 alice 15 0 66096 1580 1232 S 0.0 0.0 0:00.07 bash 25927 alice 15 0 12760 1196 836 R 0.0 0.0 0:00.01 top 30052 alice 16 0 90720 2456 1112 S 0.0 0.1 0:00.69 sshd 30053 alice 18 0 66096 1624 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage.","title":"Estimating Memory"},{"location":"materials/htcondor/part1-ex5-request/#estimating-disk","text":"Determining disk needs is easier, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage, including the following: The executable itself All \"input\" files (anything else that gets transferred TO the jobs, even if you don't think of it as \"input\") All files created during the jobs (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the access point), you should be able to view files and their sizes with the ls -lh and du commands.","title":"Estimating Disk"},{"location":"materials/htcondor/part1-ex5-request/#determine-resource-needs-by-running-test-jobs-recommended","text":"Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a Python script that does not do anything useful but consumes some real resources while running: #!/usr/bin/env python3 import time import os size = 1000000 numbers = [] for i in range ( size ): numbers . append ( str ( i )) with open ( 'numbers.txt' , 'w' ) as tempfile : tempfile . write ( ' ' . join ( numbers )) time . sleep ( 60 ) Without trying to figure out what this code does or how many resources it uses, create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the jobs will use far more). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 6739 1048576 8022934 Memory (MB) : 99 1024 1024 So, now we know that HTCondor saw that the job used 6,739 KB of disk (= about 6.5 MB) and 99 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. You should round up your resource requests a little, just in case your job occasionally uses more resources.","title":"Determine Resource Needs By Running Test Jobs (Recommended)"},{"location":"materials/htcondor/part1-ex5-request/#set-resource-requirements","text":"Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: # rounded up from 99 MB request_memory = 120MB # rounded up from 6.5 MB request_disk = 7MB Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into attributes that become part of the job's requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_* statements instead. If you still have time in this working session, add these requirements to your submit file for the Python script, rerun the jobs, and confirm in the log file that your requests were used. After changing the requirements in your submit file, did your jobs run successfully? If not, why?","title":"Set Resource Requirements"},{"location":"materials/htcondor/part1-ex6-remove/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 1.6: Remove Jobs From the Queue \u00b6 Exercise Goal \u00b6 In the last exercise, we learned how to estimate and request resources needed for our jobs. Imagine a scenario where you realize you've requested the wrong amount of resources after submitting a batch of jobs. You want to remove the jobs from the queue so you can submit them with the correct resource requirements. How do you do that? The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you can remove them, edit the submit files, and then submit again. Note Please remember to remove any jobs from the queue that you are no longer interested in. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is short, but if you are out of time, you can come back to it later. Removing a Job or Cluster From the Queue \u00b6 To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise. Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job using its full job ID, e.g. 5759.0 . [username@ap40]$ condor_rm <JOB.ID> Did the job leave the queue immediately? If not, about how long did it take? When you use the full job ID, you remove only one job from the batch of jobs (or one process from the cluster of jobs). However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: [username@ap40]$ condor_rm <CLUSTER> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: [username@ap40]$ condor_rm 5768 5769 5770 .0 5771 .2 Removing All of Your Jobs \u00b6 If you really want to remove all of your jobs at once, you can do that with: [username@ap40]$ condor_rm <USERNAME> If you want to test it: (optional, though you'll likely need this in the future) Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all with the above command Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them. :)","title":"1.6 - Remove jobs from the queue"},{"location":"materials/htcondor/part1-ex6-remove/#htc-exercise-16-remove-jobs-from-the-queue","text":"","title":"HTC Exercise 1.6: Remove Jobs From the Queue"},{"location":"materials/htcondor/part1-ex6-remove/#exercise-goal","text":"In the last exercise, we learned how to estimate and request resources needed for our jobs. Imagine a scenario where you realize you've requested the wrong amount of resources after submitting a batch of jobs. You want to remove the jobs from the queue so you can submit them with the correct resource requirements. How do you do that? The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you can remove them, edit the submit files, and then submit again. Note Please remember to remove any jobs from the queue that you are no longer interested in. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is short, but if you are out of time, you can come back to it later.","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex6-remove/#removing-a-job-or-cluster-from-the-queue","text":"To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise. Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job using its full job ID, e.g. 5759.0 . [username@ap40]$ condor_rm <JOB.ID> Did the job leave the queue immediately? If not, about how long did it take? When you use the full job ID, you remove only one job from the batch of jobs (or one process from the cluster of jobs). However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: [username@ap40]$ condor_rm <CLUSTER> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: [username@ap40]$ condor_rm 5768 5769 5770 .0 5771 .2","title":"Removing a Job or Cluster From the Queue"},{"location":"materials/htcondor/part1-ex6-remove/#removing-all-of-your-jobs","text":"If you really want to remove all of your jobs at once, you can do that with: [username@ap40]$ condor_rm <USERNAME> If you want to test it: (optional, though you'll likely need this in the future) Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all with the above command Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them. :)","title":"Removing All of Your Jobs"},{"location":"materials/htcondor/part1-ex7-queue/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 1.7: Explore condor_q \u00b6 Exercise Goal \u00b6 condor_q is a handy command to check the status of your jobs, but there are many powerful options available that can give you useful information! The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert! Selecting Jobs \u00b6 The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode is to show only your jobs in \"batch\" mode: [username@ap40]$ condor_q You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: [username@ap40]$ condor_q -all And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: [username@ap40]$ condor_q -nobatch [username@ap40]$ condor_q -all -nobatch Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? [username@ap40]$ condor_q <USERNAME1> <USERNAME2> <USERNAME3> To list just the jobs associated with a single cluster number: [username@ap40]$ condor_q <CLUSTER> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process , as in 5678.0 ): [username@ap40]$ condor_q <JOB.ID> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several jobs. List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching. List a specific cluster. List a specific job ID. Try listing several users at once. Try listing several clusters and job IDs at once. When there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens? Viewing a Job ClassAd \u00b6 You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: [username@ap40]$ condor_q -long <JOB.ID> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/username/intro-2.1-queue/sleep.log\" Requirements = ((OSGVO_OS_STRING == \"RHEL 9\")) && (TARGET.Arch == \"X86_64\") && (TARGET.OpSys == \"LINUX\") && (TARGET.Disk >= RequestDisk) && (TARGET.Memory >= RequestMemory) && ((TARGET.FileSystemDomain == MY.FileSystemDomain) || (TARGET.HasFileTransfer)) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"username\" CondorVersion = \"$CondorVersion: 24.7.3 2025-04-17 BuildID: 802763 PackageID: 24.7.3-0.802763 GitSHA: bb5d9294 RC $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Write a submit file for a sleep job that sleeps for at least 3 minutes (executable below). Submit the job. # !/bin/bash sleep 180 Before the job executes, capture its ClassAd and save to a file: condor_q -l <JOB.ID> > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file condor_q -l <JOB.ID> > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStatus Why Is My Job Not Running? \u00b6 Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why your job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: [username@ap40]$ condor_q -better-analyze 2423 .0 Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_disk = 10MB request_memory = 8TB requirements = (OSGVO_OS_STRING == \"RHEL 9\") queue Save and submit this file. Run condor_q -better-analyze on the job ID (this may take a minute to run). There is a lot of output, but a few items are worth highlighting. Here is a sample (with some lines omitted): -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... ... Job 12635168.000 defines the following attributes: RequestDisk = 10240 (kb) RequestMemory = 8388608 (mb) The Requirements expression for job 12635168.000 reduces to these conditions: Slots Step Matched Condition ----- --------- --------- [0] 7425 OSGVO_OS_STRING == \"RHEL 9\" [1] 9251 TARGET.Arch == \"X86_64\" [2] 7424 [0] && [1] [5] 9241 TARGET.Disk >= RequestDisk [7] 0 TARGET.Memory >= RequestMemory 12635168.000: Run analysis summary ignoring user priority. Of 9252 slots on 3328 machines, 9252 slots are rejected by your job's requirements 0 slots reject your job because of their own requirements 0 slots match and are willing to run your job WARNING: Be advised: No machines matched the jobs's constraints At the end of the summary, condor_q provides a breakdown of how machines and their own requirements match against my own job's requirements. 9252 total slots were considered above, and all of them were rejected based on my job's requirements . In other words, I am asking for something that is not available. But what? Further up in the output, there is an analysis of the job's requirements, along with how many slots within the pool match each of those requirements. The example above reports that 9241 slots match our small disk request request, but none of the slots matched the TARGET.Memory >= RequestMemory condition. The output also reports the value used for the RequestMemory attribute: my job asked for 8 terabytes of memory (8,388,608 MB)\u2014of course no machines matched that part of the expression! That's a lot of memory on today's machines. The output from condor_q -better-analyze may or may not be helpful, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching. Bonus: Automatic Formatting Output \u00b6 Do this exercise only if you have time, though it's pretty awesome! There is a way to select the specific job attributes you want condor_q to tell you about with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to specially format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: [username@ap40]$ condor_q -all -af Owner ClusterId Cmd moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of plain values, you can use -af:r to view the expressions, instead of what it's current evaluation.) References \u00b6 As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the HTCondor Manual","title":"Bonus Exercise 1.7 - Explore condor_q"},{"location":"materials/htcondor/part1-ex7-queue/#bonus-htc-exercise-17-explore-condor_q","text":"","title":"Bonus HTC Exercise 1.7: Explore condor_q"},{"location":"materials/htcondor/part1-ex7-queue/#exercise-goal","text":"condor_q is a handy command to check the status of your jobs, but there are many powerful options available that can give you useful information! The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert!","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex7-queue/#selecting-jobs","text":"The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode is to show only your jobs in \"batch\" mode: [username@ap40]$ condor_q You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: [username@ap40]$ condor_q -all And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: [username@ap40]$ condor_q -nobatch [username@ap40]$ condor_q -all -nobatch Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? [username@ap40]$ condor_q <USERNAME1> <USERNAME2> <USERNAME3> To list just the jobs associated with a single cluster number: [username@ap40]$ condor_q <CLUSTER> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process , as in 5678.0 ): [username@ap40]$ condor_q <JOB.ID> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several jobs. List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching. List a specific cluster. List a specific job ID. Try listing several users at once. Try listing several clusters and job IDs at once. When there are a variety of jobs in the queue, try combining a username and a different user's cluster or job ID in the same command \u2014 what happens?","title":"Selecting Jobs"},{"location":"materials/htcondor/part1-ex7-queue/#viewing-a-job-classad","text":"You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: [username@ap40]$ condor_q -long <JOB.ID> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/username/intro-2.1-queue/sleep.log\" Requirements = ((OSGVO_OS_STRING == \"RHEL 9\")) && (TARGET.Arch == \"X86_64\") && (TARGET.OpSys == \"LINUX\") && (TARGET.Disk >= RequestDisk) && (TARGET.Memory >= RequestMemory) && ((TARGET.FileSystemDomain == MY.FileSystemDomain) || (TARGET.HasFileTransfer)) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"username\" CondorVersion = \"$CondorVersion: 24.7.3 2025-04-17 BuildID: 802763 PackageID: 24.7.3-0.802763 GitSHA: bb5d9294 RC $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Write a submit file for a sleep job that sleeps for at least 3 minutes (executable below). Submit the job. # !/bin/bash sleep 180 Before the job executes, capture its ClassAd and save to a file: condor_q -l <JOB.ID> > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file condor_q -l <JOB.ID> > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStatus","title":"Viewing a Job ClassAd"},{"location":"materials/htcondor/part1-ex7-queue/#why-is-my-job-not-running","text":"Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why your job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: [username@ap40]$ condor_q -better-analyze 2423 .0 Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_disk = 10MB request_memory = 8TB requirements = (OSGVO_OS_STRING == \"RHEL 9\") queue Save and submit this file. Run condor_q -better-analyze on the job ID (this may take a minute to run). There is a lot of output, but a few items are worth highlighting. Here is a sample (with some lines omitted): -- Schedd: ap40.uw.osg-htc.org : <128.105.68.62:9618?... ... Job 12635168.000 defines the following attributes: RequestDisk = 10240 (kb) RequestMemory = 8388608 (mb) The Requirements expression for job 12635168.000 reduces to these conditions: Slots Step Matched Condition ----- --------- --------- [0] 7425 OSGVO_OS_STRING == \"RHEL 9\" [1] 9251 TARGET.Arch == \"X86_64\" [2] 7424 [0] && [1] [5] 9241 TARGET.Disk >= RequestDisk [7] 0 TARGET.Memory >= RequestMemory 12635168.000: Run analysis summary ignoring user priority. Of 9252 slots on 3328 machines, 9252 slots are rejected by your job's requirements 0 slots reject your job because of their own requirements 0 slots match and are willing to run your job WARNING: Be advised: No machines matched the jobs's constraints At the end of the summary, condor_q provides a breakdown of how machines and their own requirements match against my own job's requirements. 9252 total slots were considered above, and all of them were rejected based on my job's requirements . In other words, I am asking for something that is not available. But what? Further up in the output, there is an analysis of the job's requirements, along with how many slots within the pool match each of those requirements. The example above reports that 9241 slots match our small disk request request, but none of the slots matched the TARGET.Memory >= RequestMemory condition. The output also reports the value used for the RequestMemory attribute: my job asked for 8 terabytes of memory (8,388,608 MB)\u2014of course no machines matched that part of the expression! That's a lot of memory on today's machines. The output from condor_q -better-analyze may or may not be helpful, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching.","title":"Why Is My Job Not Running?"},{"location":"materials/htcondor/part1-ex7-queue/#bonus-automatic-formatting-output","text":"Do this exercise only if you have time, though it's pretty awesome! There is a way to select the specific job attributes you want condor_q to tell you about with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to specially format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: [username@ap40]$ condor_q -all -af Owner ClusterId Cmd moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of plain values, you can use -af:r to view the expressions, instead of what it's current evaluation.)","title":"Bonus: Automatic Formatting Output"},{"location":"materials/htcondor/part1-ex7-queue/#references","text":"As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the HTCondor Manual","title":"References"},{"location":"materials/htcondor/part1-ex8-status/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 1.8: Explore condor_status \u00b6 Exercise Goal \u00b6 The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert! Selecting Slots \u00b6 The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: [username@ap40]$ condor_status -avail Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: [username@ap40]$ condor_status <hostname> For example, if you want to see the slots on e2337.chtc.wisc.edu (in the CHTC pool): Note You can name more than one hostname on the command line, in which case slots for all of the named hostnames and/or slots are listed. List all the slots in the pool. How many are there total? How many slots are currently available? Viewing a Slot ClassAd \u00b6 Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): [username@ap40]$ condor_status -long <hostname> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 96 OpSysAndVer = \"CentOS9\" MyType = \"Machine\" LoadAvg = 7.0 TotalDisk = 1424814084 TotalMemory = 80000 Machine = \"wsu-lm02.osris.org\" CondorVersion = \"$CondorVersion: 24.7.3 2025-04-22 BuildID: 803720 PackageID: 24.7.3-1 GitSHA: e207c094 $\" As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now. Viewing Slots by ClassAd Expression \u00b6 Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 7 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: [username@ap40]$ condor_status -constraint 'OpSysAndVer == \"CentOS7\" && Memory >= 16000' Note Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on the OSPool, there are only a few slots that meet these criteria. If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is advanced material, so it is not required. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression. Bonus: Formatting Output \u00b6 The condor_status command accepts the same -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, you can query the hostname and operating system of the slots with more than 32GB of memory: [username@ap40]$ condor_status -af Machine -af OpSysAndVer -constraint 'Memory >= 32000' If you'd like, spend a few minutes now or later experimenting with condor_status formatting. References \u00b6 As suggested above, if you want to learn more about condor_status , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the appendix of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"Bonus Exercise 1.8 - Explore condor_status"},{"location":"materials/htcondor/part1-ex8-status/#bonus-htc-exercise-18-explore-condor_status","text":"","title":"Bonus HTC Exercise 1.8: Explore condor_status"},{"location":"materials/htcondor/part1-ex8-status/#exercise-goal","text":"The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert!","title":"Exercise Goal"},{"location":"materials/htcondor/part1-ex8-status/#selecting-slots","text":"The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: [username@ap40]$ condor_status -avail Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: [username@ap40]$ condor_status <hostname> For example, if you want to see the slots on e2337.chtc.wisc.edu (in the CHTC pool): Note You can name more than one hostname on the command line, in which case slots for all of the named hostnames and/or slots are listed. List all the slots in the pool. How many are there total? How many slots are currently available?","title":"Selecting Slots"},{"location":"materials/htcondor/part1-ex8-status/#viewing-a-slot-classad","text":"Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): [username@ap40]$ condor_status -long <hostname> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 96 OpSysAndVer = \"CentOS9\" MyType = \"Machine\" LoadAvg = 7.0 TotalDisk = 1424814084 TotalMemory = 80000 Machine = \"wsu-lm02.osris.org\" CondorVersion = \"$CondorVersion: 24.7.3 2025-04-22 BuildID: 803720 PackageID: 24.7.3-1 GitSHA: e207c094 $\" As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now.","title":"Viewing a Slot ClassAd"},{"location":"materials/htcondor/part1-ex8-status/#viewing-slots-by-classad-expression","text":"Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 7 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: [username@ap40]$ condor_status -constraint 'OpSysAndVer == \"CentOS7\" && Memory >= 16000' Note Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on the OSPool, there are only a few slots that meet these criteria. If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is advanced material, so it is not required. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression.","title":"Viewing Slots by ClassAd Expression"},{"location":"materials/htcondor/part1-ex8-status/#bonus-formatting-output","text":"The condor_status command accepts the same -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, you can query the hostname and operating system of the slots with more than 32GB of memory: [username@ap40]$ condor_status -af Machine -af OpSysAndVer -constraint 'Memory >= 32000' If you'd like, spend a few minutes now or later experimenting with condor_status formatting.","title":"Bonus: Formatting Output"},{"location":"materials/htcondor/part1-ex8-status/#references","text":"As suggested above, if you want to learn more about condor_status , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in the appendix of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"References"},{"location":"materials/htcondor/part2-ex1-files/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.1: Work With Input and Output Files \u00b6 Exercise Goal \u00b6 The goal of this exercise is make input files available to your job on the execution point and to return output files back created in your job back to you on the access point. This small change significantly adds to the kinds of jobs that you can run. View a Job Sandbox \u00b6 Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a minimal job with no special input or output files. Save the script below in a file named sandbox.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'Sandbox: ' ` pwd ` ls -alF # END Create a submit file for this script and submit it. When the job finishes, look at the contents of the output file. In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The number of files that you see can change depending on the HTC system you are using, but some of the files you should always see are: .chirp.config Configuration for an advanced feature sandbox.sh Your executable .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job tmp/ , var/ , tmp/ Directories in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the access point machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it. Running a Job With Input Files \u00b6 Next, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred to the job. Here is a Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python3 import os import sys if len ( sys . argv ) != 2 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} with open ( input_filename , 'r' , encoding = 'iso-8859-1' ) as my_file : for line in my_file : word = line . strip () . lower () if word in words : words [ word ] += 1 else : words [ word ] = 1 for word in sorted ( words . keys ()): print ( f ' { words [ word ] : 8d } { word } ' ) Create and save the Python script in a file named freq.py . Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory: [username@ap40]$ pelican object get osdf://ospool/uc-shared/public/school/2025/intro-2.1-words.txt . Create a submit file for the freq.py executable. Add a line called transfer_input_files = to tell HTCondor to transfer the input file to the job: transfer_input_files = intro-2.1-words.txt As with all submit file commands, it does not matter where this line goes, as long as it comes before the word queue . Since we want HTCondor to pass an argument to our Python executable, we need to remember to add an arguments = line in our submit file so that HTCondor knows to pass an argument to the job. Set this arguments = line equal to the argument to the Python script (i.e., the name the input file). Submit the job to HTCondor, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask for help! Note If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt Transferring Output Files \u00b6 So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: we will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise). Create a submit file and executable that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish. What happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the access point?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now. Transferring Specific Output Files \u00b6 As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ] ; then echo \"Usage: $0 INPUT\" ; exit 1 ; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup- $1 First, let\u2019s confirm that HTCondor does not bring back the output file (which starts with the prefix backup- ) in the subdirectory: Create a file called output.sh and save the above shell script in this file. Write a submit file that transfers any input file and runs output.sh on it (remember to include an arguments = line and pass the input filename as an argument). Submit the job, wait for it to finish, and examine the contents of your submit directory. Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to only transfer these specific files back to the submission directory using transfer_output_files = : transfer_output_files = output-timestamp.txt, subdirectory/ When using transfer_output_files = , HTCondor will only transfer back the files you name - all other files will be ignored and deleted at the end of a job. Note See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself ; the files will be written directly into the submit directory. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt . Copy the previous submit file that ran output.sh and add the transfer_output_files line from above. Submit the job, wait for it to finish, and examine the contents of your submit directory. Did it work as you expected? Thinking About Progress So Far \u00b6 At this point, you can do just about everything that you need in order to run jobs on a HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement! References \u00b6 There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read \"Submitting Jobs Without a Shared Filesystem\" in the HTCondor Manual.","title":"2.1 - Work with input and output files"},{"location":"materials/htcondor/part2-ex1-files/#htc-exercise-21-work-with-input-and-output-files","text":"","title":"HTC Exercise 2.1: Work With Input and Output Files"},{"location":"materials/htcondor/part2-ex1-files/#exercise-goal","text":"The goal of this exercise is make input files available to your job on the execution point and to return output files back created in your job back to you on the access point. This small change significantly adds to the kinds of jobs that you can run.","title":"Exercise Goal"},{"location":"materials/htcondor/part2-ex1-files/#view-a-job-sandbox","text":"Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a minimal job with no special input or output files. Save the script below in a file named sandbox.sh : #!/bin/sh echo 'Date: ' ` date ` echo 'Host: ' ` hostname ` echo 'Sandbox: ' ` pwd ` ls -alF # END Create a submit file for this script and submit it. When the job finishes, look at the contents of the output file. In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The number of files that you see can change depending on the HTC system you are using, but some of the files you should always see are: .chirp.config Configuration for an advanced feature sandbox.sh Your executable .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job tmp/ , var/ , tmp/ Directories in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the access point machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it.","title":"View a Job Sandbox"},{"location":"materials/htcondor/part2-ex1-files/#running-a-job-with-input-files","text":"Next, you will run a job that requires an input file. Remember, the initial job sandbox will contain only the job executable, unless you tell HTCondor explicitly about every other file that needs to be transferred to the job. Here is a Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python3 import os import sys if len ( sys . argv ) != 2 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} with open ( input_filename , 'r' , encoding = 'iso-8859-1' ) as my_file : for line in my_file : word = line . strip () . lower () if word in words : words [ word ] += 1 else : words [ word ] = 1 for word in sorted ( words . keys ()): print ( f ' { words [ word ] : 8d } { word } ' ) Create and save the Python script in a file named freq.py . Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory: [username@ap40]$ pelican object get osdf://ospool/uc-shared/public/school/2025/intro-2.1-words.txt . Create a submit file for the freq.py executable. Add a line called transfer_input_files = to tell HTCondor to transfer the input file to the job: transfer_input_files = intro-2.1-words.txt As with all submit file commands, it does not matter where this line goes, as long as it comes before the word queue . Since we want HTCondor to pass an argument to our Python executable, we need to remember to add an arguments = line in our submit file so that HTCondor knows to pass an argument to the job. Set this arguments = line equal to the argument to the Python script (i.e., the name the input file). Submit the job to HTCondor, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask for help! Note If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt","title":"Running a Job With Input Files"},{"location":"materials/htcondor/part2-ex1-files/#transferring-output-files","text":"So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a job creates in the sandbox. We will use a very simple method for creating a new file: we will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise). Create a submit file and executable that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish. What happened? Can you tell what HTCondor did with the output file that was created (did it end up back on the access point?), after it was created in the job sandbox? Look carefully at the list of files in your submit directory now.","title":"Transferring Output Files"},{"location":"materials/htcondor/part2-ex1-files/#transferring-specific-output-files","text":"As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ] ; then echo \"Usage: $0 INPUT\" ; exit 1 ; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup- $1 First, let\u2019s confirm that HTCondor does not bring back the output file (which starts with the prefix backup- ) in the subdirectory: Create a file called output.sh and save the above shell script in this file. Write a submit file that transfers any input file and runs output.sh on it (remember to include an arguments = line and pass the input filename as an argument). Submit the job, wait for it to finish, and examine the contents of your submit directory. Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to only transfer these specific files back to the submission directory using transfer_output_files = : transfer_output_files = output-timestamp.txt, subdirectory/ When using transfer_output_files = , HTCondor will only transfer back the files you name - all other files will be ignored and deleted at the end of a job. Note See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself ; the files will be written directly into the submit directory. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt . Copy the previous submit file that ran output.sh and add the transfer_output_files line from above. Submit the job, wait for it to finish, and examine the contents of your submit directory. Did it work as you expected?","title":"Transferring Specific Output Files"},{"location":"materials/htcondor/part2-ex1-files/#thinking-about-progress-so-far","text":"At this point, you can do just about everything that you need in order to run jobs on a HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement!","title":"Thinking About Progress So Far"},{"location":"materials/htcondor/part2-ex1-files/#references","text":"There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read \"Submitting Jobs Without a Shared Filesystem\" in the HTCondor Manual.","title":"References"},{"location":"materials/htcondor/part2-ex2-queue-n/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.2: Use queue N , $(Cluster), and $(Process) \u00b6 Background \u00b6 Suppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices: Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, HTCondor's queue statement is here to help! Exercise Goal \u00b6 The goal of the next several exercises is to learn to submit many jobs from a single HTCondor queue statement, and to control things like filenames and arguments on a per-job basis when doing so. Running Many Jobs With One queue Statement \u00b6 Example Here is a Python program that uses a stochastic (random) method to estimate the value of \u03c0. The single argument to the program is the number of samples to take. More samples should result in better estimates! #!/usr/bin/env python3 import random import sys def mcpi ( iterations : int ) -> tuple [ float , int ]: \"\"\" This function uses Monte Carlo sampling to estimate the value of pi. \"\"\" inside_circle : int = 0 for i in range ( iterations ): x : float = random . random () y : float = random . random () if ( x ** 2 + y ** 2 ) <= 1 : inside_circle += 1 pi_estimate : float = 4 * ( inside_circle / iterations ) return pi_estimate , inside_circle if __name__ == \"__main__\" : # Print usage if no argument is provided if len ( sys . argv ) == 1 : print ( \"usage: mcpi.py ITERATIONS \\n \" ) exit ( 1 ) # Read in first argument iterations = int ( sys . argv [ 1 ]) # Estimate pi pi_estimate , inside_circle = mcpi ( iterations ) # Friendly printout print ( f \" { iterations } iterations, { inside_circle } inside; pi = { pi_estimate } \\n \" ) In a new directory for this exercise, create and save the code to a file named mcpi.py Test the program with just 1000 samples: [username@ap40]$ ./mcpi.py 1000 Now suppose that you want to run the program many times, to produce many estimates. To do so, we can tell HTCondor how many jobs to \"queue up\" via the queue statement we've been putting at the end of each of our submit files. Let\u2019s see how it works: Write a normal submit file for this program Pass 1 million ( 1000000 ) as the command line argument to mcpi.py Make sure to include log , output , and error (with filenames like mcpi.log ), and request_* lines At the end of the file, write queue 3 instead of just queue (\"queue 3 jobs\" vs. \"queue a job\"). Submit the file. Note the slightly different message from condor_submit : 3 job(s) submitted to cluster *NNNN*. Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Programmers like to start counting from 0.) Now we can understand what the first column in the output, the job ID , represents. It is a job\u2019s cluster number , a dot ( . ), and the job\u2019s process number . So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor's queue to list the status of all of the jobs from one cluster? How about one specific job ID? Using queue *N* With Output \u00b6 When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file? (Hopefully it is empty!) What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted? If the output is not what you expected, what do you think happened? Using $(Process) to Distinguish Jobs \u00b6 As you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory. After all, we didn't tell it to behave any differently when it ran three jobs. We need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. Do you remember how we did this in HTC Exercise 1.3 ? When processing a submit file, HTCondor will replace any instance of $(Process) with the process number of the job, for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) in the filename, similar to the example above. Delete any standard output, standard error, and log files from previous runs. Submit the updated file. When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process? Using $(Cluster) to Separate Files Across Runs \u00b6 With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like my-output-file-2444-0.out , myoutput-file-2444-1.out , myoutput-file-2444-2.out , etc. However, the next run would have different filenames, replacing 2444 with the new Cluster number of that run. Using $(Process) and $(Cluster) in Other Statements \u00b6 The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below: log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not easily let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises. (Optional) Defining JobBatchName for Tracking \u00b6 It is possible to define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. We will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a script and submit file that runs sleep 120 . Instead of a single queue statement, write this: jobbatchname = 1 queue 5 Submit the submit file to HTCondor. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: [username@ap40]$ condor_q -constraint 'JobBatchName == \"1\"' Do you get the output that you expected? Using the example command above, how would you list your other five jobs? (There will be more on how to use HTCondor constraints in later exercises.)","title":"2.2 - Use queue N, $(Cluster), and $(Process)"},{"location":"materials/htcondor/part2-ex2-queue-n/#htc-exercise-22-use-queue-n-cluster-and-process","text":"","title":"HTC Exercise 2.2: Use queue N, $(Cluster), and $(Process)"},{"location":"materials/htcondor/part2-ex2-queue-n/#background","text":"Suppose you have a program that you want to run many times with different arguments each time. With what you know so far, you have a couple of choices: Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, HTCondor's queue statement is here to help!","title":"Background"},{"location":"materials/htcondor/part2-ex2-queue-n/#exercise-goal","text":"The goal of the next several exercises is to learn to submit many jobs from a single HTCondor queue statement, and to control things like filenames and arguments on a per-job basis when doing so.","title":"Exercise Goal"},{"location":"materials/htcondor/part2-ex2-queue-n/#running-many-jobs-with-one-queue-statement","text":"Example Here is a Python program that uses a stochastic (random) method to estimate the value of \u03c0. The single argument to the program is the number of samples to take. More samples should result in better estimates! #!/usr/bin/env python3 import random import sys def mcpi ( iterations : int ) -> tuple [ float , int ]: \"\"\" This function uses Monte Carlo sampling to estimate the value of pi. \"\"\" inside_circle : int = 0 for i in range ( iterations ): x : float = random . random () y : float = random . random () if ( x ** 2 + y ** 2 ) <= 1 : inside_circle += 1 pi_estimate : float = 4 * ( inside_circle / iterations ) return pi_estimate , inside_circle if __name__ == \"__main__\" : # Print usage if no argument is provided if len ( sys . argv ) == 1 : print ( \"usage: mcpi.py ITERATIONS \\n \" ) exit ( 1 ) # Read in first argument iterations = int ( sys . argv [ 1 ]) # Estimate pi pi_estimate , inside_circle = mcpi ( iterations ) # Friendly printout print ( f \" { iterations } iterations, { inside_circle } inside; pi = { pi_estimate } \\n \" ) In a new directory for this exercise, create and save the code to a file named mcpi.py Test the program with just 1000 samples: [username@ap40]$ ./mcpi.py 1000 Now suppose that you want to run the program many times, to produce many estimates. To do so, we can tell HTCondor how many jobs to \"queue up\" via the queue statement we've been putting at the end of each of our submit files. Let\u2019s see how it works: Write a normal submit file for this program Pass 1 million ( 1000000 ) as the command line argument to mcpi.py Make sure to include log , output , and error (with filenames like mcpi.log ), and request_* lines At the end of the file, write queue 3 instead of just queue (\"queue 3 jobs\" vs. \"queue a job\"). Submit the file. Note the slightly different message from condor_submit : 3 job(s) submitted to cluster *NNNN*. Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 mcpi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Programmers like to start counting from 0.) Now we can understand what the first column in the output, the job ID , represents. It is a job\u2019s cluster number , a dot ( . ), and the job\u2019s process number . So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor's queue to list the status of all of the jobs from one cluster? How about one specific job ID?","title":"Running Many Jobs With One queue Statement"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-queue-n-with-output","text":"When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file? (Hopefully it is empty!) What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted? If the output is not what you expected, what do you think happened?","title":"Using queue *N* With Output"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-process-to-distinguish-jobs","text":"As you saw with the experiment above, each job ended up overwriting the same output and error filenames in the submission directory. After all, we didn't tell it to behave any differently when it ran three jobs. We need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. Do you remember how we did this in HTC Exercise 1.3 ? When processing a submit file, HTCondor will replace any instance of $(Process) with the process number of the job, for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) in the filename, similar to the example above. Delete any standard output, standard error, and log files from previous runs. Submit the updated file. When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process?","title":"Using $(Process) to Distinguish Jobs"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-cluster-to-separate-files-across-runs","text":"With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like my-output-file-2444-0.out , myoutput-file-2444-1.out , myoutput-file-2444-2.out , etc. However, the next run would have different filenames, replacing 2444 with the new Cluster number of that run.","title":"Using $(Cluster) to Separate Files Across Runs"},{"location":"materials/htcondor/part2-ex2-queue-n/#using-process-and-cluster-in-other-statements","text":"The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of submit file statements and not really for others. For example, consider using $(Cluster) or $(Process) in each of the below: log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not easily let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need a solution like those in the next exercises.","title":"Using $(Process) and $(Cluster) in Other Statements"},{"location":"materials/htcondor/part2-ex2-queue-n/#optional-defining-jobbatchname-for-tracking","text":"It is possible to define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. We will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a script and submit file that runs sleep 120 . Instead of a single queue statement, write this: jobbatchname = 1 queue 5 Submit the submit file to HTCondor. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: [username@ap40]$ condor_q -constraint 'JobBatchName == \"1\"' Do you get the output that you expected? Using the example command above, how would you list your other five jobs? (There will be more on how to use HTCondor constraints in later exercises.)","title":"(Optional) Defining JobBatchName for Tracking"},{"location":"materials/htcondor/part2-ex3-queue-from/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } HTC Exercise 2.3: Submit with \u201cqueue from\u201d \u00b6 Exercise Goals \u00b6 While using queue *N* , $(Cluster) , and $(Process) can be useful for some jobs, it's not great for all jobs. What if you want to submit the same task for differently named files? What if you want to iterate over non-numerical parameters? In the next two exercises, you will explore more ways to use a single submit file to submit many jobs. The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file. Background \u00b6 When submitting many jobs from a single submit file, always consider: What makes each job unique? In other words, there is one job per _____? How should you tell HTCondor to distinguish each job? For queue *N* , jobs are distinguished by the built-in $(Process) variable. But what if you need something more customized or something that's not a number? HTCondor can do that! HTCondor can distinguish and submit multiple jobs using custom variables. Example: Counting Words in Files \u00b6 Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. You could create separate submit files for each book and submit all of the files manually, but you'd have to edit the file each time. Below is an example submit file for this task. If we created a submit file for each text, we'd have to change the last five lines above the queue statement for each text we want to analyze. executable = freq.py request_memory = 1GB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = Alice_in_Wonderland.txt arguments = Alice_in_Wonderland.txt output = Alice_in_Wonderland.txt error = Alice_in_Wonderland.err log = Alice_in_Wonderland.log queue This would be overly verbose and tedious! Let's see a better way to automate this. Queue Jobs From a List of Values \u00b6 Suppose we want to modify our word-frequency analysis from a previous exercise so that it outputs only the top N most common words of a document. However, we want to experiment with different values of N . For this analysis, we will have a new version of the word-frequency counting script. First, we need a new version of the word counting program so that it accepts an extra number as a command line argument and prints only that many of the most common words. Here is the new code (it's not necessary to understand this code): #!/usr/bin/env python3 import os import sys import operator if len ( sys . argv ) != 3 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA NUM_WORDS' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] num_words = int ( sys . argv [ 2 ]) words = {} with open ( input_filename , 'r' ) as my_file : for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words [ - num_words :]: print ( f ' { word [ 0 ] } { word [ 1 ] : 8d } ' ) To submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename: Save the script as wordcount-top-n.py . Download and unpack some books from Project Gutenberg: [username@ap40]$ pelican object get osdf://ospool/uc-shared/public/school/2025/books.tar.gz . [username@ap40]$ tar -xzvf books.tar.gz You'll notice that the books are in a directory called books/ . For simplicity's sake, let's move the files to the current directory and remove the now-empty books/ directory. [username@ap40]$ mv books/*.txt . [username@ap40]$ rmdir books/ Create a new submit file (or base it off a previous one!) named wordcount-top.sub , including memory and disk requests of 20 MB. All of the jobs will use the same executable and log statements. Update other statements to work with two variables, book and n : output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = $(book) $(n) queue book, n from books_n.txt Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : Alice_in_Wonderland.txt, 10 Alice_in_Wonderland.txt, 25 Alice_in_Wonderland.txt, 50 Pride_and_Prejudice.txt, 10 Pride_and_Prejudice.txt, 25 Pride_and_Prejudice.txt, 50 Dracula.txt, 10 Dracula.txt, 25 Dracula.txt, 50 Huckleberry_Finn.txt, 10 Huckleberry_Finn.txt, 25 Huckleberry_Finn.txt, 50 Note that we used 3 different values for n for each book. Submit the file. Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created? Extra Challenge \u00b6 You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., Alice_in_Wonderland.txt.txt ), the output filenames contain with multiple extensions (e.g., Alice_in_Wonderland.txt.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file and variable file for this exercise so that the output filenames do not include the .txt extension.","title":"2.3 - Use queue from with custom variables"},{"location":"materials/htcondor/part2-ex3-queue-from/#htc-exercise-23-submit-with-queue-from","text":"","title":"HTC Exercise 2.3: Submit with \u201cqueue from\u201d"},{"location":"materials/htcondor/part2-ex3-queue-from/#exercise-goals","text":"While using queue *N* , $(Cluster) , and $(Process) can be useful for some jobs, it's not great for all jobs. What if you want to submit the same task for differently named files? What if you want to iterate over non-numerical parameters? In the next two exercises, you will explore more ways to use a single submit file to submit many jobs. The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file.","title":"Exercise Goals"},{"location":"materials/htcondor/part2-ex3-queue-from/#background","text":"When submitting many jobs from a single submit file, always consider: What makes each job unique? In other words, there is one job per _____? How should you tell HTCondor to distinguish each job? For queue *N* , jobs are distinguished by the built-in $(Process) variable. But what if you need something more customized or something that's not a number? HTCondor can do that! HTCondor can distinguish and submit multiple jobs using custom variables.","title":"Background"},{"location":"materials/htcondor/part2-ex3-queue-from/#example-counting-words-in-files","text":"Imagine you have a collection of books, and you want to analyze how word usage varies from book to book or author to author. You could create separate submit files for each book and submit all of the files manually, but you'd have to edit the file each time. Below is an example submit file for this task. If we created a submit file for each text, we'd have to change the last five lines above the queue statement for each text we want to analyze. executable = freq.py request_memory = 1GB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = Alice_in_Wonderland.txt arguments = Alice_in_Wonderland.txt output = Alice_in_Wonderland.txt error = Alice_in_Wonderland.err log = Alice_in_Wonderland.log queue This would be overly verbose and tedious! Let's see a better way to automate this.","title":"Example: Counting Words in Files"},{"location":"materials/htcondor/part2-ex3-queue-from/#queue-jobs-from-a-list-of-values","text":"Suppose we want to modify our word-frequency analysis from a previous exercise so that it outputs only the top N most common words of a document. However, we want to experiment with different values of N . For this analysis, we will have a new version of the word-frequency counting script. First, we need a new version of the word counting program so that it accepts an extra number as a command line argument and prints only that many of the most common words. Here is the new code (it's not necessary to understand this code): #!/usr/bin/env python3 import os import sys import operator if len ( sys . argv ) != 3 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA NUM_WORDS' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] num_words = int ( sys . argv [ 2 ]) words = {} with open ( input_filename , 'r' ) as my_file : for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words [ - num_words :]: print ( f ' { word [ 0 ] } { word [ 1 ] : 8d } ' ) To submit this program with a collection of two variable values for each run, one for the number of top words and one for the filename: Save the script as wordcount-top-n.py . Download and unpack some books from Project Gutenberg: [username@ap40]$ pelican object get osdf://ospool/uc-shared/public/school/2025/books.tar.gz . [username@ap40]$ tar -xzvf books.tar.gz You'll notice that the books are in a directory called books/ . For simplicity's sake, let's move the files to the current directory and remove the now-empty books/ directory. [username@ap40]$ mv books/*.txt . [username@ap40]$ rmdir books/ Create a new submit file (or base it off a previous one!) named wordcount-top.sub , including memory and disk requests of 20 MB. All of the jobs will use the same executable and log statements. Update other statements to work with two variables, book and n : output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = $(book) $(n) queue book, n from books_n.txt Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : Alice_in_Wonderland.txt, 10 Alice_in_Wonderland.txt, 25 Alice_in_Wonderland.txt, 50 Pride_and_Prejudice.txt, 10 Pride_and_Prejudice.txt, 25 Pride_and_Prejudice.txt, 50 Dracula.txt, 10 Dracula.txt, 25 Dracula.txt, 50 Huckleberry_Finn.txt, 10 Huckleberry_Finn.txt, 25 Huckleberry_Finn.txt, 50 Note that we used 3 different values for n for each book. Submit the file. Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created?","title":"Queue Jobs From a List of Values"},{"location":"materials/htcondor/part2-ex3-queue-from/#extra-challenge","text":"You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., Alice_in_Wonderland.txt.txt ), the output filenames contain with multiple extensions (e.g., Alice_in_Wonderland.txt.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file and variable file for this exercise so that the output filenames do not include the .txt extension.","title":"Extra Challenge"},{"location":"materials/htcondor/part2-ex4-queue-matching/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus HTC Exercise 2.4: Submit With \u201cqueue matching\u201d \u00b6 Exercise Goal \u00b6 Now that you've mastered many queue statements, it's time to learn another powerful queue statement! The goal of this exercise is to submit many jobs from a single submit file by using the queue ... matching syntax. With this, you can submit jobs with variables that match files with a specified pattern! Counting Words in Files \u00b6 Returning to our book word-counting example, let's pretend that instead of three books, we have an entire library. While we could list all of the text files in a books.txt file and use queue book from books.txt , it could be a tedious process, especially for tens of thousands of files. Luckily HTCondor provides a mechanism for submitting jobs based on pattern-matched files. Queue Jobs By Matching Filenames \u00b6 This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is a new version of the script (note, we removed the 'top n words' restriction): #!/usr/bin/env python3 import os import sys import operator if len ( sys . argv ) != 2 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} with open ( input_filename , 'r' ) as my_file : for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words : print ( f ' { word [ 0 ] } { word [ 1 ] : 8d } ' ) To use the script: Create and save this script as wordcount.py . Verify the script by running it on one book manually. Create a new submit file to submit one job (pick a book file and model your submit file off of the one above). Modify the following submit file statements to work for all books: transfer_input_files = $(book) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt Note As always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs. HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. How many jobs were created? Is this what you expected? If you ran this in the same directory as Exercise 2.3, you may have noticed that a job was submitted for the books_n.txt file that holds the variable values in the queue from statement. Beware the dangers of matching more files than intended! One solution may be to put all of the books into an books directory and queue matching books/*.txt . Can you think of other solutions? If you have time, try one! Extra Challenge 1 \u00b6 In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file. Extra Challenge 2 \u00b6 Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises?","title":"Bonus Exercise 2.4 - Use queue matching with a custom variable"},{"location":"materials/htcondor/part2-ex4-queue-matching/#bonus-htc-exercise-24-submit-with-queue-matching","text":"","title":"Bonus HTC Exercise 2.4: Submit With \u201cqueue matching\u201d"},{"location":"materials/htcondor/part2-ex4-queue-matching/#exercise-goal","text":"Now that you've mastered many queue statements, it's time to learn another powerful queue statement! The goal of this exercise is to submit many jobs from a single submit file by using the queue ... matching syntax. With this, you can submit jobs with variables that match files with a specified pattern!","title":"Exercise Goal"},{"location":"materials/htcondor/part2-ex4-queue-matching/#counting-words-in-files","text":"Returning to our book word-counting example, let's pretend that instead of three books, we have an entire library. While we could list all of the text files in a books.txt file and use queue book from books.txt , it could be a tedious process, especially for tens of thousands of files. Luckily HTCondor provides a mechanism for submitting jobs based on pattern-matched files.","title":"Counting Words in Files"},{"location":"materials/htcondor/part2-ex4-queue-matching/#queue-jobs-by-matching-filenames","text":"This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is a new version of the script (note, we removed the 'top n words' restriction): #!/usr/bin/env python3 import os import sys import operator if len ( sys . argv ) != 2 : print ( f 'Usage: { os . path . basename ( sys . argv [ 0 ]) } DATA' ) sys . exit ( 1 ) input_filename = sys . argv [ 1 ] words = {} with open ( input_filename , 'r' ) as my_file : for line in my_file : line_words = line . split () for word in line_words : if word in words : words [ word ] += 1 else : words [ word ] = 1 sorted_words = sorted ( words . items (), key = operator . itemgetter ( 1 )) for word in sorted_words : print ( f ' { word [ 0 ] } { word [ 1 ] : 8d } ' ) To use the script: Create and save this script as wordcount.py . Verify the script by running it on one book manually. Create a new submit file to submit one job (pick a book file and model your submit file off of the one above). Modify the following submit file statements to work for all books: transfer_input_files = $(book) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt Note As always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs. HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. How many jobs were created? Is this what you expected? If you ran this in the same directory as Exercise 2.3, you may have noticed that a job was submitted for the books_n.txt file that holds the variable values in the queue from statement. Beware the dangers of matching more files than intended! One solution may be to put all of the books into an books directory and queue matching books/*.txt . Can you think of other solutions? If you have time, try one!","title":"Queue Jobs By Matching Filenames"},{"location":"materials/htcondor/part2-ex4-queue-matching/#extra-challenge-1","text":"In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file.","title":"Extra Challenge 1"},{"location":"materials/htcondor/part2-ex4-queue-matching/#extra-challenge-2","text":"Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises?","title":"Extra Challenge 2"},{"location":"materials/ospool/part1-ex1-where-run/","text":"OSPool Exercise 1.1: Where Do Jobs Run? \u00b6 In the lecture about the Open Science Pool, we said that the OSPool is really just a big, strange HTCondor pool. Yesterday, you learned a great deal about HTCondor, and ran your jobs in the OSPool. So in a certain way, you will not be doing anything new in these exercises. Instead, let\u2019s do a bit of research on the OSPool itself , to learn more about this strange environment. Doing these exercises will help you better understand how to get the most out of the OSPool for your own research or the research you support or teach. Locating Your Jobs \u00b6 During the lecture, you saw the (partial) map of the institutions that contribute to the OSPool. But where do your jobs run in practice? Does each one go to a different location? Do they clump together? Do they all run at the nearest institution? Let\u2019s do an experiment! There is a single computational task here, and you will run it many times to collect a sample of results. The task simply returns information about the geographic location of the Execution Point it runs on. We provide the executable and associated data, so your work will be to transform this task into a job and run it many times. Once complete, you will manually combine the results and input them to a mapping service. Where in the world are my jobs? \u00b6 To find the physical location of the computers your jobs our running on, you will run a script against a dataset we compiled with geographic locations of known OSPool contributors. To start, you will reuse some basic HTCondor skills from the HTC exercises: Log in to ap40.uw.osg-htc.org (if not already) Create and change into a new folder for this exercise, for example ospool-ex11 Download the script and associated data: $ osdf object get /ospool/uc-shared/public/school/2025/locate-ospool-job ./ $ osdf object get /ospool/uc-shared/public/school/2025/ospool-site-locations.tsv ./ You will be using locate-ospool-job as your executable and ospool-site-locations.tsv as an input file. Create a submit file that queues one job to run locate-ospool-job , passes one argument for the input filename ospool-site-locations.tsv , transfers ospool-site-locations.tsv as an input file, and uses the $(Process) macro to write different output and error files. Try to do this step without looking at materials from the earlier exercises. But if you are stuck, see HTC Exercise 2.2 . Submit the one job and wait for the results Fix any bugs, remove old output files, then submit one batch of 50 jobs Combining your results \u00b6 When your jobs finish, it\u2019s time to combine them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: $ cat location-*.out The * is a wildcard, so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in combination with the sort and uniq commands using \u201cpipes\u201d ( | ) to print only the unique results: $ cat location-*.out | sort | uniq Mapping your results \u00b6 To see the locations of the Execution Points that your jobs ran on, use the MapCustomizer website: Go to the site Click on the \u201cBulk Entry\u201d button on the right-hand side Copy the combined (unique) results from your terminal window Paste them into the \u201cLocations\u201d text box (they are formatted correctly already) Click the \u201cAdd locations\u201d button to plot the locations Where did your jobs run? Did they spread out a lot, clump together, all go to the nearest institution, or something else? At this point in your learning, do you have any ideas about why they ran where they did? If you have time, talk to your neighbor(s) about it! Next exercise \u00b6 Once completed, move onto the next exercise: How Much Can I Get? . Extra Challenge: Cleaning up your submit directory \u00b6 If you run ls in the directory from which you submitted your job, you may see that you now have a hundred files or so. Proper data management starts to become a requirement as you start to scale up; it may be helpful to separate your submit files, code, and input data from your output data. Try editing your submit file so that all your output and error files are saved to separate directories within your submit directory. Tip Experiment with just one or two jobs per submission until you\u2019re confident you have it right, then go back to submitting 50 jobs. Remember: Test small and scale up! Submit your file and track the status of your jobs. Did your jobs complete successfully with output and error files saved in separate directories? If not, can you find any useful information in the job logs or hold messages? If you get stuck, review the slides from Monday .","title":"1.1 - Where do jobs run?"},{"location":"materials/ospool/part1-ex1-where-run/#ospool-exercise-11-where-do-jobs-run","text":"In the lecture about the Open Science Pool, we said that the OSPool is really just a big, strange HTCondor pool. Yesterday, you learned a great deal about HTCondor, and ran your jobs in the OSPool. So in a certain way, you will not be doing anything new in these exercises. Instead, let\u2019s do a bit of research on the OSPool itself , to learn more about this strange environment. Doing these exercises will help you better understand how to get the most out of the OSPool for your own research or the research you support or teach.","title":"OSPool Exercise 1.1: Where Do Jobs Run?"},{"location":"materials/ospool/part1-ex1-where-run/#locating-your-jobs","text":"During the lecture, you saw the (partial) map of the institutions that contribute to the OSPool. But where do your jobs run in practice? Does each one go to a different location? Do they clump together? Do they all run at the nearest institution? Let\u2019s do an experiment! There is a single computational task here, and you will run it many times to collect a sample of results. The task simply returns information about the geographic location of the Execution Point it runs on. We provide the executable and associated data, so your work will be to transform this task into a job and run it many times. Once complete, you will manually combine the results and input them to a mapping service.","title":"Locating Your Jobs"},{"location":"materials/ospool/part1-ex1-where-run/#where-in-the-world-are-my-jobs","text":"To find the physical location of the computers your jobs our running on, you will run a script against a dataset we compiled with geographic locations of known OSPool contributors. To start, you will reuse some basic HTCondor skills from the HTC exercises: Log in to ap40.uw.osg-htc.org (if not already) Create and change into a new folder for this exercise, for example ospool-ex11 Download the script and associated data: $ osdf object get /ospool/uc-shared/public/school/2025/locate-ospool-job ./ $ osdf object get /ospool/uc-shared/public/school/2025/ospool-site-locations.tsv ./ You will be using locate-ospool-job as your executable and ospool-site-locations.tsv as an input file. Create a submit file that queues one job to run locate-ospool-job , passes one argument for the input filename ospool-site-locations.tsv , transfers ospool-site-locations.tsv as an input file, and uses the $(Process) macro to write different output and error files. Try to do this step without looking at materials from the earlier exercises. But if you are stuck, see HTC Exercise 2.2 . Submit the one job and wait for the results Fix any bugs, remove old output files, then submit one batch of 50 jobs","title":"Where in the world are my jobs?"},{"location":"materials/ospool/part1-ex1-where-run/#combining-your-results","text":"When your jobs finish, it\u2019s time to combine them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: $ cat location-*.out The * is a wildcard, so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in combination with the sort and uniq commands using \u201cpipes\u201d ( | ) to print only the unique results: $ cat location-*.out | sort | uniq","title":"Combining your results"},{"location":"materials/ospool/part1-ex1-where-run/#mapping-your-results","text":"To see the locations of the Execution Points that your jobs ran on, use the MapCustomizer website: Go to the site Click on the \u201cBulk Entry\u201d button on the right-hand side Copy the combined (unique) results from your terminal window Paste them into the \u201cLocations\u201d text box (they are formatted correctly already) Click the \u201cAdd locations\u201d button to plot the locations Where did your jobs run? Did they spread out a lot, clump together, all go to the nearest institution, or something else? At this point in your learning, do you have any ideas about why they ran where they did? If you have time, talk to your neighbor(s) about it!","title":"Mapping your results"},{"location":"materials/ospool/part1-ex1-where-run/#next-exercise","text":"Once completed, move onto the next exercise: How Much Can I Get? .","title":"Next exercise"},{"location":"materials/ospool/part1-ex1-where-run/#extra-challenge-cleaning-up-your-submit-directory","text":"If you run ls in the directory from which you submitted your job, you may see that you now have a hundred files or so. Proper data management starts to become a requirement as you start to scale up; it may be helpful to separate your submit files, code, and input data from your output data. Try editing your submit file so that all your output and error files are saved to separate directories within your submit directory. Tip Experiment with just one or two jobs per submission until you\u2019re confident you have it right, then go back to submitting 50 jobs. Remember: Test small and scale up! Submit your file and track the status of your jobs. Did your jobs complete successfully with output and error files saved in separate directories? If not, can you find any useful information in the job logs or hold messages? If you get stuck, review the slides from Monday .","title":"Extra Challenge: Cleaning up your submit directory"},{"location":"materials/ospool/part1-ex2-capacity/","text":"OSPool Exercise 1.2: How Much Can I Get? \u00b6 As noted in the lecture, contributions to the OSPool typically come from 90\u2013100 sites and therefore vary a great deal. When you are thinking about a list of computational tasks and how to transform them into OSPool jobs, you may ask what is possible to get from the OSPool. In this exercise, you will focus on one resource: memory. But similar methods could be used for other kinds of resources. Getting Information From Free OSPool Slots \u00b6 Yesterday, you used the condor_status command to view information about available slots in the OSPool. Here is a more precise command that will output the number of CPU cores and memory (in MB) for each available slot and save that output as a CSV file: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs Memory > test.csv Let\u2019s examine this data a bit: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex12 Run the command above Open the resulting text file using less , nano , or whatever tool you like What do you see? At first, it probably appears to be a lot of noisy data! Are you surprised to see so many rows in which the first number \u2014 the number of CPU cores \u2014 is greater than one? See below for why. Do you notice repeated rows? For example, there may be many rows of 1, 4096 ; i.e., one CPU core and 4,096 MB (or 4 GB) or memory. Why would that be the case? Contributions come in many sizes \u00b6 As you can see, contributions to the OSPool come in many sizes, even when just considering the number of CPU cores and memory. Why is this so and what does it mean for running jobs on the OSPool? The variation in size mostly comes from the contributing institutions and site owners or admins, and how they think it is best to allocate capacity to the OSPool. Some sites prefer to share capacity in very fine-grained units \u2014 one core at a time, with associated memory and disk \u2014 whereas others prefer to share in larger units. Sites decide on a sharing policy that best fits into how they manage their total capacity, including how that fits in with local usage. What does HTCondor do with larger contributions, especially ones with multiple CPU cores? It is able to break up those larger chunks of capacity into ones that fit researcher jobs that are waiting to run. Suppose a contribution originally consists of 4 cores and 16 GB of memory. HTCondor may decide to split off 1 core and 2 GB of memory for a researcher job that needs that much, leaving 3 cores and 14 GB of memory still available. When the researcher job is done, HTCondor may reuse the 1 core/2 GB slot for other similar jobs, or return that capacity back to the original contribution. Because HTCondor does not give less fractional CPU cores to researcher jobs, our hypothetical 4 core/16 GB memory contribution can run at most four 1-core jobs. Memory (and other resources like disk) can be broken up (\u201cpartitioned\u201d) more finely. So, one reason you saw so much variation in available contributions is because contributions are varied! But also, you asked HTCondor to show available capacity, which means that you also saw partial contributions (like the 3 core/14 GB memory example above) waiting to be further partitioned and given to researcher jobs. The OSPool is constantly changing, as contributions come and go, and as HTCondor matches researcher jobs to available capacity, breaking up bigger contributions and recombining the smaller pieces and those jobs come and go. Analyzing the Data \u00b6 Now that you better understand the raw data you have collected, analyze it! This part is mostly up to you. You have a CSV file, and you can import that into a spreadsheet or other software, maybe even a quick script that you write. It may be interesting to plot the data, generate histograms, or calculate other statistics. Consider limiting yourself to 10\u201315 minutes for this step. First, here are some tips that may help with data analysis: We tend to talk about memory in GBs these days, but the default unit in HTCondor is MBs. You can get the raw data in GBs with a more complex command: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs \"eval(Memory / 1024.0)\" > cpus-memory-gb.csv For that matter, you can even add a column to the output for memory (in GBs) per core: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs \"eval(Memory / 1024.0)\" \"eval((Memory / 1024.0) / CPUs)\" > cpus-memory-percore.csv Finally, if you want to remove duplicate rows, you can add a sort -u command between condor_status and the output file: $ condor_status [ whatever options you want ] | sort -u > cpus-memory.csv Then, here are some specific questions to explore: This exercise is titled \u201cHow Much Can I Get?\u201d \u2014 so, how much memory can you get? What are the ranges available, especially if you break it down by number of cores? What is \u201ctypical\u201d? The \u201corange table\u201d in the slides (and in our documentation ) says that ideal jobs use less than a few GBs of memory. Clearly, more than that is available. Why do ideal jobs stay within a few GB? Related to the previous questions, if you calculate the available memory per core (i.e., memory / cores), what are the ranges available and what is typical? Why might this be a more relevant analysis? Thought experiment: If a contribution consists of, say, 24 cores and 156 GB memory, what happens if one single-core job takes all 156 GB memory? What is left of the contribution? What do you think HTCondor does with the remaining capacity? Discuss your analysis with neighbors and staff! What About Disk Capacity? \u00b6 You could certainly run the same kinds of commands and do the same kinds of analysis for disk capacity (in KB) instead of memory. However, the amount of variation in disk is orders of magnitude greater than for memory, and so this is probably not very interesting. Also to some extent, it is less interesting to ask about disk capacity. In short, disk is cheap and there tends to be a lot of it. And if you look carefully at the description of \u201cIdeal Jobs\u201d in the table, you will see we say nothing about disk needs, but rather focus on the amount of data to be transferred in and out of the Execution Point. That is because network capacity is usually a greater bottleneck than disk! There will be more on this topic on Wednesday. Next Exercise \u00b6 If this exercise was about analyzing a snapshot of available capacity, the next one is about how that capacity changes over time. When ready, move on to the next exercise: How Does Capacity Change?","title":"1.2 - How much can I get?"},{"location":"materials/ospool/part1-ex2-capacity/#ospool-exercise-12-how-much-can-i-get","text":"As noted in the lecture, contributions to the OSPool typically come from 90\u2013100 sites and therefore vary a great deal. When you are thinking about a list of computational tasks and how to transform them into OSPool jobs, you may ask what is possible to get from the OSPool. In this exercise, you will focus on one resource: memory. But similar methods could be used for other kinds of resources.","title":"OSPool Exercise 1.2: How Much Can I Get?"},{"location":"materials/ospool/part1-ex2-capacity/#getting-information-from-free-ospool-slots","text":"Yesterday, you used the condor_status command to view information about available slots in the OSPool. Here is a more precise command that will output the number of CPU cores and memory (in MB) for each available slot and save that output as a CSV file: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs Memory > test.csv Let\u2019s examine this data a bit: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex12 Run the command above Open the resulting text file using less , nano , or whatever tool you like What do you see? At first, it probably appears to be a lot of noisy data! Are you surprised to see so many rows in which the first number \u2014 the number of CPU cores \u2014 is greater than one? See below for why. Do you notice repeated rows? For example, there may be many rows of 1, 4096 ; i.e., one CPU core and 4,096 MB (or 4 GB) or memory. Why would that be the case?","title":"Getting Information From Free OSPool Slots"},{"location":"materials/ospool/part1-ex2-capacity/#contributions-come-in-many-sizes","text":"As you can see, contributions to the OSPool come in many sizes, even when just considering the number of CPU cores and memory. Why is this so and what does it mean for running jobs on the OSPool? The variation in size mostly comes from the contributing institutions and site owners or admins, and how they think it is best to allocate capacity to the OSPool. Some sites prefer to share capacity in very fine-grained units \u2014 one core at a time, with associated memory and disk \u2014 whereas others prefer to share in larger units. Sites decide on a sharing policy that best fits into how they manage their total capacity, including how that fits in with local usage. What does HTCondor do with larger contributions, especially ones with multiple CPU cores? It is able to break up those larger chunks of capacity into ones that fit researcher jobs that are waiting to run. Suppose a contribution originally consists of 4 cores and 16 GB of memory. HTCondor may decide to split off 1 core and 2 GB of memory for a researcher job that needs that much, leaving 3 cores and 14 GB of memory still available. When the researcher job is done, HTCondor may reuse the 1 core/2 GB slot for other similar jobs, or return that capacity back to the original contribution. Because HTCondor does not give less fractional CPU cores to researcher jobs, our hypothetical 4 core/16 GB memory contribution can run at most four 1-core jobs. Memory (and other resources like disk) can be broken up (\u201cpartitioned\u201d) more finely. So, one reason you saw so much variation in available contributions is because contributions are varied! But also, you asked HTCondor to show available capacity, which means that you also saw partial contributions (like the 3 core/14 GB memory example above) waiting to be further partitioned and given to researcher jobs. The OSPool is constantly changing, as contributions come and go, and as HTCondor matches researcher jobs to available capacity, breaking up bigger contributions and recombining the smaller pieces and those jobs come and go.","title":"Contributions come in many sizes"},{"location":"materials/ospool/part1-ex2-capacity/#analyzing-the-data","text":"Now that you better understand the raw data you have collected, analyze it! This part is mostly up to you. You have a CSV file, and you can import that into a spreadsheet or other software, maybe even a quick script that you write. It may be interesting to plot the data, generate histograms, or calculate other statistics. Consider limiting yourself to 10\u201315 minutes for this step. First, here are some tips that may help with data analysis: We tend to talk about memory in GBs these days, but the default unit in HTCondor is MBs. You can get the raw data in GBs with a more complex command: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs \"eval(Memory / 1024.0)\" > cpus-memory-gb.csv For that matter, you can even add a column to the output for memory (in GBs) per core: $ condor_status -avail -const 'Activity == \"Idle\"' -af:, CPUs \"eval(Memory / 1024.0)\" \"eval((Memory / 1024.0) / CPUs)\" > cpus-memory-percore.csv Finally, if you want to remove duplicate rows, you can add a sort -u command between condor_status and the output file: $ condor_status [ whatever options you want ] | sort -u > cpus-memory.csv Then, here are some specific questions to explore: This exercise is titled \u201cHow Much Can I Get?\u201d \u2014 so, how much memory can you get? What are the ranges available, especially if you break it down by number of cores? What is \u201ctypical\u201d? The \u201corange table\u201d in the slides (and in our documentation ) says that ideal jobs use less than a few GBs of memory. Clearly, more than that is available. Why do ideal jobs stay within a few GB? Related to the previous questions, if you calculate the available memory per core (i.e., memory / cores), what are the ranges available and what is typical? Why might this be a more relevant analysis? Thought experiment: If a contribution consists of, say, 24 cores and 156 GB memory, what happens if one single-core job takes all 156 GB memory? What is left of the contribution? What do you think HTCondor does with the remaining capacity? Discuss your analysis with neighbors and staff!","title":"Analyzing the Data"},{"location":"materials/ospool/part1-ex2-capacity/#what-about-disk-capacity","text":"You could certainly run the same kinds of commands and do the same kinds of analysis for disk capacity (in KB) instead of memory. However, the amount of variation in disk is orders of magnitude greater than for memory, and so this is probably not very interesting. Also to some extent, it is less interesting to ask about disk capacity. In short, disk is cheap and there tends to be a lot of it. And if you look carefully at the description of \u201cIdeal Jobs\u201d in the table, you will see we say nothing about disk needs, but rather focus on the amount of data to be transferred in and out of the Execution Point. That is because network capacity is usually a greater bottleneck than disk! There will be more on this topic on Wednesday.","title":"What About Disk Capacity?"},{"location":"materials/ospool/part1-ex2-capacity/#next-exercise","text":"If this exercise was about analyzing a snapshot of available capacity, the next one is about how that capacity changes over time. When ready, move on to the next exercise: How Does Capacity Change?","title":"Next Exercise"},{"location":"materials/ospool/part1-ex3-dynamic-capacity/","text":"OSPool Exercise 1.3: How Does Capacity Change? \u00b6 The previous exercise said that OSPool capacity is constantly changing. Let\u2019s explore that! This is one exercise that you cannot finish today. The main idea is to capture some statistics about the OSPool today, then capture the same statistics another day and compare. Capturing Some OSPool Statistics \u00b6 Below are several commands to capture statistics about available capacity and its characteristics in the OSPool. Each one saves its data to a text file. Note the sort | uniq -c part of each command \u2014 this reduces the output by combining identical lines of output and adding counts of identical lines. The count comes first. So, looking at the first few lines of saved output from the first (CPUs) command: 874 1 470 2 186 3 there were 874 slots available with one CPU core, 470 slots with two CPU cores, and 186 slots with three CPUs cores (and so on). The steps are easy here: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex13 Run each of the commands below Number of available slots by CPUs available in that slot: $ condor_status -avail -af CPUs | sort -n | uniq -c > cpus-snapshot1.txt Number of available slots by GPUs: $ condor_status -avail -af GPUs | sort -n | uniq -c > gpus-snapshot1.txt Number of available slots by memory per CPU core, rounded up to the nearest 2 GB: $ condor_status -avail -af \"eval(quantize(Memory/CPUs, 2048))\" | sort -n | uniq -c > memory-snapshot1.txt Number of available slots by operating system and version: $ condor_status -avail -af OpSysAndVer | sort | uniq -c > opsysandver-snapshot1.txt Number of available slots by HTCondor version of the Execution Point: $ condor_status -avail -af CondorVersion | sort | uniq -c > condorver-snapshot1.txt Number of available slots by Pelican plugin version on the Execution Point (which you will learn about Wednesday): $ condor_status -avail -af PelicanPluginVersion | sort | uniq -c > pelicanver-snapshot1.txt Of course, you are welcome to look at and think about the data you collected. And as usual, talk to neighbors or staff about it, if you like! Stop here and come back another day. (We\u2019ll try to remind you.) Capturing New Statistics \u00b6 Go back to the same exercise directory you used earlier, and run the same commands as before. However , be sure to change the output filenames for each one, or you will overwrite your previous data. Here are the commands again, with different output filenames: $ condor_status -avail -af CPUs | sort -n | uniq -c > cpus-snapshot2.txt $ condor_status -avail -af GPUs | sort -n | uniq -c > gpus-snapshot2.txt $ condor_status -avail -af \"eval(quantize(Memory/CPUs, 2048))\" | sort -n | uniq -c > memory-snapshot2.txt $ condor_status -avail -af OpSysAndVer | sort | uniq -c > opsysandver-snapshot2.txt $ condor_status -avail -af CondorVersion | sort | uniq -c > condorver-snapshot2.txt $ condor_status -avail -af PelicanPluginVersion | sort | uniq -c > pelicanver-snapshot2.txt Per type of file (cpus-, gpus-, etc.), compare your snapshots. Do you see any interesting differences? Some are more likely to change than others. Once again, talk to others about what you found, if you like. Next Exercise \u00b6 The last exercise and this one were about capacity. For the next one, we will look at one aspect of what an Execution Point is. When ready, move on to the next exercise: What Is In an Execution Point?","title":"1.3 - How Does Capacity Change?"},{"location":"materials/ospool/part1-ex3-dynamic-capacity/#ospool-exercise-13-how-does-capacity-change","text":"The previous exercise said that OSPool capacity is constantly changing. Let\u2019s explore that! This is one exercise that you cannot finish today. The main idea is to capture some statistics about the OSPool today, then capture the same statistics another day and compare.","title":"OSPool Exercise 1.3: How Does Capacity Change?"},{"location":"materials/ospool/part1-ex3-dynamic-capacity/#capturing-some-ospool-statistics","text":"Below are several commands to capture statistics about available capacity and its characteristics in the OSPool. Each one saves its data to a text file. Note the sort | uniq -c part of each command \u2014 this reduces the output by combining identical lines of output and adding counts of identical lines. The count comes first. So, looking at the first few lines of saved output from the first (CPUs) command: 874 1 470 2 186 3 there were 874 slots available with one CPU core, 470 slots with two CPU cores, and 186 slots with three CPUs cores (and so on). The steps are easy here: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex13 Run each of the commands below Number of available slots by CPUs available in that slot: $ condor_status -avail -af CPUs | sort -n | uniq -c > cpus-snapshot1.txt Number of available slots by GPUs: $ condor_status -avail -af GPUs | sort -n | uniq -c > gpus-snapshot1.txt Number of available slots by memory per CPU core, rounded up to the nearest 2 GB: $ condor_status -avail -af \"eval(quantize(Memory/CPUs, 2048))\" | sort -n | uniq -c > memory-snapshot1.txt Number of available slots by operating system and version: $ condor_status -avail -af OpSysAndVer | sort | uniq -c > opsysandver-snapshot1.txt Number of available slots by HTCondor version of the Execution Point: $ condor_status -avail -af CondorVersion | sort | uniq -c > condorver-snapshot1.txt Number of available slots by Pelican plugin version on the Execution Point (which you will learn about Wednesday): $ condor_status -avail -af PelicanPluginVersion | sort | uniq -c > pelicanver-snapshot1.txt Of course, you are welcome to look at and think about the data you collected. And as usual, talk to neighbors or staff about it, if you like! Stop here and come back another day. (We\u2019ll try to remind you.)","title":"Capturing Some OSPool Statistics"},{"location":"materials/ospool/part1-ex3-dynamic-capacity/#capturing-new-statistics","text":"Go back to the same exercise directory you used earlier, and run the same commands as before. However , be sure to change the output filenames for each one, or you will overwrite your previous data. Here are the commands again, with different output filenames: $ condor_status -avail -af CPUs | sort -n | uniq -c > cpus-snapshot2.txt $ condor_status -avail -af GPUs | sort -n | uniq -c > gpus-snapshot2.txt $ condor_status -avail -af \"eval(quantize(Memory/CPUs, 2048))\" | sort -n | uniq -c > memory-snapshot2.txt $ condor_status -avail -af OpSysAndVer | sort | uniq -c > opsysandver-snapshot2.txt $ condor_status -avail -af CondorVersion | sort | uniq -c > condorver-snapshot2.txt $ condor_status -avail -af PelicanPluginVersion | sort | uniq -c > pelicanver-snapshot2.txt Per type of file (cpus-, gpus-, etc.), compare your snapshots. Do you see any interesting differences? Some are more likely to change than others. Once again, talk to others about what you found, if you like.","title":"Capturing New Statistics"},{"location":"materials/ospool/part1-ex3-dynamic-capacity/#next-exercise","text":"The last exercise and this one were about capacity. For the next one, we will look at one aspect of what an Execution Point is. When ready, move on to the next exercise: What Is In an Execution Point?","title":"Next Exercise"},{"location":"materials/ospool/part1-ex4-ep-sandbox/","text":"OSPool Exercise 1.4: What Is In an Execution Point? \u00b6 When using the OSPool, or any HTCondor pool, it can be difficult to build a mental model of the Execution Point and what is going on there while your job is running. In particular, it may be hard to imagine what your working directory on the Execution Point is like and how that compares to your working directory on the Access Point. So let\u2019s explore both! Capturing Details of Execution Point Directories \u00b6 In this exercise, you will run jobs on the OSPool to collect information about what Execution Point \u201csandbox\u201d directories look like. Every time that HTCondor starts your job on an Execution Point, it creates and changes to a \u201csandbox\u201d directory \u2014 your job\u2019s temporary working space within the Execution Point. Here is a simple shell script that captures a bit of data about the sandbox directory it is running in: #!/bin/sh # Print data elements to standard out: # - Current date and time of the system running this script # - Fully-qualified (long) hostname of the system # - Path to the current working directory # - The site name of the system echo \"$(date),$(hostname -f),$(pwd),${OSG_SITE_NAME}\" # Dump a recursive directory listing of the current working directory to an output file ls -lR \"`pwd`\" > output.txt 2>&1 Here\u2019s what to do: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex14 Create a file named fs-probe.sh with the shell script above Create an HTCondor submit file to run this script Tips: here are no command-line arguments and no input file. Save standard output and error to filenames that include the HTCondor process ID. Request 1 GB of memory and just 200 KB of disk. If you are not sure about how to do these things, look at Monday\u2019s slides or exercises, or ask around. Add special commands to the submit file to handle the output file(s) Because the script creates an output file with the same name in each job, not only do we need to transfer the file back but we need to rename it, too. You\u2019ll learn more about this technique tomorrow, but for now, just add these two lines to your submit file: transfer_output_files = output.txt transfer_output_remaps = \"output.txt = fsprobe-$(PROCESS)-ls-output.txt\" Try running just one job like this and fix any issues When ready, run 20\u201330 copies of this job in one submission Reviewing the Data \u00b6 The script produces CSV output in the standard output files and a recursive directory listing of the Execution Point sandbox directory in each fsprobe-*-ls-output.txt . Let\u2019s examine each type of output in turn. CSV output \u00b6 You can combine the CSV outputs as follows (assuming you named your output files something-ProcID.out ): cat *.out > combined-output.csv Look at that combined file directly or, probably better yet, copy the contents to a CSV file on your laptop and open it as a spreadsheet. Use the script comments to understand what each column is, but in summary: timestamp, EP hostname, sandbox directory path, site name. The key here is to look through the sandbox directory paths. Are they all the same? Are any of them the same as your working directory for this exercise on the Access Point? Why or why not? Note Do you see some sandbox paths that are just /srv ? If so, that job ran inside a default OSPool container (that we provided). You will learn more about containers later today, but the key idea here is that your job cannot see the host filesystem outside of the container, and we (as creators of the container image) arbitrarily picked /srv as the sandbox directory. Directory Listings \u00b6 Look through each of the directory listing files ( fsprobe-N-ls-output.txt ). (Fun Linux hack: You can view them all at once by running less *ls-output.txt and then typing :n to go to the next file in the set or :p to go backward; the current filename is at the lower left and q quits the less program.) Each directory listing captures what files and directories (recursively) were in the sandbox directory at the moment the shell script ran the ls command. What was there? What was not there? How are these directories similar to and different from each other. How are they similar to and different from your working directory on the Access Point. Think about these things when preparing and running jobs for the OSPool! Next Exercise \u00b6 This was the final suggested exercise of this section. But if you like, there is a Bonus Exercise to look at various views of OSPool information: Viewing OSPool Information","title":"1.4 - What Is In an Execution Point?"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#ospool-exercise-14-what-is-in-an-execution-point","text":"When using the OSPool, or any HTCondor pool, it can be difficult to build a mental model of the Execution Point and what is going on there while your job is running. In particular, it may be hard to imagine what your working directory on the Execution Point is like and how that compares to your working directory on the Access Point. So let\u2019s explore both!","title":"OSPool Exercise 1.4: What Is In an Execution Point?"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#capturing-details-of-execution-point-directories","text":"In this exercise, you will run jobs on the OSPool to collect information about what Execution Point \u201csandbox\u201d directories look like. Every time that HTCondor starts your job on an Execution Point, it creates and changes to a \u201csandbox\u201d directory \u2014 your job\u2019s temporary working space within the Execution Point. Here is a simple shell script that captures a bit of data about the sandbox directory it is running in: #!/bin/sh # Print data elements to standard out: # - Current date and time of the system running this script # - Fully-qualified (long) hostname of the system # - Path to the current working directory # - The site name of the system echo \"$(date),$(hostname -f),$(pwd),${OSG_SITE_NAME}\" # Dump a recursive directory listing of the current working directory to an output file ls -lR \"`pwd`\" > output.txt 2>&1 Here\u2019s what to do: Make sure you are logged in to ap40.uw.osg-htc.org Create and change into a new folder for this exercise, for example ospool-ex14 Create a file named fs-probe.sh with the shell script above Create an HTCondor submit file to run this script Tips: here are no command-line arguments and no input file. Save standard output and error to filenames that include the HTCondor process ID. Request 1 GB of memory and just 200 KB of disk. If you are not sure about how to do these things, look at Monday\u2019s slides or exercises, or ask around. Add special commands to the submit file to handle the output file(s) Because the script creates an output file with the same name in each job, not only do we need to transfer the file back but we need to rename it, too. You\u2019ll learn more about this technique tomorrow, but for now, just add these two lines to your submit file: transfer_output_files = output.txt transfer_output_remaps = \"output.txt = fsprobe-$(PROCESS)-ls-output.txt\" Try running just one job like this and fix any issues When ready, run 20\u201330 copies of this job in one submission","title":"Capturing Details of Execution Point Directories"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#reviewing-the-data","text":"The script produces CSV output in the standard output files and a recursive directory listing of the Execution Point sandbox directory in each fsprobe-*-ls-output.txt . Let\u2019s examine each type of output in turn.","title":"Reviewing the Data"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#csv-output","text":"You can combine the CSV outputs as follows (assuming you named your output files something-ProcID.out ): cat *.out > combined-output.csv Look at that combined file directly or, probably better yet, copy the contents to a CSV file on your laptop and open it as a spreadsheet. Use the script comments to understand what each column is, but in summary: timestamp, EP hostname, sandbox directory path, site name. The key here is to look through the sandbox directory paths. Are they all the same? Are any of them the same as your working directory for this exercise on the Access Point? Why or why not? Note Do you see some sandbox paths that are just /srv ? If so, that job ran inside a default OSPool container (that we provided). You will learn more about containers later today, but the key idea here is that your job cannot see the host filesystem outside of the container, and we (as creators of the container image) arbitrarily picked /srv as the sandbox directory.","title":"CSV output"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#directory-listings","text":"Look through each of the directory listing files ( fsprobe-N-ls-output.txt ). (Fun Linux hack: You can view them all at once by running less *ls-output.txt and then typing :n to go to the next file in the set or :p to go backward; the current filename is at the lower left and q quits the less program.) Each directory listing captures what files and directories (recursively) were in the sandbox directory at the moment the shell script ran the ls command. What was there? What was not there? How are these directories similar to and different from each other. How are they similar to and different from your working directory on the Access Point. Think about these things when preparing and running jobs for the OSPool!","title":"Directory Listings"},{"location":"materials/ospool/part1-ex4-ep-sandbox/#next-exercise","text":"This was the final suggested exercise of this section. But if you like, there is a Bonus Exercise to look at various views of OSPool information: Viewing OSPool Information","title":"Next Exercise"},{"location":"materials/ospool/part1-ex5-ospool-views/","text":"OSPool Exercise 1.5 (Optional): Viewing OSPool Information \u00b6 As a final \u2014 and completely optional \u2014 step in our exploration of the OSPool, we invite you to look at some of the views on the OSPool that we provide publicly for funding agencies, researchers, capacity contributors, and even ourselves. Maybe these views will help you better understand the scope and dynamic nature of the OSPool. Maps \u00b6 You have seen at least one map of OSPool contributors so far. Where do these maps come from? Please visit https://map.osg-htc.org/ . From the View drop-down menu, try different options. The default \u201cOSG Sites\u201d view is not well defined\u2026 maybe don\u2019t think too hard about that one! CC* refers to the NSF Campus Cyberinfrastructure program, which has funded (as you can see) many sites that contribute to the OSPool. You will learn more about the \u201cOpen Science Data Federation\u201d (or OSDF) on Wednesday Projects \u00b6 You can explore some information about research projects that have used the OSPool: Please visit https://osg-htc.org/services/ospool/projects.html ; be patient, these pages get live data and take a while to fully load. As the page says, the data is for projects that ran at least 100 jobs in the OSPool in the past year. Note that most projects are named after the home institution and last name of the project PI. For example, FIU_Li is a project based at Florida International University, and t\u200b\u200bhe PI is Jun Li. Often, the PI is not the person running jobs, but rather students and post-docs. Continuing the FIU_Li example, Professor Li is not running these jobs\u2026 but the person who is running them is right here in the auditorium with us! Who is it?!? Scroll down to see a table of projects; click on a project name to learn more about that project. CE Dashboard \u00b6 For a view from the capacity contributor\u2019s perspective, we have a new feature called the \u201cCE Dashboard\u201d \u2014 the name comes from the HTCondor-CE, from which the source data is collected. Please visit https://ce-dashboard.ospool.osg-htc.org/index.html . Click on a CE name to see information about the past 2 weeks of contributions (for a local example, scroll down and try \u201cCHTC-Spark-CE1\u201d). You can see information about the recent history of CPU, memory, and disk contributions and allocations to researcher jobs. Scroll down to see information about projects (as in the previous section) that ran on the site. There is too much information to describe here, but if you are interested in this view, talk to the staff, especially Tim C. who uses the CE Dashboard daily. What\u2019s Next? \u00b6 This is the last exercise in the OSPool series. Either move on to the next series, if ready, or work on applying what you have learned to your own research or the research you support.","title":"Bonus Exercise 1.5 - Viewing OSPool Information"},{"location":"materials/ospool/part1-ex5-ospool-views/#ospool-exercise-15-optional-viewing-ospool-information","text":"As a final \u2014 and completely optional \u2014 step in our exploration of the OSPool, we invite you to look at some of the views on the OSPool that we provide publicly for funding agencies, researchers, capacity contributors, and even ourselves. Maybe these views will help you better understand the scope and dynamic nature of the OSPool.","title":"OSPool Exercise 1.5 (Optional): Viewing OSPool Information"},{"location":"materials/ospool/part1-ex5-ospool-views/#maps","text":"You have seen at least one map of OSPool contributors so far. Where do these maps come from? Please visit https://map.osg-htc.org/ . From the View drop-down menu, try different options. The default \u201cOSG Sites\u201d view is not well defined\u2026 maybe don\u2019t think too hard about that one! CC* refers to the NSF Campus Cyberinfrastructure program, which has funded (as you can see) many sites that contribute to the OSPool. You will learn more about the \u201cOpen Science Data Federation\u201d (or OSDF) on Wednesday","title":"Maps"},{"location":"materials/ospool/part1-ex5-ospool-views/#projects","text":"You can explore some information about research projects that have used the OSPool: Please visit https://osg-htc.org/services/ospool/projects.html ; be patient, these pages get live data and take a while to fully load. As the page says, the data is for projects that ran at least 100 jobs in the OSPool in the past year. Note that most projects are named after the home institution and last name of the project PI. For example, FIU_Li is a project based at Florida International University, and t\u200b\u200bhe PI is Jun Li. Often, the PI is not the person running jobs, but rather students and post-docs. Continuing the FIU_Li example, Professor Li is not running these jobs\u2026 but the person who is running them is right here in the auditorium with us! Who is it?!? Scroll down to see a table of projects; click on a project name to learn more about that project.","title":"Projects"},{"location":"materials/ospool/part1-ex5-ospool-views/#ce-dashboard","text":"For a view from the capacity contributor\u2019s perspective, we have a new feature called the \u201cCE Dashboard\u201d \u2014 the name comes from the HTCondor-CE, from which the source data is collected. Please visit https://ce-dashboard.ospool.osg-htc.org/index.html . Click on a CE name to see information about the past 2 weeks of contributions (for a local example, scroll down and try \u201cCHTC-Spark-CE1\u201d). You can see information about the recent history of CPU, memory, and disk contributions and allocations to researcher jobs. Scroll down to see information about projects (as in the previous section) that ran on the site. There is too much information to describe here, but if you are interested in this view, talk to the staff, especially Tim C. who uses the CE Dashboard daily.","title":"CE Dashboard"},{"location":"materials/ospool/part1-ex5-ospool-views/#whats-next","text":"This is the last exercise in the OSPool series. Either move on to the next series, if ready, or work on applying what you have learned to your own research or the research you support.","title":"What\u2019s Next?"},{"location":"materials/scaling/part1-ex1-organization/","text":"Organizing HTC Workloads \u00b6 Exercise Goal \u00b6 When working with large datasets or running a task many times over (e.g. across subsets, parameters, or files), organizing your workload becomes essential. High-throughput computing (HTC) allows you to run thousands of independent jobs efficiently, but success depends on how well you prepare your files and manage your project structure. This exercise introduces core principles of HTC workflow organization through a practical example. While the specific task\u2014read mapping\u2014comes from bioinformatics, the underlying strategies apply to any domain that handles data at scale. You'll learn how to: Plan and organize your workload for high-throughput execution Structure input, output, and support files for efficient job management Track outputs, logs, and errors across many jobs in a clean, reproducible way In this exercise, you\u2019ll use a typical bioinformatics read mapping workflow to explore how to sustainably scale your workloads on the OSPool. You\u2019ll focus on job-level organization, building a multi-job submit file, tracking job progress with logs, and troubleshooting job failures. Log into an OSPool Access Point \u00b6 Make sure you are logged into ap40.uw.osg-htc.org . Get Files \u00b6 To get the files for this exercise: Make a new directory in ~/scaling-up/ and change directory into it Use pelican to get the input files for our exercises: osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.mmi ./ osdf object get /ospool/uc-shared/public/school/2025/reads.fastq ./ osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif ./ Slow Download Speeds Some files, particularly reads.fastq , may take a few minutes to download. If the issue remains for more than 10 minutes, reach out to one of the School staff members for help. Using Files from the Data Exercises section We will be reusing some of the files from the Data Exercises 2 - OSDF for inputs and OSDF for outputs sections. If you did not complete these or wish to simply get a fresh set of these files, use the command below: osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.mmi ./ osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif ./ Our Workload \u00b6 Imagine you're working with a large set of DNA sequencing data from an organism. The sequencing file contains millions of snippets of DNA, which we call reads . Your goal is to figure out where these reads came from by matching them to a known reference genome using a read mapping tool. One commonly used tool for this task is minimap2 , which quickly finds where each DNA read best matches the reference. For this exercise, we'll use minimap2 to map our reads to a reference genome. To do that, we might run a command like: $ minimap2 -ax map-ont [ Celegans_ref.mmi ] [ sequencing_reads.fastq ] > output.sam Here\u2019s what these files are: - Celegans_ref.mmi : a file containing the complete reference genome we\u2019re mapping to. - sequencing_reads.fastq : a file containing millions of DNA fragments, called reads , from a sequencing machine. - output.sam : a SAM file , which is a standard text format, used to store the results of mapping\u2014where each read aligns in the genome. We want to run this command to map all our reads against the reference genome. FASTQ files contain 4 lines per read, you can run the following command to calculate the number of reads in your FASTQ file: $ expr $( wc -l < reads.fastq ) / 4 $ 1392453 Read mapping using algorithms, like minimap2 , do not scale up well by simply adding additional CPUs to the problem. These mappers typically plateau their speed around 2-4 CPUs. This problem, however, can be solved by employing a \"divide and conquer\" approach. With this approach, we can subdivide our input reads.fastq file into smaller subsets which can be submitting to HTCondor as a set of independent parallel-running jobs. We expect to be working with the following files in each of our jobs: Inputs A subset of reads.fastq - reads_subset_a.fastq The indexed reference genome - Celegans_ref.mmi A minimap2 container image - minimap2.sif The executable - run_minimap2.sh Outputs A SAM-formatted output file - reads_subset_a.sam System Generated Files A set of log, standard error, and standard out files Make an Organization Plan \u00b6 Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the Access Point? There will also be system and HTCondor files produced when we submit a job \u2014 how would you organize the log, standard output, and standard error files? When to Use the Open Science Data Federation (OSDF) Make sure to consider which files will be re-used often (common files across all jobs) versus which files will be used only once. Files often re-used, can be placed in your /ospool/ap40/data/<user.name>/ directory to take advantage of the caching benefits when using the osdf:// transfer plugin. Organize Files \u00b6 There are many different ways to organize files; a simple method that works for most workloads is having a directory for your input files and a directory for your output files. For our exercise, we will use the following data organizational structure: \u251c\u2500\u2500 /home/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u251c\u2500\u2500 outputs \u2502 \u2502 \u251c\u2500\u2500 logs \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u251c\u2500\u2500 error \u2502 \u2502 \u2502 \u251c\u2500\u2500 output \u251c\u2500\u2500 /ospool/ap40/data/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u251c\u2500\u2500 software Set up this structure on the command line by running: mkdir -p inputs mkdir -p outputs mkdir -p logs mkdir -p logs/log mkdir -p logs/error mkdir -p logs/output cd /ospool/ap40/data/$USER/ mkdir -p scaling-up/inputs mkdir -p scaling-up/software Move the reads.fastq file to your inputs directory (the one under /home ) using the mv command. Move the Celegans_ref.mmi file to your /ospool/ap40/data/<user.name>/scaling-up/inputs directory using the mv command. Move the minimap2.sif container image file to your /ospool/ap40/data/<user.name>/scaling-up/software directory using the mv command. Stop and Consider: Why are we moving our files to these directories? When preparing your jobs for high-throughput computing, think about how each file will be used. Will the file be reused across many jobs? Or is it specific to a single job? Will it need to be transferred repeatedly\u2014or just once? With that in mind: - Why might it make sense to place reads.fastq in your /home directory? - Why are we storing Celegans_ref.mmi and minimap2.sif in /ospool/ap40/data/ instead? Solution Files in /home are transferred to each job, directly from the AP to the EP. These files do not get cached in the OSDF, which makes sense for files that change across jobs\u2014such as subsets of reads.fastq . But Celegans_ref.mmi and minimap2.sif are the same in every job. By storing them in /ospool/ap40/data/ and using osdf:// to access them, we avoid repeatedly transferring them. Instead, they're cached near where jobs run, which speeds things up and reduces network load. Wrangling the Data \u00b6 To get ready for our mapping step, we need to prepare our read files. This includes two crucial steps, splitting our reads. Navigate to your ~/scaling-up/inputs/ directory. cd ~/scaling-up/inputs/ Split the FASTQ file into subsets. We can divide our reads.fastq into subsets of ~ 14,000 reads per subset. Since each FASTQ read consist of four lines in the FASTQ file, we can split reads.fastq every 56,000 lines. split -l 56000 reads.fastq reads_fastq_chunk_ rm reads.fastq This will generate 100 read subset files with the prefix eads_fastq_chunk_ . Subsetting Data - Your Milage May Vary When subsetting your data, you should exercise your own judgement. The ideal job profile on the OSPool typically looks something like: Runtime: Between 10mins and <10hrs Memory (RAM): 1-5 GB Inputs: <1 GB Outputs: <1 GB Jobs with larger profiles may still run, but will likely face significant increases in idle state while waiting for a machine to match. Delete (or move) the reads.fastq file from the input directory. In the next exercise, we will generate a list of jobs for HTCondor using the the subset files in this directory. To avoid accidentally including the reads.fastq file in our list of jobs, please delete it or move it out of this directory. Organization in HTC is Critical! One of the most important steps in scaling up our workflows to run on HTC systems, such as the OSPool, is maintaining a clear organizational structure. This includes deleting files we will not be using anymore. For the rest of the exercise, we will not be using the reads.fastq file after splitting it. Not deleting this file can cause downstream issues. Do not skip this step. Review Your Progress \u00b6 Now that we've set up our directory structure and pre-processed our data, we can focus on preparing our submission scripts and getting our jobs running! Checking your progress Before you move on, take this time to comb through your directory structure and compare it to the structure below. View the current directory and its subdirectories by using the ls command with the recursive ( -R ) flag. Your overall structure should look something like this: \u251c\u2500\u2500 /home/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_a \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_b \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_c \u2502 \u2502 \u251c\u2500\u2500 outputs \u2502 \u2502 \u251c\u2500\u2500 logs \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u251c\u2500\u2500 error \u2502 \u2502 \u2502 \u251c\u2500\u2500 output \u251c\u2500\u2500 /ospool/ap40/data/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u2502 \u251c\u2500\u2500 Celegans_ref.mmi \u2502 \u2502 \u251c\u2500\u2500 software \u2502 \u2502 \u2502 \u251c\u2500\u2500 minimap2.sif","title":"1.1 - Organizing HTC workloads"},{"location":"materials/scaling/part1-ex1-organization/#organizing-htc-workloads","text":"","title":"Organizing HTC Workloads"},{"location":"materials/scaling/part1-ex1-organization/#exercise-goal","text":"When working with large datasets or running a task many times over (e.g. across subsets, parameters, or files), organizing your workload becomes essential. High-throughput computing (HTC) allows you to run thousands of independent jobs efficiently, but success depends on how well you prepare your files and manage your project structure. This exercise introduces core principles of HTC workflow organization through a practical example. While the specific task\u2014read mapping\u2014comes from bioinformatics, the underlying strategies apply to any domain that handles data at scale. You'll learn how to: Plan and organize your workload for high-throughput execution Structure input, output, and support files for efficient job management Track outputs, logs, and errors across many jobs in a clean, reproducible way In this exercise, you\u2019ll use a typical bioinformatics read mapping workflow to explore how to sustainably scale your workloads on the OSPool. You\u2019ll focus on job-level organization, building a multi-job submit file, tracking job progress with logs, and troubleshooting job failures.","title":"Exercise Goal"},{"location":"materials/scaling/part1-ex1-organization/#log-into-an-ospool-access-point","text":"Make sure you are logged into ap40.uw.osg-htc.org .","title":"Log into an OSPool Access Point"},{"location":"materials/scaling/part1-ex1-organization/#get-files","text":"To get the files for this exercise: Make a new directory in ~/scaling-up/ and change directory into it Use pelican to get the input files for our exercises: osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.mmi ./ osdf object get /ospool/uc-shared/public/school/2025/reads.fastq ./ osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif ./ Slow Download Speeds Some files, particularly reads.fastq , may take a few minutes to download. If the issue remains for more than 10 minutes, reach out to one of the School staff members for help. Using Files from the Data Exercises section We will be reusing some of the files from the Data Exercises 2 - OSDF for inputs and OSDF for outputs sections. If you did not complete these or wish to simply get a fresh set of these files, use the command below: osdf object get /ospool/uc-shared/public/school/2025/Celegans_ref.mmi ./ osdf object get /ospool/uc-shared/public/school/2025/minimap2.sif ./","title":"Get Files"},{"location":"materials/scaling/part1-ex1-organization/#our-workload","text":"Imagine you're working with a large set of DNA sequencing data from an organism. The sequencing file contains millions of snippets of DNA, which we call reads . Your goal is to figure out where these reads came from by matching them to a known reference genome using a read mapping tool. One commonly used tool for this task is minimap2 , which quickly finds where each DNA read best matches the reference. For this exercise, we'll use minimap2 to map our reads to a reference genome. To do that, we might run a command like: $ minimap2 -ax map-ont [ Celegans_ref.mmi ] [ sequencing_reads.fastq ] > output.sam Here\u2019s what these files are: - Celegans_ref.mmi : a file containing the complete reference genome we\u2019re mapping to. - sequencing_reads.fastq : a file containing millions of DNA fragments, called reads , from a sequencing machine. - output.sam : a SAM file , which is a standard text format, used to store the results of mapping\u2014where each read aligns in the genome. We want to run this command to map all our reads against the reference genome. FASTQ files contain 4 lines per read, you can run the following command to calculate the number of reads in your FASTQ file: $ expr $( wc -l < reads.fastq ) / 4 $ 1392453 Read mapping using algorithms, like minimap2 , do not scale up well by simply adding additional CPUs to the problem. These mappers typically plateau their speed around 2-4 CPUs. This problem, however, can be solved by employing a \"divide and conquer\" approach. With this approach, we can subdivide our input reads.fastq file into smaller subsets which can be submitting to HTCondor as a set of independent parallel-running jobs. We expect to be working with the following files in each of our jobs: Inputs A subset of reads.fastq - reads_subset_a.fastq The indexed reference genome - Celegans_ref.mmi A minimap2 container image - minimap2.sif The executable - run_minimap2.sh Outputs A SAM-formatted output file - reads_subset_a.sam System Generated Files A set of log, standard error, and standard out files","title":"Our Workload"},{"location":"materials/scaling/part1-ex1-organization/#make-an-organization-plan","text":"Based on what you know about the script, inputs, and outputs, how would you organize this HTC workload in directories (folders) on the Access Point? There will also be system and HTCondor files produced when we submit a job \u2014 how would you organize the log, standard output, and standard error files? When to Use the Open Science Data Federation (OSDF) Make sure to consider which files will be re-used often (common files across all jobs) versus which files will be used only once. Files often re-used, can be placed in your /ospool/ap40/data/<user.name>/ directory to take advantage of the caching benefits when using the osdf:// transfer plugin.","title":"Make an Organization Plan"},{"location":"materials/scaling/part1-ex1-organization/#organize-files","text":"There are many different ways to organize files; a simple method that works for most workloads is having a directory for your input files and a directory for your output files. For our exercise, we will use the following data organizational structure: \u251c\u2500\u2500 /home/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u251c\u2500\u2500 outputs \u2502 \u2502 \u251c\u2500\u2500 logs \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u251c\u2500\u2500 error \u2502 \u2502 \u2502 \u251c\u2500\u2500 output \u251c\u2500\u2500 /ospool/ap40/data/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u251c\u2500\u2500 software Set up this structure on the command line by running: mkdir -p inputs mkdir -p outputs mkdir -p logs mkdir -p logs/log mkdir -p logs/error mkdir -p logs/output cd /ospool/ap40/data/$USER/ mkdir -p scaling-up/inputs mkdir -p scaling-up/software Move the reads.fastq file to your inputs directory (the one under /home ) using the mv command. Move the Celegans_ref.mmi file to your /ospool/ap40/data/<user.name>/scaling-up/inputs directory using the mv command. Move the minimap2.sif container image file to your /ospool/ap40/data/<user.name>/scaling-up/software directory using the mv command. Stop and Consider: Why are we moving our files to these directories? When preparing your jobs for high-throughput computing, think about how each file will be used. Will the file be reused across many jobs? Or is it specific to a single job? Will it need to be transferred repeatedly\u2014or just once? With that in mind: - Why might it make sense to place reads.fastq in your /home directory? - Why are we storing Celegans_ref.mmi and minimap2.sif in /ospool/ap40/data/ instead? Solution Files in /home are transferred to each job, directly from the AP to the EP. These files do not get cached in the OSDF, which makes sense for files that change across jobs\u2014such as subsets of reads.fastq . But Celegans_ref.mmi and minimap2.sif are the same in every job. By storing them in /ospool/ap40/data/ and using osdf:// to access them, we avoid repeatedly transferring them. Instead, they're cached near where jobs run, which speeds things up and reduces network load.","title":"Organize Files"},{"location":"materials/scaling/part1-ex1-organization/#wrangling-the-data","text":"To get ready for our mapping step, we need to prepare our read files. This includes two crucial steps, splitting our reads. Navigate to your ~/scaling-up/inputs/ directory. cd ~/scaling-up/inputs/ Split the FASTQ file into subsets. We can divide our reads.fastq into subsets of ~ 14,000 reads per subset. Since each FASTQ read consist of four lines in the FASTQ file, we can split reads.fastq every 56,000 lines. split -l 56000 reads.fastq reads_fastq_chunk_ rm reads.fastq This will generate 100 read subset files with the prefix eads_fastq_chunk_ . Subsetting Data - Your Milage May Vary When subsetting your data, you should exercise your own judgement. The ideal job profile on the OSPool typically looks something like: Runtime: Between 10mins and <10hrs Memory (RAM): 1-5 GB Inputs: <1 GB Outputs: <1 GB Jobs with larger profiles may still run, but will likely face significant increases in idle state while waiting for a machine to match. Delete (or move) the reads.fastq file from the input directory. In the next exercise, we will generate a list of jobs for HTCondor using the the subset files in this directory. To avoid accidentally including the reads.fastq file in our list of jobs, please delete it or move it out of this directory. Organization in HTC is Critical! One of the most important steps in scaling up our workflows to run on HTC systems, such as the OSPool, is maintaining a clear organizational structure. This includes deleting files we will not be using anymore. For the rest of the exercise, we will not be using the reads.fastq file after splitting it. Not deleting this file can cause downstream issues. Do not skip this step.","title":"Wrangling the Data"},{"location":"materials/scaling/part1-ex1-organization/#review-your-progress","text":"Now that we've set up our directory structure and pre-processed our data, we can focus on preparing our submission scripts and getting our jobs running! Checking your progress Before you move on, take this time to comb through your directory structure and compare it to the structure below. View the current directory and its subdirectories by using the ls command with the recursive ( -R ) flag. Your overall structure should look something like this: \u251c\u2500\u2500 /home/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_a \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_b \u2502 \u2502 \u2502 \u251c\u2500\u2500 reads_fastq_chunk_c \u2502 \u2502 \u251c\u2500\u2500 outputs \u2502 \u2502 \u251c\u2500\u2500 logs \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u251c\u2500\u2500 error \u2502 \u2502 \u2502 \u251c\u2500\u2500 output \u251c\u2500\u2500 /ospool/ap40/data/<user.name>/ \u2502 \u251c\u2500\u2500 scaling-up \u2502 \u2502 \u251c\u2500\u2500 inputs \u2502 \u2502 \u2502 \u251c\u2500\u2500 Celegans_ref.mmi \u2502 \u2502 \u251c\u2500\u2500 software \u2502 \u2502 \u2502 \u251c\u2500\u2500 minimap2.sif","title":"Review Your Progress"},{"location":"materials/scaling/part1-ex2-composing-the-job/","text":"Composing Your Jobs \u00b6 Exercise Goal \u00b6 In our previous exercise, Scaling-Up Exercise 1 Part 1 , we learned about the importance of preparing and organizing a directory structure for large-scale workloads. In this section, we'll learn strategies to compose and test these large-scale workloads in the form of jobs. Introduction \u00b6 High throughput computing allows us to efficiently scale analyses by distributing jobs across many computing resources. In this lesson, we will continue the example from the previous exercise, now learning how to structure and submit a read mapping workflow using the OSPool and minimap2 . This includes adapting your executable script and submit file to dynamically handle many input files in parallel. Halt! Do not proceed if you haven't completed the Scaling-Up Exercise 1 Part 1 This is part two of our Scaling Up Exercise 1 set and should only be completed after you've successfully completed Scaling-Up Exercise 1 Part 1 . Log into an OSPool Access Point \u00b6 Make sure you are logged into ap40.uw.osg-htc.org . Generating the List of Jobs \u00b6 Next, we need to generate a list of jobs for HTCondor to run. In previous exercises, we've used the queue statements such as queue <num> and queue <variable> matching *.txt . For our exercise, we will use the queue <var> from <list> submission strategy. Think Ahead! What values should we pass to HTCondor to scale our minimap2 workflow up? Move to your ~/scaling-up/inputs/ directory $ cd ~/scaling-up/inputs/ $ ls -la total 11626328 drwxr-xr-x 2 username username 4096 Jun 13 16:08 . drwx------ 10 username username 4096 Jun 13 16:07 .. -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_a -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_b -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_c -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_d Make a list of all the files in ~/scaling-up/inputs/ and save it to ~/scaling-up/list_of_fastq.txt $ ls > ~/scaling-up/list_of_fastq.txt $ cd ~/scaling-up/ $ ls -la total 12 drwxr-xr-x 2 username username 4096 Jun 13 16:08 . drwx------ 10 username username 4096 Jun 13 16:07 .. -rw-r--r-- 1 username username 14 Jun 13 16:08 list_of_fastq.txt Use head to preview the first 10 lines of list_of_fastq.txt $ head ~/scaling-up/list_of_fastq.txt reads_fastq_chunk_aa reads_fastq_chunk_ab reads_fastq_chunk_ac reads_fastq_chunk_ad reads_fastq_chunk_ae ... reads_fastq_chunk_aj Adapting the Executable \u00b6 Now that we have our data partitioned into independent subsets to be mapped in parallel, we can work on adapting our executable for use on the OSPool. We will start with the following template executable file, which is also found in your project directory under ~/scaling-up/minimap2.sh . # !/bin/bash # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi reads.fastq > output.sam Command Segment minimap2 -ax map-ont Celegans_ref.mmi reads.fastq > output.sam Meaning The program we'll run to map our reads Specifies the type of reads we're using (Oxford Nanopore Technologies reads) The input reference we're mapping to The reads we are mapping against our genome redirects the output of minimap2 to a file The output file of our mapping step Time-Out! Think about how you would adapt this executable template for HTC If we want to map each one of our reads subsets against the reference genome, think about the following questions: What parts of the command will change with each job? What parts of the command will stay the same? Let's start by editing our template executable file! In our executable there's two main segments of the minimap2 command that will be changed: The input reads.fastq file and the output output.sam file. Thinking Ahead Before Errors! Renaming our output files What do you think would happen if we do keep the output file on our executable as output.sam ? Modify the executable to accept the name of our input reads.fastq subsets as an argument. # !/bin/bash reads_subset_file=\"$1\" # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi \"$reads_subset_file\" > output.sam Modify the executable use the name of our input reads subset file ( $reads_subset_file ) as the prefix of our output file. # !/bin/bash reads_subset_file=\"$1\" # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi \"$reads_subset_file\" > \"${reads_subset_file}_output.sam\" Not sure how variables work on bash? Reach out for help from one of the School staff members! You can also review the Software Carpentries' Unix Shell - Loops tutorial for examples on how to use these variables in your daily computational use. Testing Our Jobs - Submit a Test List of Jobs \u00b6 Now we want to submit a test job with our organizing scheme and adapted executable, using only a small set of our reads subset. We're going to start off with the multi-job submit template below. container_image = <path_to_sif> executable = <path_to_executable> transfer_input_files = <path_to_input_files> transfer_output_files = <path_to_output_files> log = <path_to_log_file> error = <path_to_stderror_file> output = <path_to_stdout_file> request_cpus = <num-of-cpus> request_memory = <amount-of-memory> request_disk = <amount-of-disk> queue Try It Yourself! You've split your large FASTQ file into multiple read subsets, and you're ready to run minimap2 on all of them in parallel. Before moving forward, check your understanding by trying to write the submit file yourself! Consider the following: What queue strategy discussed in the OSG School is best for our setup? Think about the List Of Jobs created in the Generating the List of Jobs section How can we dynamically specify the arguments , transfer_input_files , transfer_output_files fields values with each read_subset_file . Ensure your log , error , and output files all include the name of the read subset file being used mapped in this job. Organize the output files using the correct transfer_output_remaps statement Remember, we want our output to be saved as ~/scaling-up/outputs/reads_fastq_chunk_a_output.sam on the Access Point Which file transfer protocols should we use for our inputs/outputs? Consider whether these files are used one or repeatedly across all your jobs. Try to Draft a Submit File Before Moving Forward\ufe0f For our template, lets use read_subset_file as our variable name to pass the name of each subset file to. It is useful to think about constructing your submit file starting with the queue statement. This helps us predict how we need to structure our submit file in order to dynamically submit many jobs at once. What queue statement do you think would work best for our current workflow? Consider the following: We have a set of FASTQ formatted reads_fastq_chunk_ subset files we need to submit along with each job. Each reads_fastq_chunk_ subset file name is in our ~/scaling-up/list_of_fastq.txt list of jobs. Solution Since we have our list of job files saved in ~/scaling-up/list_of_fastq.txt , we will use the queue <var> from <list_of_var.txt> syntax. Our queue statement will be: queue read_subset_file from ~/scaling-up/list_of_fastq.txt Fill in the incomplete lines of the submit file, as shown below: container_image = osdf:///ospool/ap40/data/<user.name>/scaling-up/software/minimap2.sif executable = minimap2.sh arguments = $(read_subset_file) Now, specify where the input and output files should be: transfer_input_files = inputs/$(read_subset_file), osdf:///ospool/ap40/data/<user.name>/scaling-up/inputs/Celegans_ref.mmi transfer_output_files = $(read_subset_file)_output.sam transfer_output_remaps = \"$(read_subset_file)_output.sam=outputs/$(read_subset_file)_output.sam\" To tell HTCondor the location of the input file, we need to include the input directory. Also, this submit file uses the transfer_output_remaps feature that you learned about; it will move the output file to the output directory by renaming or remapping it. Next, edit the submit file lines that tell the log, output, and error files where to go: output = logs/output/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.out error = logs/error/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.err log = logs/log/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.log Add to the submit file your resource requirements: request_cpus = 2 request_disk = 4 GB request_memory = 4 GB Lastly, finish your submit file with the queue statement: queue read_subset_file from ./test_list_of_fastq.txt We will be using ./test_list_of_fastq.txt instead while will only have a sample (3) of our reads subsets. We will use the fully scale list in the next section. Thinking of our jobs as a for or while loop We can think of our multi-job submission as a sort of for or while loop in bash. For Loop: If you are familiar with the for loop structure, imagine you wished to run the following loop: for read_subset_file in reads_fastq_chunk_a reads_fastq_chunk_b reads_fastq_chunk_c ... reads_fastq_chunk_z do ./minimap2.sh $(read_subset_file) done In the example above, we would feed the list of FASTQ files in ~/scaling-up/inputs/ to the variable $(read_subset_file) as a list of strings. To express your jobs as a for loop in condor, we would instead use the queue <Var> in <List> syntax. In the example above, this would be represented as: queue read_subset_file in (reads_fastq_chunk_a reads_fastq_chunk_b reads_fastq_chunk_c ... reads_fastq_chunk_z) While Loop: A closer representation to HTCondor's list of jobs structure is the while loop. If you are familiar with the while loop in bash, you could also consider the set of job submissions to mirror something like: while read read_subset_file; do ./minimap2.sh $(read_subset_file) done < list_of_fastq.txt Here we feed the contents of list_of_fastq.txt , the list of files in ~/scaling-up/inputs/ to the same $(read_subset_file) variable. The while loop iterates through each line of list_of_fastq.txt , appending the line's value to $(read_subset_file) . To express your jobs as a for loop in condor, we would instead use the queue <Var> in <List> syntax. In the example above, this would be represented as: queue read_subset_file from ./list_of_files.txt For jobs with more than 5 values, we generally recommend using the queue var from list_of_files.txt syntax. Generate ./test_list_of_fastq.txt using head command head -n 3 ./list_of_fastq.txt > ./test_list_of_fastq.txt Submit your job and monitor its progress. Remember to Check Your Submit File A common reason your job may go on hold is forgetting to replace <user.name> with your OSPool username. Check your files and path are setup correctly. For example, your container_image path should look something like: osdf:///ospool/ap40/data/daniel.morales/scaling-up/software/minimap2.sif . Submit your test job using condor_submit condor_submit multi_job_minimap.sub Monitor the progress of your job using condor_watch_q condor_watch_q Always Check Your Test Jobs Worked! Review your condor_watch_q output and your files on the Access Point. Submit Multiple Jobs - Scaling to Full Dataset \u00b6 Now, you are ready to submit the whole workload. Try It Yourself! You've split your large FASTQ file into multiple read subsets, and you're ready to run minimap2 on all of them in parallel. Edit your submit file to use the queue <var> from <file> syntax. Ensure the arguments , transfer_input_files , transfer_output_files fields change with each input. Ensure your log , error , and output files all include the name of the read subset file being used mapped in this job. Organize the output files using the correct transfer_output_remaps statement Before submitting: Are all your subset filenames listed in list_of_fastq.txt ? Did you test at least one job successfully? Are you remapping outputs into the outputs/ folder? Think about what needs to change on your multi_job_minimap.sub submit file to submit the full dataset. When ready, submit with: condor_submit minimap2_multi.submit Solution - \u26a0\ufe0f Try to Solve Before Viewing \u26a0\ufe0f Make sure to change your queue statement to use the full dataset. Your final submit file, minimap2_multi.submit , should look something like this: container_image = osdf:///ospool/ap40/data/<user.name>/scaling-up/software/minimap2.sif executable = ./minimap2.sh arguments = $(read_subset_file) transfer_input_files = ./inputs/$(read_subset_file), osdf:///ospool/ap40/data/<user.name>/scaling-up/inputs/Celegans_ref.mmi transfer_output_files = ./$(read_subset_file)_output.sam transfer_output_remaps = \"$(read_subset_file)_output.sam=outputs/$(read_subset_file)_output.sam\" output = logs/output/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.out error = logs/error/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.err log = logs/log/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.log request_cpus = 2 request_disk = 4 GB request_memory = 4 GB queue read_subset_file from ./list_of_fastq.txt Checking Your Jobs' Progress \u00b6 We can use the command condor_watch_q to track our job submission. As your jobs progress through the various job state , Condor will update the output of condor_watch_q . Pro-Tip: Jobs Holds Aren't Always Bad! Seeing your jobs in the Held state? Don\u2019t panic! This is often just HTCondor doing its job to protect your workflow. Holds can happen for a variety of reasons: missing input files, typos in submit files, or temporary system issues. In many cases, these issues are transient and easily recoverable. To check why your job is held: condor_q -held To release your held job after fixing the issue (e.g., typos or missing files): [Not necessarily true. They'll need condor_qedit or resubmission] condor_release <JobID> If you're unsure whether to release or dig deeper, or if the hold message is cryptic\u2014we\u2019re here to help! Reach out to the OSG Support team at support@osg-htc.org with your job ID(s) and a brief description of what you're trying to do. We\u2019re always happy to assist. \u2705 Remember: held jobs are a signal, not a failure. Use them to improve and scale your workflows with confidence.","title":"1.2 - Composing Your Jobs"},{"location":"materials/scaling/part1-ex2-composing-the-job/#composing-your-jobs","text":"","title":"Composing Your Jobs"},{"location":"materials/scaling/part1-ex2-composing-the-job/#exercise-goal","text":"In our previous exercise, Scaling-Up Exercise 1 Part 1 , we learned about the importance of preparing and organizing a directory structure for large-scale workloads. In this section, we'll learn strategies to compose and test these large-scale workloads in the form of jobs.","title":"Exercise Goal"},{"location":"materials/scaling/part1-ex2-composing-the-job/#introduction","text":"High throughput computing allows us to efficiently scale analyses by distributing jobs across many computing resources. In this lesson, we will continue the example from the previous exercise, now learning how to structure and submit a read mapping workflow using the OSPool and minimap2 . This includes adapting your executable script and submit file to dynamically handle many input files in parallel. Halt! Do not proceed if you haven't completed the Scaling-Up Exercise 1 Part 1 This is part two of our Scaling Up Exercise 1 set and should only be completed after you've successfully completed Scaling-Up Exercise 1 Part 1 .","title":"Introduction"},{"location":"materials/scaling/part1-ex2-composing-the-job/#log-into-an-ospool-access-point","text":"Make sure you are logged into ap40.uw.osg-htc.org .","title":"Log into an OSPool Access Point"},{"location":"materials/scaling/part1-ex2-composing-the-job/#generating-the-list-of-jobs","text":"Next, we need to generate a list of jobs for HTCondor to run. In previous exercises, we've used the queue statements such as queue <num> and queue <variable> matching *.txt . For our exercise, we will use the queue <var> from <list> submission strategy. Think Ahead! What values should we pass to HTCondor to scale our minimap2 workflow up? Move to your ~/scaling-up/inputs/ directory $ cd ~/scaling-up/inputs/ $ ls -la total 11626328 drwxr-xr-x 2 username username 4096 Jun 13 16:08 . drwx------ 10 username username 4096 Jun 13 16:07 .. -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_a -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_b -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_c -rw-r--r-- 1 username username 14 Jun 13 16:08 reads_fastq_chunk_d Make a list of all the files in ~/scaling-up/inputs/ and save it to ~/scaling-up/list_of_fastq.txt $ ls > ~/scaling-up/list_of_fastq.txt $ cd ~/scaling-up/ $ ls -la total 12 drwxr-xr-x 2 username username 4096 Jun 13 16:08 . drwx------ 10 username username 4096 Jun 13 16:07 .. -rw-r--r-- 1 username username 14 Jun 13 16:08 list_of_fastq.txt Use head to preview the first 10 lines of list_of_fastq.txt $ head ~/scaling-up/list_of_fastq.txt reads_fastq_chunk_aa reads_fastq_chunk_ab reads_fastq_chunk_ac reads_fastq_chunk_ad reads_fastq_chunk_ae ... reads_fastq_chunk_aj","title":"Generating the List of Jobs"},{"location":"materials/scaling/part1-ex2-composing-the-job/#adapting-the-executable","text":"Now that we have our data partitioned into independent subsets to be mapped in parallel, we can work on adapting our executable for use on the OSPool. We will start with the following template executable file, which is also found in your project directory under ~/scaling-up/minimap2.sh . # !/bin/bash # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi reads.fastq > output.sam Command Segment minimap2 -ax map-ont Celegans_ref.mmi reads.fastq > output.sam Meaning The program we'll run to map our reads Specifies the type of reads we're using (Oxford Nanopore Technologies reads) The input reference we're mapping to The reads we are mapping against our genome redirects the output of minimap2 to a file The output file of our mapping step Time-Out! Think about how you would adapt this executable template for HTC If we want to map each one of our reads subsets against the reference genome, think about the following questions: What parts of the command will change with each job? What parts of the command will stay the same? Let's start by editing our template executable file! In our executable there's two main segments of the minimap2 command that will be changed: The input reads.fastq file and the output output.sam file. Thinking Ahead Before Errors! Renaming our output files What do you think would happen if we do keep the output file on our executable as output.sam ? Modify the executable to accept the name of our input reads.fastq subsets as an argument. # !/bin/bash reads_subset_file=\"$1\" # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi \"$reads_subset_file\" > output.sam Modify the executable use the name of our input reads subset file ( $reads_subset_file ) as the prefix of our output file. # !/bin/bash reads_subset_file=\"$1\" # Use minimap2 to map the basecalled reads to the reference genome minimap2 -ax map-ont Celegans_ref.mmi \"$reads_subset_file\" > \"${reads_subset_file}_output.sam\" Not sure how variables work on bash? Reach out for help from one of the School staff members! You can also review the Software Carpentries' Unix Shell - Loops tutorial for examples on how to use these variables in your daily computational use.","title":"Adapting the Executable"},{"location":"materials/scaling/part1-ex2-composing-the-job/#testing-our-jobs-submit-a-test-list-of-jobs","text":"Now we want to submit a test job with our organizing scheme and adapted executable, using only a small set of our reads subset. We're going to start off with the multi-job submit template below. container_image = <path_to_sif> executable = <path_to_executable> transfer_input_files = <path_to_input_files> transfer_output_files = <path_to_output_files> log = <path_to_log_file> error = <path_to_stderror_file> output = <path_to_stdout_file> request_cpus = <num-of-cpus> request_memory = <amount-of-memory> request_disk = <amount-of-disk> queue Try It Yourself! You've split your large FASTQ file into multiple read subsets, and you're ready to run minimap2 on all of them in parallel. Before moving forward, check your understanding by trying to write the submit file yourself! Consider the following: What queue strategy discussed in the OSG School is best for our setup? Think about the List Of Jobs created in the Generating the List of Jobs section How can we dynamically specify the arguments , transfer_input_files , transfer_output_files fields values with each read_subset_file . Ensure your log , error , and output files all include the name of the read subset file being used mapped in this job. Organize the output files using the correct transfer_output_remaps statement Remember, we want our output to be saved as ~/scaling-up/outputs/reads_fastq_chunk_a_output.sam on the Access Point Which file transfer protocols should we use for our inputs/outputs? Consider whether these files are used one or repeatedly across all your jobs. Try to Draft a Submit File Before Moving Forward\ufe0f For our template, lets use read_subset_file as our variable name to pass the name of each subset file to. It is useful to think about constructing your submit file starting with the queue statement. This helps us predict how we need to structure our submit file in order to dynamically submit many jobs at once. What queue statement do you think would work best for our current workflow? Consider the following: We have a set of FASTQ formatted reads_fastq_chunk_ subset files we need to submit along with each job. Each reads_fastq_chunk_ subset file name is in our ~/scaling-up/list_of_fastq.txt list of jobs. Solution Since we have our list of job files saved in ~/scaling-up/list_of_fastq.txt , we will use the queue <var> from <list_of_var.txt> syntax. Our queue statement will be: queue read_subset_file from ~/scaling-up/list_of_fastq.txt Fill in the incomplete lines of the submit file, as shown below: container_image = osdf:///ospool/ap40/data/<user.name>/scaling-up/software/minimap2.sif executable = minimap2.sh arguments = $(read_subset_file) Now, specify where the input and output files should be: transfer_input_files = inputs/$(read_subset_file), osdf:///ospool/ap40/data/<user.name>/scaling-up/inputs/Celegans_ref.mmi transfer_output_files = $(read_subset_file)_output.sam transfer_output_remaps = \"$(read_subset_file)_output.sam=outputs/$(read_subset_file)_output.sam\" To tell HTCondor the location of the input file, we need to include the input directory. Also, this submit file uses the transfer_output_remaps feature that you learned about; it will move the output file to the output directory by renaming or remapping it. Next, edit the submit file lines that tell the log, output, and error files where to go: output = logs/output/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.out error = logs/error/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.err log = logs/log/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.log Add to the submit file your resource requirements: request_cpus = 2 request_disk = 4 GB request_memory = 4 GB Lastly, finish your submit file with the queue statement: queue read_subset_file from ./test_list_of_fastq.txt We will be using ./test_list_of_fastq.txt instead while will only have a sample (3) of our reads subsets. We will use the fully scale list in the next section. Thinking of our jobs as a for or while loop We can think of our multi-job submission as a sort of for or while loop in bash. For Loop: If you are familiar with the for loop structure, imagine you wished to run the following loop: for read_subset_file in reads_fastq_chunk_a reads_fastq_chunk_b reads_fastq_chunk_c ... reads_fastq_chunk_z do ./minimap2.sh $(read_subset_file) done In the example above, we would feed the list of FASTQ files in ~/scaling-up/inputs/ to the variable $(read_subset_file) as a list of strings. To express your jobs as a for loop in condor, we would instead use the queue <Var> in <List> syntax. In the example above, this would be represented as: queue read_subset_file in (reads_fastq_chunk_a reads_fastq_chunk_b reads_fastq_chunk_c ... reads_fastq_chunk_z) While Loop: A closer representation to HTCondor's list of jobs structure is the while loop. If you are familiar with the while loop in bash, you could also consider the set of job submissions to mirror something like: while read read_subset_file; do ./minimap2.sh $(read_subset_file) done < list_of_fastq.txt Here we feed the contents of list_of_fastq.txt , the list of files in ~/scaling-up/inputs/ to the same $(read_subset_file) variable. The while loop iterates through each line of list_of_fastq.txt , appending the line's value to $(read_subset_file) . To express your jobs as a for loop in condor, we would instead use the queue <Var> in <List> syntax. In the example above, this would be represented as: queue read_subset_file from ./list_of_files.txt For jobs with more than 5 values, we generally recommend using the queue var from list_of_files.txt syntax. Generate ./test_list_of_fastq.txt using head command head -n 3 ./list_of_fastq.txt > ./test_list_of_fastq.txt Submit your job and monitor its progress. Remember to Check Your Submit File A common reason your job may go on hold is forgetting to replace <user.name> with your OSPool username. Check your files and path are setup correctly. For example, your container_image path should look something like: osdf:///ospool/ap40/data/daniel.morales/scaling-up/software/minimap2.sif . Submit your test job using condor_submit condor_submit multi_job_minimap.sub Monitor the progress of your job using condor_watch_q condor_watch_q Always Check Your Test Jobs Worked! Review your condor_watch_q output and your files on the Access Point.","title":"Testing Our Jobs - Submit a Test List of Jobs"},{"location":"materials/scaling/part1-ex2-composing-the-job/#submit-multiple-jobs-scaling-to-full-dataset","text":"Now, you are ready to submit the whole workload. Try It Yourself! You've split your large FASTQ file into multiple read subsets, and you're ready to run minimap2 on all of them in parallel. Edit your submit file to use the queue <var> from <file> syntax. Ensure the arguments , transfer_input_files , transfer_output_files fields change with each input. Ensure your log , error , and output files all include the name of the read subset file being used mapped in this job. Organize the output files using the correct transfer_output_remaps statement Before submitting: Are all your subset filenames listed in list_of_fastq.txt ? Did you test at least one job successfully? Are you remapping outputs into the outputs/ folder? Think about what needs to change on your multi_job_minimap.sub submit file to submit the full dataset. When ready, submit with: condor_submit minimap2_multi.submit Solution - \u26a0\ufe0f Try to Solve Before Viewing \u26a0\ufe0f Make sure to change your queue statement to use the full dataset. Your final submit file, minimap2_multi.submit , should look something like this: container_image = osdf:///ospool/ap40/data/<user.name>/scaling-up/software/minimap2.sif executable = ./minimap2.sh arguments = $(read_subset_file) transfer_input_files = ./inputs/$(read_subset_file), osdf:///ospool/ap40/data/<user.name>/scaling-up/inputs/Celegans_ref.mmi transfer_output_files = ./$(read_subset_file)_output.sam transfer_output_remaps = \"$(read_subset_file)_output.sam=outputs/$(read_subset_file)_output.sam\" output = logs/output/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.out error = logs/error/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.err log = logs/log/job.$(ClusterID).$(ProcID)_$(read_subset_file)_output.log request_cpus = 2 request_disk = 4 GB request_memory = 4 GB queue read_subset_file from ./list_of_fastq.txt","title":"Submit Multiple Jobs - Scaling to Full Dataset"},{"location":"materials/scaling/part1-ex2-composing-the-job/#checking-your-jobs-progress","text":"We can use the command condor_watch_q to track our job submission. As your jobs progress through the various job state , Condor will update the output of condor_watch_q . Pro-Tip: Jobs Holds Aren't Always Bad! Seeing your jobs in the Held state? Don\u2019t panic! This is often just HTCondor doing its job to protect your workflow. Holds can happen for a variety of reasons: missing input files, typos in submit files, or temporary system issues. In many cases, these issues are transient and easily recoverable. To check why your job is held: condor_q -held To release your held job after fixing the issue (e.g., typos or missing files): [Not necessarily true. They'll need condor_qedit or resubmission] condor_release <JobID> If you're unsure whether to release or dig deeper, or if the hold message is cryptic\u2014we\u2019re here to help! Reach out to the OSG Support team at support@osg-htc.org with your job ID(s) and a brief description of what you're trying to do. We\u2019re always happy to assist. \u2705 Remember: held jobs are a signal, not a failure. Use them to improve and scale your workflows with confidence.","title":"Checking Your Jobs' Progress"},{"location":"materials/scaling/part2-ex1-chtc-node/","text":"Scaling Up Exercise 2.1: Log Into a Local Pool \u00b6 Introduction \u00b6 So far, all of our HTC exercises (and your workloads) have used the Open Science Pool, accessed through the ap40 Access Point. But what if you had access to another computing resource also? In this section of exercises, we will explore similarities and differences between the OSPool, and a campus-focused high throughput computing system operated by the Center for High Throughput Computing. Log In \u00b6 The CHTC Access Point we will be using is called ap2003.chtc.wisc.edu . You should be able to log into this computer with the same username and login steps as for ap40.uw.osg-htc.org . Initial Exploration \u00b6 In the following exercises, we will explore the differences between the OSPool and the CHTC pool more deeply. But to start, let's run a few commands to compare. What's in the pool? \u00b6 Log into ap2003.chtc.wisc.edu . Run these commands: console [username@ap2003 ~]$ condor_status | tail [username@ap2003 ~]$ condor_status -compact | tail -n 15 How many \"slots\" are in the pool? How many of each operating system? In a separate window, log into ap40.uw.osg-htc.org and run the same commands. How many \"slots\" are in the OSPool? How many of each operating system? Jobs in the queue \u00b6 On ap2003.chtc.wisc.edu run condor_q -all . On ap40.uw.osg-htc.org , run the same command. How many jobs are on ap40 compared to ap2003 ?","title":"2.1 - Log Into a Local Pool"},{"location":"materials/scaling/part2-ex1-chtc-node/#scaling-up-exercise-21-log-into-a-local-pool","text":"","title":"Scaling Up Exercise 2.1: Log Into a Local Pool"},{"location":"materials/scaling/part2-ex1-chtc-node/#introduction","text":"So far, all of our HTC exercises (and your workloads) have used the Open Science Pool, accessed through the ap40 Access Point. But what if you had access to another computing resource also? In this section of exercises, we will explore similarities and differences between the OSPool, and a campus-focused high throughput computing system operated by the Center for High Throughput Computing.","title":"Introduction"},{"location":"materials/scaling/part2-ex1-chtc-node/#log-in","text":"The CHTC Access Point we will be using is called ap2003.chtc.wisc.edu . You should be able to log into this computer with the same username and login steps as for ap40.uw.osg-htc.org .","title":"Log In"},{"location":"materials/scaling/part2-ex1-chtc-node/#initial-exploration","text":"In the following exercises, we will explore the differences between the OSPool and the CHTC pool more deeply. But to start, let's run a few commands to compare.","title":"Initial Exploration"},{"location":"materials/scaling/part2-ex1-chtc-node/#whats-in-the-pool","text":"Log into ap2003.chtc.wisc.edu . Run these commands: console [username@ap2003 ~]$ condor_status | tail [username@ap2003 ~]$ condor_status -compact | tail -n 15 How many \"slots\" are in the pool? How many of each operating system? In a separate window, log into ap40.uw.osg-htc.org and run the same commands. How many \"slots\" are in the OSPool? How many of each operating system?","title":"What's in the pool?"},{"location":"materials/scaling/part2-ex1-chtc-node/#jobs-in-the-queue","text":"On ap2003.chtc.wisc.edu run condor_q -all . On ap40.uw.osg-htc.org , run the same command. How many jobs are on ap40 compared to ap2003 ?","title":"Jobs in the queue"},{"location":"materials/scaling/part2-ex2-hardware-diffs/","text":"Scaling Up Exercise 2.2: Hardware Differences Between OSPool and the CHTC Pool \u00b6 The goal of this exercise is to compare hardware differences between the Open Science Pool and a local HTC system (the CHTC pool), using similar tools as the Tuesday exercises. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit batches of jobs that differ only in how much memory each one requests. We will request memory from 2 - 128 GB, doubling the memory each time. One set of jobs will be submitted to CHTC, and the other, identical set of jobs will be submitted to the OSPool. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run. Checking CHTC memory availability \u00b6 In this first part, you will create the submit file that will be used for both sets of jobs. Tip Make sure you are logged into ap2003.chtc.wisc.edu for the first part of this exercise. Create the submit file \u00b6 To create our parameter sweep, we will create a new submit file with the queue...in syntax and change the value of our parameter ( request_memory ) for each batch of jobs. Create a subdirectory for this exercise (we've used mem-requests below). In this folder, create this template submit file: executable = /usr/bin/sleep arguments = 30 transfer_executable = false log = memory_sweep.$(Cluster).log request_cpus = 1 request_disk = 1024 queue 5 request_memory in ( 1GB 2GB 3GB ) The queue statement, is iterating through the list shown between the parentheses (1, 2, 3) using those values as the memory request. Five jobs are submitted for each memory value. Edit the list to reflect different amounts of memory. We recommend this list in gigabytes (GB), although you can use whatever values you want: 2, 4, 8, 16, 32, 64, 128, 256, 512 Submit the submit file (should be about 45 jobs). Monitoring the local jobs \u00b6 Every few minutes, run condor_q and see how your sleep jobs are doing. To display the number of jobs remaining for each request_memory parameter specified, run the following command: $ condor_q <Cluster ID> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested, in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 8 GB 10 6 16 GB 10 7 32 GB 10 8 64 GB 10 9 In the meantime, between checking on your local jobs, start the next section \u2013 but take a break every few minutes to switch back to CHTCSUBMITNODE and record progress on your jobs. Checking OSPool memory availability \u00b6 Now you will do essentially the same thing on the OSPool. Copy the mem-requests directory from the section above from ap2003.chtc.wisc.edu to ap40.uw.osg-htc.org . Not sure how to copy the files? There are different ways to copy files from server to server. For this task, one option is: [user.name@ap2003 ~]$ scp -r path/to/mem-requests user.name@ap40.uw.osg-htc.org:/home/user.name Edit the username and the paths to reflect your home directory organization. Log in or switch to ap40.uw.osg-htc.org Submit the jobs to the OSPool Monitoring the OSPool jobs \u00b6 As you did in the first part, use condor_q to track how your sleep jobs are doing. It is fine to move on to the next exercise, but keep tracking the status of both sets of these jobs. After you are done with the next exercise , come back to this exercise and analyze the results. Analyzing the results \u00b6 Have all of your jobs from this exercise completed on both CHTC and the OSPool? How many jobs have completed thus far on CHTC? How many have completed thus far on the OSPool? Due to the dynamic nature of the OSPool, the demand for higher memory jobs there may have resulted in a temporary increase in high-memory slots there. That being said, high-memory are a high-demand, low-availability resource in the OSPool so your 64 GB jobs (or higher!) may have taken longer to run or complete. On the other hand, CHTC has a fair number of 64 GB (and greater) slots so all your jobs have a high chance of running. This is useful information if your job profile includes high memory jobs. It could help you decide which jobs to run on CHTC versus the OSPool. If you have access to different computing systems, it is a good idea to find out this information - even run some tests, lik we did here! - in order to get the most out of what is available to you.","title":"2.2 - Hardware Differences Between OSPool and the CHTC Pool"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#scaling-up-exercise-22-hardware-differences-between-ospool-and-the-chtc-pool","text":"The goal of this exercise is to compare hardware differences between the Open Science Pool and a local HTC system (the CHTC pool), using similar tools as the Tuesday exercises. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit batches of jobs that differ only in how much memory each one requests. We will request memory from 2 - 128 GB, doubling the memory each time. One set of jobs will be submitted to CHTC, and the other, identical set of jobs will be submitted to the OSPool. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run.","title":"Scaling Up Exercise 2.2: Hardware Differences Between OSPool and the CHTC Pool"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#checking-chtc-memory-availability","text":"In this first part, you will create the submit file that will be used for both sets of jobs. Tip Make sure you are logged into ap2003.chtc.wisc.edu for the first part of this exercise.","title":"Checking CHTC memory availability"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#create-the-submit-file","text":"To create our parameter sweep, we will create a new submit file with the queue...in syntax and change the value of our parameter ( request_memory ) for each batch of jobs. Create a subdirectory for this exercise (we've used mem-requests below). In this folder, create this template submit file: executable = /usr/bin/sleep arguments = 30 transfer_executable = false log = memory_sweep.$(Cluster).log request_cpus = 1 request_disk = 1024 queue 5 request_memory in ( 1GB 2GB 3GB ) The queue statement, is iterating through the list shown between the parentheses (1, 2, 3) using those values as the memory request. Five jobs are submitted for each memory value. Edit the list to reflect different amounts of memory. We recommend this list in gigabytes (GB), although you can use whatever values you want: 2, 4, 8, 16, 32, 64, 128, 256, 512 Submit the submit file (should be about 45 jobs).","title":"Create the submit file"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#monitoring-the-local-jobs","text":"Every few minutes, run condor_q and see how your sleep jobs are doing. To display the number of jobs remaining for each request_memory parameter specified, run the following command: $ condor_q <Cluster ID> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested, in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 8 GB 10 6 16 GB 10 7 32 GB 10 8 64 GB 10 9 In the meantime, between checking on your local jobs, start the next section \u2013 but take a break every few minutes to switch back to CHTCSUBMITNODE and record progress on your jobs.","title":"Monitoring the local jobs"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#checking-ospool-memory-availability","text":"Now you will do essentially the same thing on the OSPool. Copy the mem-requests directory from the section above from ap2003.chtc.wisc.edu to ap40.uw.osg-htc.org . Not sure how to copy the files? There are different ways to copy files from server to server. For this task, one option is: [user.name@ap2003 ~]$ scp -r path/to/mem-requests user.name@ap40.uw.osg-htc.org:/home/user.name Edit the username and the paths to reflect your home directory organization. Log in or switch to ap40.uw.osg-htc.org Submit the jobs to the OSPool","title":"Checking OSPool memory availability"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#monitoring-the-ospool-jobs","text":"As you did in the first part, use condor_q to track how your sleep jobs are doing. It is fine to move on to the next exercise, but keep tracking the status of both sets of these jobs. After you are done with the next exercise , come back to this exercise and analyze the results.","title":"Monitoring the OSPool jobs"},{"location":"materials/scaling/part2-ex2-hardware-diffs/#analyzing-the-results","text":"Have all of your jobs from this exercise completed on both CHTC and the OSPool? How many jobs have completed thus far on CHTC? How many have completed thus far on the OSPool? Due to the dynamic nature of the OSPool, the demand for higher memory jobs there may have resulted in a temporary increase in high-memory slots there. That being said, high-memory are a high-demand, low-availability resource in the OSPool so your 64 GB jobs (or higher!) may have taken longer to run or complete. On the other hand, CHTC has a fair number of 64 GB (and greater) slots so all your jobs have a high chance of running. This is useful information if your job profile includes high memory jobs. It could help you decide which jobs to run on CHTC versus the OSPool. If you have access to different computing systems, it is a good idea to find out this information - even run some tests, lik we did here! - in order to get the most out of what is available to you.","title":"Analyzing the results"},{"location":"materials/scaling/part2-ex3-software-diffs/","text":"Scaling Up Exercise 2.3: Software Differences Between OSPool and the CHTC Pool \u00b6 The goal of this exercise is to see some differences in the default Linux versions and software available in the OSPool and CHTC Pool. We will do this by submitting a large batch of jobs that runs a simple shell script, testing for software versions. Create and test a software probe \u00b6 The following shell script probes for software and returns the version if it is installed: #!/bin/sh host_info (){ hostname source /etc/os-release echo $PRETTY_NAME } get_version (){ program = $1 $program --version > /dev/null 2 > & 1 double_dash_rc = $? $program -version > /dev/null 2 > & 1 single_dash_rc = $? which $program > /dev/null 2 > & 1 which_rc = $? if [ $double_dash_rc -eq 0 ] ; then $program --version 2 > & 1 elif [ $single_dash_rc -eq 0 ] ; then $program -version 2 > & 1 elif [ $which_rc -eq 0 ] ; then echo \" $program installed but could not find version information\" else echo \" $program not installed\" fi } host_info get_version 'R' get_version 'cmake' get_version 'python' get_version 'python3' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version 'nslookup' Log into ap2003.chtc.wisc.edu . Create a folder for this exercise Save the above script as a file named sw_probe.sh Make sure the script can be run: chmod a+x sw_probe.sh Try running the script in place to make sure it works: ./sw_probe.sh Surveying the CHTC Pool \u00b6 Now we will run our shell script on the CHTC pool to survey what resources are available. Create a submit file that runs sw_probe.sh 100 times and uses submit file variables to write different output , and error files. You might want to separate these into their own folder. Submit your job and wait for the results Summarizing Results Here's a little bit of shell magic to summarize the results. If you want to compare the output from line 3 across all the jobs ( R , if you didn't edit the above script): [user.name@ap2003]$ for f in *.out; do cat $f | head -n 4 | tail -n 1; done | sort | uniq -c Surveying the OSPool \u00b6 Copy or recreate the job setup above on ap40.uw.osg-htc.org . Submit the jobs. Look at the results. Similar or different? Reflection \u00b6 Were there any surprises about what was available? Knowing what software is likely (or not) to be available on a given system can inform how you install or manage your software when submitting jobs. CHTC does not have a lot of software available by default, so software portability is important. Because the OSPool varies so much, containers are a good idea.","title":"2.3 - Software Differences Between OSPool and the CHTC Pool"},{"location":"materials/scaling/part2-ex3-software-diffs/#scaling-up-exercise-23-software-differences-between-ospool-and-the-chtc-pool","text":"The goal of this exercise is to see some differences in the default Linux versions and software available in the OSPool and CHTC Pool. We will do this by submitting a large batch of jobs that runs a simple shell script, testing for software versions.","title":"Scaling Up Exercise 2.3: Software Differences Between OSPool and the CHTC Pool"},{"location":"materials/scaling/part2-ex3-software-diffs/#create-and-test-a-software-probe","text":"The following shell script probes for software and returns the version if it is installed: #!/bin/sh host_info (){ hostname source /etc/os-release echo $PRETTY_NAME } get_version (){ program = $1 $program --version > /dev/null 2 > & 1 double_dash_rc = $? $program -version > /dev/null 2 > & 1 single_dash_rc = $? which $program > /dev/null 2 > & 1 which_rc = $? if [ $double_dash_rc -eq 0 ] ; then $program --version 2 > & 1 elif [ $single_dash_rc -eq 0 ] ; then $program -version 2 > & 1 elif [ $which_rc -eq 0 ] ; then echo \" $program installed but could not find version information\" else echo \" $program not installed\" fi } host_info get_version 'R' get_version 'cmake' get_version 'python' get_version 'python3' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version 'nslookup' Log into ap2003.chtc.wisc.edu . Create a folder for this exercise Save the above script as a file named sw_probe.sh Make sure the script can be run: chmod a+x sw_probe.sh Try running the script in place to make sure it works: ./sw_probe.sh","title":"Create and test a software probe"},{"location":"materials/scaling/part2-ex3-software-diffs/#surveying-the-chtc-pool","text":"Now we will run our shell script on the CHTC pool to survey what resources are available. Create a submit file that runs sw_probe.sh 100 times and uses submit file variables to write different output , and error files. You might want to separate these into their own folder. Submit your job and wait for the results Summarizing Results Here's a little bit of shell magic to summarize the results. If you want to compare the output from line 3 across all the jobs ( R , if you didn't edit the above script): [user.name@ap2003]$ for f in *.out; do cat $f | head -n 4 | tail -n 1; done | sort | uniq -c","title":"Surveying the CHTC Pool"},{"location":"materials/scaling/part2-ex3-software-diffs/#surveying-the-ospool","text":"Copy or recreate the job setup above on ap40.uw.osg-htc.org . Submit the jobs. Look at the results. Similar or different?","title":"Surveying the OSPool"},{"location":"materials/scaling/part2-ex3-software-diffs/#reflection","text":"Were there any surprises about what was available? Knowing what software is likely (or not) to be available on a given system can inform how you install or manage your software when submitting jobs. CHTC does not have a lot of software available by default, so software portability is important. Because the OSPool varies so much, containers are a good idea.","title":"Reflection"},{"location":"materials/scaling/part3-ex1-job-attributes/","text":"Exercise 3.1: Investigating Job Attributes \u00b6 The objective of this exercise is to your awareness of job \"class ad attributes\", especially ones that may help you look for issues with your jobs in the OSPool. Recall that a job class ad contains attributes and their values that describe what HTCondor knows about the job. OSPool jobs contain extra attributes that are specific to that pool. Thus, an OSPool job class ad may have well over 150 attributes. Some OSPool job attributes are especially helpful when you are scaling up jobs and want to see if jobs are running as expected or are maybe doing surprising things that are worth extra attention. Preparing exercise files \u00b6 Because this exercise focuses on OSPool job attributes, please use your OSPool account on ap40.uw.osg-htc.org . Create a shell script for testing called simple.sh : #!/bin/bash SLEEPTIME=$1 hostname pwd whoami for i in {1..5} do echo \"performing iteration $i\" sleep $SLEEPTIME done Create an HTCondor submit file that queues three jobs: universe = vanilla log = logs/$(Cluster)_$(Process).log error = logs/$(Cluster)_$(Process).err output = $(Cluster)_$(Process).out executable = simple.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT request_cpus = 1 request_memory = 1GB request_disk = 1GB # set arguments, queue a normal job arguments = 600 queue 1 # queue a job that will go on hold transfer_input_files = test.txt queue 1 # queue a job that will never start request_memory = 40TB queue 1 Exploring OSPool job class ad attributes \u00b6 For this exercise, you will submit the three jobs defined in the submit file above, then examine their job class ad attributes. Here are some attributes that may be interesting: CpusProvisioned is the number of CPUs given to your job for the current or most recent run ResidentSetSize_RAW is the maximum amount of memory that HTCondor has noticed your job using (in KB) DiskUsage_RAW is the maximum amount of disk that HTCondor has noticed your job using (in MB) NumJobStarts is the number of times HTCondor has started your job; 1 is typical for a running job, and higher counts may indicate issues running the job LastRemoteHost identifies the name for the slot where your job is running or most recently ran MachineAttrGLIDEIN_ResourceName*N* is a set of numbered attributes that identify the most recent sites where your job ran; N is 0 for the most recent (or current) run, 1 for the previous run, and so on up to 9 ExitCode exists only if your job exited (completed) at least once; a value of 0 typically means success HoldReasonCode exists only if your job went on hold; if so, it is a number corresponding to the main hold reason (see here for details) NumHoldsByReason is a list of all of the main reasons your job has gone on hold so far with counts of each hold type Let\u2019s explore these attributes on real jobs. Submit the jobs (above) and note the cluster ID When one job from the cluster is running, view all of its job class ad attributes: $ condor_q -l <JobId> where <JobId> is your job's ID, and -l stands for -long This command lists all of the job\u2019s class ad attributes. Details of some of the attributes are in the HTcondor Manual . Others are defined (and not well documented) only for the OSPool. Can you find any of the attributes listed above? Next, use condor_q -af <AttributeName> to examine one attribute at a time for several jobs: $ condor_q -af NumJobStarts <ClusterID> where <ClusterID> is the HTCondor cluster ID noted above, and -af stands for -autoformat . What does the output tell you? Finally, display several attributes at once for the jobs: $ condor_q -af:j NumJobStarts DiskUsage_RAW LastRemoteHost HoldReasonCode <ClusterID> Why do some values appear as undefined ?","title":"Exercise 3.1: Investigating Job Attributes"},{"location":"materials/scaling/part3-ex1-job-attributes/#exercise-31-investigating-job-attributes","text":"The objective of this exercise is to your awareness of job \"class ad attributes\", especially ones that may help you look for issues with your jobs in the OSPool. Recall that a job class ad contains attributes and their values that describe what HTCondor knows about the job. OSPool jobs contain extra attributes that are specific to that pool. Thus, an OSPool job class ad may have well over 150 attributes. Some OSPool job attributes are especially helpful when you are scaling up jobs and want to see if jobs are running as expected or are maybe doing surprising things that are worth extra attention.","title":"Exercise 3.1: Investigating Job Attributes"},{"location":"materials/scaling/part3-ex1-job-attributes/#preparing-exercise-files","text":"Because this exercise focuses on OSPool job attributes, please use your OSPool account on ap40.uw.osg-htc.org . Create a shell script for testing called simple.sh : #!/bin/bash SLEEPTIME=$1 hostname pwd whoami for i in {1..5} do echo \"performing iteration $i\" sleep $SLEEPTIME done Create an HTCondor submit file that queues three jobs: universe = vanilla log = logs/$(Cluster)_$(Process).log error = logs/$(Cluster)_$(Process).err output = $(Cluster)_$(Process).out executable = simple.sh should_transfer_files = YES when_to_transfer_output = ON_EXIT request_cpus = 1 request_memory = 1GB request_disk = 1GB # set arguments, queue a normal job arguments = 600 queue 1 # queue a job that will go on hold transfer_input_files = test.txt queue 1 # queue a job that will never start request_memory = 40TB queue 1","title":"Preparing exercise files"},{"location":"materials/scaling/part3-ex1-job-attributes/#exploring-ospool-job-class-ad-attributes","text":"For this exercise, you will submit the three jobs defined in the submit file above, then examine their job class ad attributes. Here are some attributes that may be interesting: CpusProvisioned is the number of CPUs given to your job for the current or most recent run ResidentSetSize_RAW is the maximum amount of memory that HTCondor has noticed your job using (in KB) DiskUsage_RAW is the maximum amount of disk that HTCondor has noticed your job using (in MB) NumJobStarts is the number of times HTCondor has started your job; 1 is typical for a running job, and higher counts may indicate issues running the job LastRemoteHost identifies the name for the slot where your job is running or most recently ran MachineAttrGLIDEIN_ResourceName*N* is a set of numbered attributes that identify the most recent sites where your job ran; N is 0 for the most recent (or current) run, 1 for the previous run, and so on up to 9 ExitCode exists only if your job exited (completed) at least once; a value of 0 typically means success HoldReasonCode exists only if your job went on hold; if so, it is a number corresponding to the main hold reason (see here for details) NumHoldsByReason is a list of all of the main reasons your job has gone on hold so far with counts of each hold type Let\u2019s explore these attributes on real jobs. Submit the jobs (above) and note the cluster ID When one job from the cluster is running, view all of its job class ad attributes: $ condor_q -l <JobId> where <JobId> is your job's ID, and -l stands for -long This command lists all of the job\u2019s class ad attributes. Details of some of the attributes are in the HTcondor Manual . Others are defined (and not well documented) only for the OSPool. Can you find any of the attributes listed above? Next, use condor_q -af <AttributeName> to examine one attribute at a time for several jobs: $ condor_q -af NumJobStarts <ClusterID> where <ClusterID> is the HTCondor cluster ID noted above, and -af stands for -autoformat . What does the output tell you? Finally, display several attributes at once for the jobs: $ condor_q -af:j NumJobStarts DiskUsage_RAW LastRemoteHost HoldReasonCode <ClusterID> Why do some values appear as undefined ?","title":"Exploring OSPool job class ad attributes"},{"location":"materials/scaling/part3-ex2-log-files/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Getting Job Information from Log Files \u00b6 HTCondor job log files contain useful information about submitted, running, and/or completed jobs, but the format of that information may not always be useful to you . Here, we have a few examples of how to use some powerful Unix commands ( grep , sort , uniq ) to pull information out of these job log files. It is now time for you to try these on your own jobs! Before starting this exercise, copy a couple of your job log files from previous exercises (for example, HTC Exercise 1.5 and/or OSG Exercise 1.1) in to a new directory for this exercise. Use these log files in place of my-job.log in the examples below. The grep command displays lines from a file matching a given pattern, where the pattern is the first argument provided to grep . For example grep 'alice' address_book.txt would print out all lines containing the characters alice in the file named address_book.txt . While working through this exercise, consider keeping one of your job log files open in a separate window to see if you can figure out how we came up with the patterns presented in this exercise. Job terminations \u00b6 Lines for job termination events in the job log always start with 005 and contain the timestamp of when the job(s) ended. Use the following grep command to get a list of when jobs ended in your log files: $ grep '^005' my-job.log Optional challenge : What is the importance of ^ in the pattern ( ^005 ) provided above? Recall that executables typically exit with code 0 when they exit normally, which often (but not always!) means that they exited successfully. Lines containing jobs' exit codes (i.e. return values) all contain the word termination . Use grep to get a list of jobs' exit codes: $ grep termination my-job.log By \"piping\" the output of the previous command through the sort and then uniq commands, we can get a count of each exit code: $ grep termination my-job.log | sort | uniq -c Here's an example of the output from the previous commands when run on a log file written to from eight jobs. Six jobs exited with exit code 0 , while two exited 1 : [username@ap40]$ grep '^005' my-job.log 005 (236881.000.000) 2022-07-27 15:07:38 Job terminated. 005 (236883.000.000) 2022-07-27 15:07:42 Job terminated. 005 (236882.000.000) 2022-07-27 15:08:01 Job terminated. 005 (236880.000.000) 2022-07-27 15:08:07 Job terminated. 005 (236891.000.000) 2022-07-27 15:13:31 Job terminated. 005 (236893.000.000) 2022-07-27 15:13:32 Job terminated. 005 (236892.000.000) 2022-07-27 15:13:58 Job terminated. 005 (236890.000.000) 2022-07-27 15:13:59 Job terminated. [username@ap40]$ grep 'termination' my-job.log (1) Normal termination (return value 0) (1) Normal termination (return value 1) (1) Normal termination (return value 0) (1) Normal termination (return value 0) (1) Normal termination (return value 1) (1) Normal termination (return value 0) (1) Normal termination (return value 0) (1) Normal termination (return value 0) [username@ap40]$ grep 'termination' my-job.log | sort | uniq -c 6 (1) Normal termination (return value 0) 2 (1) Normal termination (return value 1) Job resource usage \u00b6 Jobs' resource usages (and requests and allocations) are logged in the following format: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 10382 1048576 1468671 Memory (MB) : 692 1024 1024 Run the following grep command to pull out the memory information from your job logs: $ grep 'Memory (MB) *:' my-job.log Look back at the format in the example above. Columns after the : will first show memory usage, then memory requested, and then the memory allocated to your job. Similarly, use the following command to get the disk information from your job logs: $ grep 'Disk (KB) *:' my-job.log Here's some example output from running the memory grep command on the same eight-job log file: [username@ap40]$ grep 'Memory (MB) *:' my-job.log Memory (MB) : 692 1024 1024 Memory (MB) : 714 1024 1024 Memory (MB) : 703 1024 1024 Memory (MB) : 699 1024 1024 Memory (MB) : 705 1024 1024 Memory (MB) : 704 1024 1024 Memory (MB) : 711 1024 1024 Memory (MB) : 697 1024 1024 In this example, the memory usage for the jobs ranged from 692 to 714 MB, and they all requested (and were allocated) 1 GB of memory. Other job information \u00b6 See if you can come up with grep commands to gather the number of bytes sent and received by jobs (i.e. how much data was transferred to/from the access point). Here is some example output for comparison: [username@ap40]$ grep '<PATTERN REMOVED>' my-job.log 760393 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760397 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760393 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760397 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job [username@ap40]$ grep '<PATTERN REMOVED>' my-job.log 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job Job log files may also contain additional information about held jobs or interrupted jobs. If you feel that your jobs are bouncing from idle to running and back to idle, or that they are otherwise not making as much progress as you expect, the log files are a good place to check. Though they might eventually become impossibly large to read line-by-line once you start scaling up, using grep to pull out specific lines and using sort and uniq to reduce the output can help you make sense of the information contained in the logs.","title":"Part3 ex2 log files"},{"location":"materials/scaling/part3-ex2-log-files/#getting-job-information-from-log-files","text":"HTCondor job log files contain useful information about submitted, running, and/or completed jobs, but the format of that information may not always be useful to you . Here, we have a few examples of how to use some powerful Unix commands ( grep , sort , uniq ) to pull information out of these job log files. It is now time for you to try these on your own jobs! Before starting this exercise, copy a couple of your job log files from previous exercises (for example, HTC Exercise 1.5 and/or OSG Exercise 1.1) in to a new directory for this exercise. Use these log files in place of my-job.log in the examples below. The grep command displays lines from a file matching a given pattern, where the pattern is the first argument provided to grep . For example grep 'alice' address_book.txt would print out all lines containing the characters alice in the file named address_book.txt . While working through this exercise, consider keeping one of your job log files open in a separate window to see if you can figure out how we came up with the patterns presented in this exercise.","title":"Getting Job Information from Log Files"},{"location":"materials/scaling/part3-ex2-log-files/#job-terminations","text":"Lines for job termination events in the job log always start with 005 and contain the timestamp of when the job(s) ended. Use the following grep command to get a list of when jobs ended in your log files: $ grep '^005' my-job.log Optional challenge : What is the importance of ^ in the pattern ( ^005 ) provided above? Recall that executables typically exit with code 0 when they exit normally, which often (but not always!) means that they exited successfully. Lines containing jobs' exit codes (i.e. return values) all contain the word termination . Use grep to get a list of jobs' exit codes: $ grep termination my-job.log By \"piping\" the output of the previous command through the sort and then uniq commands, we can get a count of each exit code: $ grep termination my-job.log | sort | uniq -c Here's an example of the output from the previous commands when run on a log file written to from eight jobs. Six jobs exited with exit code 0 , while two exited 1 : [username@ap40]$ grep '^005' my-job.log 005 (236881.000.000) 2022-07-27 15:07:38 Job terminated. 005 (236883.000.000) 2022-07-27 15:07:42 Job terminated. 005 (236882.000.000) 2022-07-27 15:08:01 Job terminated. 005 (236880.000.000) 2022-07-27 15:08:07 Job terminated. 005 (236891.000.000) 2022-07-27 15:13:31 Job terminated. 005 (236893.000.000) 2022-07-27 15:13:32 Job terminated. 005 (236892.000.000) 2022-07-27 15:13:58 Job terminated. 005 (236890.000.000) 2022-07-27 15:13:59 Job terminated. [username@ap40]$ grep 'termination' my-job.log (1) Normal termination (return value 0) (1) Normal termination (return value 1) (1) Normal termination (return value 0) (1) Normal termination (return value 0) (1) Normal termination (return value 1) (1) Normal termination (return value 0) (1) Normal termination (return value 0) (1) Normal termination (return value 0) [username@ap40]$ grep 'termination' my-job.log | sort | uniq -c 6 (1) Normal termination (return value 0) 2 (1) Normal termination (return value 1)","title":"Job terminations"},{"location":"materials/scaling/part3-ex2-log-files/#job-resource-usage","text":"Jobs' resource usages (and requests and allocations) are logged in the following format: Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 10382 1048576 1468671 Memory (MB) : 692 1024 1024 Run the following grep command to pull out the memory information from your job logs: $ grep 'Memory (MB) *:' my-job.log Look back at the format in the example above. Columns after the : will first show memory usage, then memory requested, and then the memory allocated to your job. Similarly, use the following command to get the disk information from your job logs: $ grep 'Disk (KB) *:' my-job.log Here's some example output from running the memory grep command on the same eight-job log file: [username@ap40]$ grep 'Memory (MB) *:' my-job.log Memory (MB) : 692 1024 1024 Memory (MB) : 714 1024 1024 Memory (MB) : 703 1024 1024 Memory (MB) : 699 1024 1024 Memory (MB) : 705 1024 1024 Memory (MB) : 704 1024 1024 Memory (MB) : 711 1024 1024 Memory (MB) : 697 1024 1024 In this example, the memory usage for the jobs ranged from 692 to 714 MB, and they all requested (and were allocated) 1 GB of memory.","title":"Job resource usage"},{"location":"materials/scaling/part3-ex2-log-files/#other-job-information","text":"See if you can come up with grep commands to gather the number of bytes sent and received by jobs (i.e. how much data was transferred to/from the access point). Here is some example output for comparison: [username@ap40]$ grep '<PATTERN REMOVED>' my-job.log 760393 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760397 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760393 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job 760397 - Total Bytes Sent By Job 760395 - Total Bytes Sent By Job [username@ap40]$ grep '<PATTERN REMOVED>' my-job.log 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job 19240 - Total Bytes Received By Job Job log files may also contain additional information about held jobs or interrupted jobs. If you feel that your jobs are bouncing from idle to running and back to idle, or that they are otherwise not making as much progress as you expect, the log files are a good place to check. Though they might eventually become impossibly large to read line-by-line once you start scaling up, using grep to pull out specific lines and using sort and uniq to reduce the output can help you make sense of the information contained in the logs.","title":"Other job information"},{"location":"materials/software/part1-ex1-run-apptainer/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 1.1: Run and Explore Containers \u00b6 Objective : Run a container interactively Why learn this? : Being able to run a container directly allows you to confirm what is installed and whether any additional scripts or code will work in the context of the container. Setup \u00b6 Make sure you are logged into ap40.uw.osg-htc.org . For this exercise we will be using Apptainer containers maintained by OSG staff or existing containers on Docker Hub. There is some set-up that we should do to help lighten the load on the Access Point as we work with containers. First, download the apptainer-setup.sh script: osdf object get /ospool/uc-shared/public/school/2025/apptainer-setup.sh ./ Then run the script using this command: . apptainer-setup.sh You should see a message that the setup has been completed. (This is the only time you'll need to run this script.) Exploring Apptainer Containers \u00b6 First, let's try to run a container from the OSG-Supported List . Find the full path for the ubuntu 22.04 container image. To run it, use this command: $ apptainer shell /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 It may take a few minutes to start - don't worry if this happens. About the /cvmfs path In the above example, we used a path beginning with /cvmfs to launch a container. This should be the only place that you use such a path! Once the container starts, the prompt will change to either Singularity> or Apptainer> . Run ls and pwd . Where are you? Do you see your files? The apptainer shell command will automatically connect your home directory to the running container so you can use your files. How do we know we're in a different Linux environment? Try printing out the Linux version, or checking the version of common tools like gcc or Python: $ grep \"PRETTY_NAME\" /etc/os-release $ gcc --version $ python3 --version Full Linux version information In the above example, we used grep to only show the value of the PRETTY_NAME in the /etc/os-release file. There's a lot more information in that file about the Linux version, which you can see with $ cat /etc/os-release Exit out of the container by typing exit . Type the same commands back on the normal Access Point. Should they give the same results as when typed in the container, or different? $ grep \"PRETTY_NAME\" /etc/os-release $ gcc --version $ python3 --version Exploring Docker Containers \u00b6 The process for interactively running a Docker container will be very similar to an apptainer container. The main difference is a docker:// prefix before the container's identifying name. We are going to be using a Python image from Docker Hub . Click on the \"Tags\" tab to see all the different versions of this container that exists. Let's use version 3.10 . To run it interactively, use this command: $ apptainer shell docker://python:3.10 Once the container starts and the prompt changes, try running similar commands as above. What version of Linux is used in this container? Does the version of Python match what you expect, based on the name of the container? Once done, type exit to leave the container. Apply to Your Work \u00b6 Is the software you want to use already available via a container registry such as DockerHub , NVIDIA Catalog , or elsewhere? Consider what it takes to install your software - if you could choose the operating system, would that make it easier to install your software?","title":"1.1 - Run and Explore Apptainer Containers"},{"location":"materials/software/part1-ex1-run-apptainer/#software-exercise-11-run-and-explore-containers","text":"Objective : Run a container interactively Why learn this? : Being able to run a container directly allows you to confirm what is installed and whether any additional scripts or code will work in the context of the container.","title":"Software Exercise 1.1: Run and Explore Containers"},{"location":"materials/software/part1-ex1-run-apptainer/#setup","text":"Make sure you are logged into ap40.uw.osg-htc.org . For this exercise we will be using Apptainer containers maintained by OSG staff or existing containers on Docker Hub. There is some set-up that we should do to help lighten the load on the Access Point as we work with containers. First, download the apptainer-setup.sh script: osdf object get /ospool/uc-shared/public/school/2025/apptainer-setup.sh ./ Then run the script using this command: . apptainer-setup.sh You should see a message that the setup has been completed. (This is the only time you'll need to run this script.)","title":"Setup"},{"location":"materials/software/part1-ex1-run-apptainer/#exploring-apptainer-containers","text":"First, let's try to run a container from the OSG-Supported List . Find the full path for the ubuntu 22.04 container image. To run it, use this command: $ apptainer shell /cvmfs/singularity.opensciencegrid.org/htc/ubuntu:22.04 It may take a few minutes to start - don't worry if this happens. About the /cvmfs path In the above example, we used a path beginning with /cvmfs to launch a container. This should be the only place that you use such a path! Once the container starts, the prompt will change to either Singularity> or Apptainer> . Run ls and pwd . Where are you? Do you see your files? The apptainer shell command will automatically connect your home directory to the running container so you can use your files. How do we know we're in a different Linux environment? Try printing out the Linux version, or checking the version of common tools like gcc or Python: $ grep \"PRETTY_NAME\" /etc/os-release $ gcc --version $ python3 --version Full Linux version information In the above example, we used grep to only show the value of the PRETTY_NAME in the /etc/os-release file. There's a lot more information in that file about the Linux version, which you can see with $ cat /etc/os-release Exit out of the container by typing exit . Type the same commands back on the normal Access Point. Should they give the same results as when typed in the container, or different? $ grep \"PRETTY_NAME\" /etc/os-release $ gcc --version $ python3 --version","title":"Exploring Apptainer Containers"},{"location":"materials/software/part1-ex1-run-apptainer/#exploring-docker-containers","text":"The process for interactively running a Docker container will be very similar to an apptainer container. The main difference is a docker:// prefix before the container's identifying name. We are going to be using a Python image from Docker Hub . Click on the \"Tags\" tab to see all the different versions of this container that exists. Let's use version 3.10 . To run it interactively, use this command: $ apptainer shell docker://python:3.10 Once the container starts and the prompt changes, try running similar commands as above. What version of Linux is used in this container? Does the version of Python match what you expect, based on the name of the container? Once done, type exit to leave the container.","title":"Exploring Docker Containers"},{"location":"materials/software/part1-ex1-run-apptainer/#apply-to-your-work","text":"Is the software you want to use already available via a container registry such as DockerHub , NVIDIA Catalog , or elsewhere? Consider what it takes to install your software - if you could choose the operating system, would that make it easier to install your software?","title":"Apply to Your Work"},{"location":"materials/software/part1-ex2-apptainer-jobs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 1.2: Use Apptainer Containers in OSPool Jobs \u00b6 Objective : Submit a job that uses an existing apptainer container; compare default job environment with a specific container job environment. Why learn this? : By comparing a non-container and container job, you'll better understand what a container can do on the OSPool. This may also be how you end up submitting your jobs if you can find an existing apptainer container with your software. Default Environment \u00b6 First, let's run a job without a container to see what the typical job environment is. Create a bash script named script.sh with the following lines: #!/bin/bash hostname grep \"PRETTY_NAME\" /etc/os-release gcc --version python3 --version This will print out the version of Linux on the computer, the version of gcc , a common software compiler, and the version of Python 3. Make the script executable: $ chmod +x script.sh Run the script on the Access Point. $ ./script.sh What results did you get? Copy a submit file from a previous OSPool job and edit it so that the script you just wrote is the executable. Submit the job and read the standard output file when it completes. What version of Linux was used for the job? What is the version of gcc or Python? Container Environment \u00b6 Now, let's try running that same script inside a container. For this job, we will use the OSG-provided Ubuntu \"Focal\" image, as we did in the previous exercise. The container_image submit file option will tell HTCondor to use this container for the job: universe = container OSDF_URL = osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64 container_image = $(OSDF_URL)/htc__ubuntu__22.04.sif If the submit file you copied has something like requirements = (OSGVO_OS_STRING == \"RHEL 9\") , remove that. When you use containers, you should not specify an OS in the requirements as that will unnecessarily limit the number of resources you can run on. Submit the job and read the standard output file when it completes. What version of Linux was used for the job? What is the version of gcc ? or Python? Experimenting With Other Containers \u00b6 Look at the list of OSG-Supported containers: OSG Supported Containers Try submitting a job that uses one of these containers. Change the executable script to explore different aspects of that container. Apply to Your Work \u00b6 Could you use any of the OSG-supported containers to run your calculations? If not, could you use any of them as a starting point for installing the missing software? How important is it for your jobs to have a consistent software environment? How portable are scripts and software that you use?","title":"1.2 - Use Apptainer Containers in OSPool Jobs"},{"location":"materials/software/part1-ex2-apptainer-jobs/#software-exercise-12-use-apptainer-containers-in-ospool-jobs","text":"Objective : Submit a job that uses an existing apptainer container; compare default job environment with a specific container job environment. Why learn this? : By comparing a non-container and container job, you'll better understand what a container can do on the OSPool. This may also be how you end up submitting your jobs if you can find an existing apptainer container with your software.","title":"Software Exercise 1.2: Use Apptainer Containers in OSPool Jobs"},{"location":"materials/software/part1-ex2-apptainer-jobs/#default-environment","text":"First, let's run a job without a container to see what the typical job environment is. Create a bash script named script.sh with the following lines: #!/bin/bash hostname grep \"PRETTY_NAME\" /etc/os-release gcc --version python3 --version This will print out the version of Linux on the computer, the version of gcc , a common software compiler, and the version of Python 3. Make the script executable: $ chmod +x script.sh Run the script on the Access Point. $ ./script.sh What results did you get? Copy a submit file from a previous OSPool job and edit it so that the script you just wrote is the executable. Submit the job and read the standard output file when it completes. What version of Linux was used for the job? What is the version of gcc or Python?","title":"Default Environment"},{"location":"materials/software/part1-ex2-apptainer-jobs/#container-environment","text":"Now, let's try running that same script inside a container. For this job, we will use the OSG-provided Ubuntu \"Focal\" image, as we did in the previous exercise. The container_image submit file option will tell HTCondor to use this container for the job: universe = container OSDF_URL = osdf:///ospool/uc-shared/public/OSG-Staff/images/repo/x86_64 container_image = $(OSDF_URL)/htc__ubuntu__22.04.sif If the submit file you copied has something like requirements = (OSGVO_OS_STRING == \"RHEL 9\") , remove that. When you use containers, you should not specify an OS in the requirements as that will unnecessarily limit the number of resources you can run on. Submit the job and read the standard output file when it completes. What version of Linux was used for the job? What is the version of gcc ? or Python?","title":"Container Environment"},{"location":"materials/software/part1-ex2-apptainer-jobs/#experimenting-with-other-containers","text":"Look at the list of OSG-Supported containers: OSG Supported Containers Try submitting a job that uses one of these containers. Change the executable script to explore different aspects of that container.","title":"Experimenting With Other Containers"},{"location":"materials/software/part1-ex2-apptainer-jobs/#apply-to-your-work","text":"Could you use any of the OSG-supported containers to run your calculations? If not, could you use any of them as a starting point for installing the missing software? How important is it for your jobs to have a consistent software environment? How portable are scripts and software that you use?","title":"Apply to Your Work"},{"location":"materials/software/part1-ex3-docker-jobs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 1.3: Use Docker Containers in OSPool Jobs \u00b6 Objective : Create a local copy of a Docker container, use it to submit a job. Why learn this? : Same as the previous exercise; this may also be how you end up submitting your jobs if you can find an existing Docker container with your software. Create Local Copy of Docker Container \u00b6 While it is technically possible to use a Docker container directly in a job, there are some good reasons for converting it to a local Apptainer container first. We'll do this with the same python:3.10 Docker container we used in the first exercise . Ensure environment is ready for Apptainer commands If your login has been interrupted or you've changed terminals since Software Exercise 1.1 , then make sure to run the commands in the Setup section of that exercise before proceeding! To convert the Docker container to a local Apptainer container, run: $ apptainer build local-py310.sif docker://python:3.10 The first argument after build is the name of the new Apptainer container file, the second argument is what we're building from (in this case, Docker). Submit File and Executable \u00b6 Make a copy of your submit file from the previous container exercise or build from an existing submit file. Add the following lines to the submit file or modify existing lines to match the lines below: universe = container container_image = local-py310.sif Use the same executable as the previous exercise . Once these steps are done, submit the job. You might get a warning about using OSDF for container transfers - ignore this warning for now. Proper location for container .sif files The above example is storing the local-py310.sif file in the home directory on the Access Point, and in turn that means the job will transfer the file via the Access Point. Container image files, however, are typically large and if you are submitting many jobs, the Access Point can be overwhelmed trying to transfer so many large files! In practice, container image files like this one should be placed in your $DATA directory, and the submit file should use the osdf:/// protocol to declare the transfer. For more information, see the Data Exercises or the OSPool guide on using the OSDF . Finding Docker Containers \u00b6 There are a lot of Docker containers on Docker Hub, but they are not all created equal. Anyone can create an account on Docker Hub and share container images there, so it\u2019s important to exercise caution when choosing a container image on Docker Hub. These are some indicators that a container image on Docker Hub is consistently maintained, functional and secure: The container image is updated regularly. The container image is associated with a well established company, community, or other group that is well-known. There is a Dockerfile or other listing of what has been installed to the container image. The container image page has documentation on how to use the container image. 1 Given these indicators: Can you find a container on Docker Hub that would be useful for running Jupyter notebooks that use tensorflow? Does your chosen image meet at least 2 of the criteria above? Apply to Your Work \u00b6 Do you or your colleagues use Docker containers for running calculations? Is the software you want to use already available via a container registry such as DockerHub , NVIDIA Catalog , or elsewhere? This list and previous text taken from Introduction to Docker \u21a9","title":"1.3 - Use Docker Containers in OSPool Jobs"},{"location":"materials/software/part1-ex3-docker-jobs/#software-exercise-13-use-docker-containers-in-ospool-jobs","text":"Objective : Create a local copy of a Docker container, use it to submit a job. Why learn this? : Same as the previous exercise; this may also be how you end up submitting your jobs if you can find an existing Docker container with your software.","title":"Software Exercise 1.3: Use Docker Containers in OSPool Jobs"},{"location":"materials/software/part1-ex3-docker-jobs/#create-local-copy-of-docker-container","text":"While it is technically possible to use a Docker container directly in a job, there are some good reasons for converting it to a local Apptainer container first. We'll do this with the same python:3.10 Docker container we used in the first exercise . Ensure environment is ready for Apptainer commands If your login has been interrupted or you've changed terminals since Software Exercise 1.1 , then make sure to run the commands in the Setup section of that exercise before proceeding! To convert the Docker container to a local Apptainer container, run: $ apptainer build local-py310.sif docker://python:3.10 The first argument after build is the name of the new Apptainer container file, the second argument is what we're building from (in this case, Docker).","title":"Create Local Copy of Docker Container"},{"location":"materials/software/part1-ex3-docker-jobs/#submit-file-and-executable","text":"Make a copy of your submit file from the previous container exercise or build from an existing submit file. Add the following lines to the submit file or modify existing lines to match the lines below: universe = container container_image = local-py310.sif Use the same executable as the previous exercise . Once these steps are done, submit the job. You might get a warning about using OSDF for container transfers - ignore this warning for now. Proper location for container .sif files The above example is storing the local-py310.sif file in the home directory on the Access Point, and in turn that means the job will transfer the file via the Access Point. Container image files, however, are typically large and if you are submitting many jobs, the Access Point can be overwhelmed trying to transfer so many large files! In practice, container image files like this one should be placed in your $DATA directory, and the submit file should use the osdf:/// protocol to declare the transfer. For more information, see the Data Exercises or the OSPool guide on using the OSDF .","title":"Submit File and Executable"},{"location":"materials/software/part1-ex3-docker-jobs/#finding-docker-containers","text":"There are a lot of Docker containers on Docker Hub, but they are not all created equal. Anyone can create an account on Docker Hub and share container images there, so it\u2019s important to exercise caution when choosing a container image on Docker Hub. These are some indicators that a container image on Docker Hub is consistently maintained, functional and secure: The container image is updated regularly. The container image is associated with a well established company, community, or other group that is well-known. There is a Dockerfile or other listing of what has been installed to the container image. The container image page has documentation on how to use the container image. 1 Given these indicators: Can you find a container on Docker Hub that would be useful for running Jupyter notebooks that use tensorflow? Does your chosen image meet at least 2 of the criteria above?","title":"Finding Docker Containers"},{"location":"materials/software/part1-ex3-docker-jobs/#apply-to-your-work","text":"Do you or your colleagues use Docker containers for running calculations? Is the software you want to use already available via a container registry such as DockerHub , NVIDIA Catalog , or elsewhere? This list and previous text taken from Introduction to Docker \u21a9","title":"Apply to Your Work"},{"location":"materials/software/part1-ex4-apptainer-build/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 1.4: Build, Test, and Deploy an Apptainer Container \u00b6 Objective : to practice building and using a custom apptainer container Why learn this? : You may need to go through this process if you want to use a container for your jobs and can't find one that has what you need. Motivating Script \u00b6 Create a script called hello-cow.py : #!/usr/bin/env python3 import cowsay cowsay.cow('Hello OSG User School') Give it executable permissions: $ chmod +x hello-cow.py Try running the script: $ ./hello-cow.py It will likely fail, because the cowsay library isn't installed. This is a scenario where we will want to build our own container that includes a base Python installation and the cowsay Python library. Preparing a Definition File \u00b6 We can describe our desired Apptainer image in a special format called a definition file . This has special keywords that will direct Apptainer when it builds the container image. Create a file called py-cowsay.def with these contents: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 %post apt-get update -y apt-get install -y \\ python3-pip \\ python3-numpy python3 -m pip install cowsay Note that we are starting with the same ubuntu base we used in previous exercises. The %post statement includes our installation commands, including updating the pip and numpy packages, and then using pip to install cowsay . To learn more about definition files, see Exercise 3.1 Build the Container \u00b6 Once the definition file is complete, we can build the container. Ensure environment is ready for Apptainer commands If your login has been interrupted or you've changed terminals since Software Exercise 1.1 , then make sure to run the commands in the Setup section of that exercise before proceeding! Run the following command to build the container: $ apptainer build py-cowsay.sif py-cowsay.def As with the Docker image in the previous exercise , the first argument is the name to give to the newly create image file and the second argument is how to build the container image - in this case, the definition file. Testing the Image Locally \u00b6 Do you remember how to interactively test an image? Look back at Exercise 1.1 and guess what command would allow us to test our new container. Try running: $ apptainer shell py-cowsay.sif Then try running the hello-cow.py script: Apptainer> ./hello-cow.py If it produces an output, our container works! We can now exit (by typing exit ) and submit a job. Submit a Job \u00b6 Make a copy of a submit file from a previous exercise in this section. Can you guess what options need to be used or modified? Make sure you have the following (in addition to log , error , output and CPU and memory requests): universe = container container_image = py-cowsay.sif executable = hello-cow.py Submit the job and verify the output when it completes. ______________________ | Hello OSG User School! | ====================== \\ \\ ^__^ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Proper location for container .sif files The above example is storing the py-cowsay.sif file in the home directory on the Access Point, and in turn that means the job will transfer the file via the Access Point. Container image files, however, are typically large and if you are submitting many jobs, the Access Point can be overwhelmed trying to transfer so many large files! In practice, container image files like this one should be placed in your $DATA directory, and the submit file should use the osdf:/// protocol to declare the transfer. For more information, see the Data Exercises or the OSPool guide on using the OSDF . Apply to Your Work \u00b6 Have you ever wanted to \"just install\" your software for use on the OSPool? Could you accomplish that by building your own Apptainer container? Do you know how to install your software on a brand new computer? If so, how you would incorporate those instructions into an Apptainer definition file? If not, can you find the necessary instructions? Do you have a simple test you can use to check if the software you want to use is working as expected?","title":"1.4 - Build, Test, and Deploy an Apptainer Container"},{"location":"materials/software/part1-ex4-apptainer-build/#software-exercise-14-build-test-and-deploy-an-apptainer-container","text":"Objective : to practice building and using a custom apptainer container Why learn this? : You may need to go through this process if you want to use a container for your jobs and can't find one that has what you need.","title":"Software Exercise 1.4: Build, Test, and Deploy an Apptainer Container"},{"location":"materials/software/part1-ex4-apptainer-build/#motivating-script","text":"Create a script called hello-cow.py : #!/usr/bin/env python3 import cowsay cowsay.cow('Hello OSG User School') Give it executable permissions: $ chmod +x hello-cow.py Try running the script: $ ./hello-cow.py It will likely fail, because the cowsay library isn't installed. This is a scenario where we will want to build our own container that includes a base Python installation and the cowsay Python library.","title":"Motivating Script"},{"location":"materials/software/part1-ex4-apptainer-build/#preparing-a-definition-file","text":"We can describe our desired Apptainer image in a special format called a definition file . This has special keywords that will direct Apptainer when it builds the container image. Create a file called py-cowsay.def with these contents: Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 %post apt-get update -y apt-get install -y \\ python3-pip \\ python3-numpy python3 -m pip install cowsay Note that we are starting with the same ubuntu base we used in previous exercises. The %post statement includes our installation commands, including updating the pip and numpy packages, and then using pip to install cowsay . To learn more about definition files, see Exercise 3.1","title":"Preparing a Definition File"},{"location":"materials/software/part1-ex4-apptainer-build/#build-the-container","text":"Once the definition file is complete, we can build the container. Ensure environment is ready for Apptainer commands If your login has been interrupted or you've changed terminals since Software Exercise 1.1 , then make sure to run the commands in the Setup section of that exercise before proceeding! Run the following command to build the container: $ apptainer build py-cowsay.sif py-cowsay.def As with the Docker image in the previous exercise , the first argument is the name to give to the newly create image file and the second argument is how to build the container image - in this case, the definition file.","title":"Build the Container"},{"location":"materials/software/part1-ex4-apptainer-build/#testing-the-image-locally","text":"Do you remember how to interactively test an image? Look back at Exercise 1.1 and guess what command would allow us to test our new container. Try running: $ apptainer shell py-cowsay.sif Then try running the hello-cow.py script: Apptainer> ./hello-cow.py If it produces an output, our container works! We can now exit (by typing exit ) and submit a job.","title":"Testing the Image Locally"},{"location":"materials/software/part1-ex4-apptainer-build/#submit-a-job","text":"Make a copy of a submit file from a previous exercise in this section. Can you guess what options need to be used or modified? Make sure you have the following (in addition to log , error , output and CPU and memory requests): universe = container container_image = py-cowsay.sif executable = hello-cow.py Submit the job and verify the output when it completes. ______________________ | Hello OSG User School! | ====================== \\ \\ ^__^ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || Proper location for container .sif files The above example is storing the py-cowsay.sif file in the home directory on the Access Point, and in turn that means the job will transfer the file via the Access Point. Container image files, however, are typically large and if you are submitting many jobs, the Access Point can be overwhelmed trying to transfer so many large files! In practice, container image files like this one should be placed in your $DATA directory, and the submit file should use the osdf:/// protocol to declare the transfer. For more information, see the Data Exercises or the OSPool guide on using the OSDF .","title":"Submit a Job"},{"location":"materials/software/part1-ex4-apptainer-build/#apply-to-your-work","text":"Have you ever wanted to \"just install\" your software for use on the OSPool? Could you accomplish that by building your own Apptainer container? Do you know how to install your software on a brand new computer? If so, how you would incorporate those instructions into an Apptainer definition file? If not, can you find the necessary instructions? Do you have a simple test you can use to check if the software you want to use is working as expected?","title":"Apply to Your Work"},{"location":"materials/software/part1-ex5-pick-an-option/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 1.5 - Choose Software Options \u00b6 Objective : Decide how you want to make your software portable Why learn this? : This is the next step to getting your own research jobs running on the OSPool! Know Your Software \u00b6 Pick at least one software you want to use on the OSPool as a test subject. Then: Find the download and/or installation page and read through the instructions and options there. Is the software available as a binary download, or will you need to run some kind of command to install it or compile it from source? If there are multiple download/installation options, which is which? What pre-requisites does this software need to be installed? Example 1: an R package will require a base R installation Example 2: some codes require that a library called the \"Gnu Scientific Library (GSL) be already installed on your computer\" Choose a Strategy \u00b6 Are there any existing containers that contain this software already? Explore OSG-Supported Containers Explore DockerHub , for example: miniconda rocker jupyter nvidia (and many more!) If yes, try using this container first, as shown in Exercise 1.2 and Exercise 1.3 Is there a simple download or easy compilation process? If so, can you download the software and use it via a wrapper script? See the exercises from Part 4 ( Download Software Files , Use a Wrapper Script , Wrapper Script Arguments ). To learn more about using this approach for specific softwares, see the examples in Part 5 . Are you using conda? See the specific example in Exercise 5.3 If neither of the above options works (which may be true for more software!), you may want to build your own container. If you want to just use this container on the OSPool, build an Apptainer container as described in Exercise 1.4 and with more information in Exercise 3.1 If you want to use the container on your own computer or share with others who would use it on a laptop or desktop, look at the Docker container example in Exercise 3.2 . Don't do ALL of the software exercises in parts 3 - 5! Instead, choose the section(s) that makes sense based on how you want to manage your software. Talk to the School instructors to help make this decision if you are unsure. Create an Executable \u00b6 Regardless of which approach you use, check out the Build an HTC-Friendly Executable exercise for some tips on how to make your script more robust and easy to use with multiple jobs. Apply to Your Work \u00b6 This whole page is about applying the previous exercises to your work \ud83d\ude42.","title":"1.5 - Choose Software Options"},{"location":"materials/software/part1-ex5-pick-an-option/#software-exercise-15-choose-software-options","text":"Objective : Decide how you want to make your software portable Why learn this? : This is the next step to getting your own research jobs running on the OSPool!","title":"Software Exercise 1.5 - Choose Software Options"},{"location":"materials/software/part1-ex5-pick-an-option/#know-your-software","text":"Pick at least one software you want to use on the OSPool as a test subject. Then: Find the download and/or installation page and read through the instructions and options there. Is the software available as a binary download, or will you need to run some kind of command to install it or compile it from source? If there are multiple download/installation options, which is which? What pre-requisites does this software need to be installed? Example 1: an R package will require a base R installation Example 2: some codes require that a library called the \"Gnu Scientific Library (GSL) be already installed on your computer\"","title":"Know Your Software"},{"location":"materials/software/part1-ex5-pick-an-option/#choose-a-strategy","text":"Are there any existing containers that contain this software already? Explore OSG-Supported Containers Explore DockerHub , for example: miniconda rocker jupyter nvidia (and many more!) If yes, try using this container first, as shown in Exercise 1.2 and Exercise 1.3 Is there a simple download or easy compilation process? If so, can you download the software and use it via a wrapper script? See the exercises from Part 4 ( Download Software Files , Use a Wrapper Script , Wrapper Script Arguments ). To learn more about using this approach for specific softwares, see the examples in Part 5 . Are you using conda? See the specific example in Exercise 5.3 If neither of the above options works (which may be true for more software!), you may want to build your own container. If you want to just use this container on the OSPool, build an Apptainer container as described in Exercise 1.4 and with more information in Exercise 3.1 If you want to use the container on your own computer or share with others who would use it on a laptop or desktop, look at the Docker container example in Exercise 3.2 . Don't do ALL of the software exercises in parts 3 - 5! Instead, choose the section(s) that makes sense based on how you want to manage your software. Talk to the School instructors to help make this decision if you are unsure.","title":"Choose a Strategy"},{"location":"materials/software/part1-ex5-pick-an-option/#create-an-executable","text":"Regardless of which approach you use, check out the Build an HTC-Friendly Executable exercise for some tips on how to make your script more robust and easy to use with multiple jobs.","title":"Create an Executable"},{"location":"materials/software/part1-ex5-pick-an-option/#apply-to-your-work","text":"This whole page is about applying the previous exercises to your work \ud83d\ude42.","title":"Apply to Your Work"},{"location":"materials/software/part2-ex1-build-executable/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 2.1 Build an HTC-Friendly Executable \u00b6 Objective : Modify an existing script to include arguments and headers. Why learn this? : A little bit of preparation can make it easier to reuse the same script over and over to run many jobs. Setup \u00b6 Download the materials for this exercise using $ osdf object get /ospool/uc-shared/public/school/2025/contexts.tar.gz ./ $ tar -xzf contexts.tar.gz There are two scripts included in the materials: word-variations.py and word-contexts.py . The first one will identify \"trivial\" variations (capitalization, punctuation) on the provided word. For example: $ ./word-variations.py Alice_in_Wonderland.txt Dodo The second one expands the detection of the provided word to report an additional word before and after the provided word, for example: $ ./word-contexts.py Alice_in_Wonderland.txt Dodo We want to use these two scripts to find all the contexts where a word was used, regardless of the \"trivial\" variations. To do so, we first save the results of the word-variations.py command to a file, then use the contents of that file with the word-contexts.py command (also saving the output to file): $ ./word-variations.py Alice_in_Wonderland.txt Dodo > variations.Dodo.txt $ ./word-contexts.py Alice_in_Wonderland.txt variations.Dodo.txt > contexts.Dodo.txt Add a Header \u00b6 To create a basic script, you can put the above commands into a file called get_contexts.sh . To make it clear what language we expect to use to run the script, we will add the following header on the first line: `#!/bin/bash #!/bin/bash ./word-variations.py Alice_in_Wonderland.txt Dodo > variations.Dodo.txt ./word-contexts.py Alice_in_Wonderland.txt variations.Dodo.txt > contexts.Dodo.txt The \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that you would run individual commands on the command line. We use /bin/bash instead of just bash because that is the full path to the bash software file. Other languages We can use the same principle for any scripting language. For example, the header for a Python script could be either #!/usr/bin/python3 or #!/usr/bin/env python3 . Similar logic works for perl, R, julia and other scripting languages. Can you now run the script? $ ./get_contexts.sh This gives \"permission denied.\" Let's add executable permissions to the script and try again: $ chmod +x get_contexts.sh $ ./get_contexts.sh Incorporate Arguments \u00b6 Let's say that you want to find the contexts for every occurrence of a character's name. Can you imagine trying to run this script for each name? It would be tedious to edit it for each one, especially since the name is used in multiple places in the script. And if you want to use this script to analyze other texts, you'd also have to change the filename. Instead of manually editing the get_contexts.sh for each time we want to run a different analysis, we should add arguments to the script to make it easy to reuse. Any information in a script or executable that is going to change or vary across jobs or analyses should likely be turned into an argument that is specified on the command line. In our example above, which pieces of the script are likely to change or vary? The name of the input file ( Alice_in_Wonderland.txt ) and the name to detect ( Dodo ) should be turned into arguments. Can you envision what our script should look like if we ran it with input arguments? Let's say we want to be able to run the following command: $ ./get_contexts.sh Alice_in_Wonderland.txt Dodo In order to get arguments from the command line into the script, you have to use special variables in the script. In bash, these are $1 (for the first argument), $2 (for the second argument) and so on. Try to figure out where these should go in our get_contexts.sh script. Other Languages Each language is going to have its own syntax for reading command line arguments into the script. In Python, sys.argv is a basic method, and more advanced libraries like argparse can be used. In R, the commandArgs() function can do this. Google \"command line arguments in ______\" to find the right syntax for your language of choice! A first pass at adding arguments might look like this: #!/bin/bash ./word-variations.py $1 $2 > variations.$2.txt ./word-contexts.py $1 variations.$2.txt > contexts.$2.txt Try running it as described above. Does it work? While we now have arguments, we have lost some of the readability of our script. The numbers $1 and $2 are not very meaningful in themselves! Let's rewrite the script to assign the arguments to meaningful variable names: #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > variations.${TARGET_WORD}.txt ./word-contexts.py ${FILENAME} variations.${TARGET_WORD}.txt > contexts.${TARGET_WORD}.txt Why curly brackets? You'll notice above that we started using curly brackets around our variables. While you technically don't need them ( $TARGET_WORD would also be fine), using them makes the name of the variable (compared to other text) completely clear. This is especially useful when combining variables with underscores, and in the case of specifying the numerical arguments ( $1 , $2 , etc.) the curly braces allow you to go above a value of 9 without error. There is one final place where we could optimize this script. The output filename variations.${TARGET_WORD}.txt is used in multiple places in the script. If we decide we want to change the naming convention of the output file, we'd have to remember to edit all the locations where that name is used, which is asking for typos. Instead, we should use a variable for referring to the output filename, so that we only ever need to change one spot in the file. We can do a similar thing for the second output file. After making these changes, the script will look like this: #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" VARIATIONS_OUTPUT=\"variations.${TARGET_WORD}.txt\" CONTEXTS_OUTPUT=\"contexts.${TARGET_WORD}.txt\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > ${VARIATIONS_OUTPUT} ./word-contexts.py ${FILENAME} ${VARIATIONS_OUTPUT} > ${CONTEXTS_OUTPUT} You may consider this optimization to be optimal and that's fine. But for longer scripts, being able to change \"customizable\" values in a single place helps prevent inconsistencies when making edits. Furthermore, this structure makes it easier to add additional arguments to the script. For example, if we wanted to always define custom names for the output files we could change the definitions at the top of script to read in the third and fourth arguments as the value, i.e., #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" VARIATIONS_OUTPUT=\"${3}\" CONTEXTS_OUTPUT=\"${4}\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > ${VARIATIONS_OUTPUT} ./word-contexts.py ${FILENAME} ${VARIATIONS_OUTPUT} > ${CONTEXTS_OUTPUT} Apply to Your Work \u00b6 Are you using a scripting language where you could add a header to your main script? If so, what should it be? What items in your main code or commands are changing? Do you need to add arguments to your code? How could you use variables in your script to make it easier to pass arguments and make changes to your code?","title":"2.1 - Build an HTC-Friendly Executable"},{"location":"materials/software/part2-ex1-build-executable/#software-exercise-21-build-an-htc-friendly-executable","text":"Objective : Modify an existing script to include arguments and headers. Why learn this? : A little bit of preparation can make it easier to reuse the same script over and over to run many jobs.","title":"Software Exercise 2.1 Build an HTC-Friendly Executable"},{"location":"materials/software/part2-ex1-build-executable/#setup","text":"Download the materials for this exercise using $ osdf object get /ospool/uc-shared/public/school/2025/contexts.tar.gz ./ $ tar -xzf contexts.tar.gz There are two scripts included in the materials: word-variations.py and word-contexts.py . The first one will identify \"trivial\" variations (capitalization, punctuation) on the provided word. For example: $ ./word-variations.py Alice_in_Wonderland.txt Dodo The second one expands the detection of the provided word to report an additional word before and after the provided word, for example: $ ./word-contexts.py Alice_in_Wonderland.txt Dodo We want to use these two scripts to find all the contexts where a word was used, regardless of the \"trivial\" variations. To do so, we first save the results of the word-variations.py command to a file, then use the contents of that file with the word-contexts.py command (also saving the output to file): $ ./word-variations.py Alice_in_Wonderland.txt Dodo > variations.Dodo.txt $ ./word-contexts.py Alice_in_Wonderland.txt variations.Dodo.txt > contexts.Dodo.txt","title":"Setup"},{"location":"materials/software/part2-ex1-build-executable/#add-a-header","text":"To create a basic script, you can put the above commands into a file called get_contexts.sh . To make it clear what language we expect to use to run the script, we will add the following header on the first line: `#!/bin/bash #!/bin/bash ./word-variations.py Alice_in_Wonderland.txt Dodo > variations.Dodo.txt ./word-contexts.py Alice_in_Wonderland.txt variations.Dodo.txt > contexts.Dodo.txt The \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that you would run individual commands on the command line. We use /bin/bash instead of just bash because that is the full path to the bash software file. Other languages We can use the same principle for any scripting language. For example, the header for a Python script could be either #!/usr/bin/python3 or #!/usr/bin/env python3 . Similar logic works for perl, R, julia and other scripting languages. Can you now run the script? $ ./get_contexts.sh This gives \"permission denied.\" Let's add executable permissions to the script and try again: $ chmod +x get_contexts.sh $ ./get_contexts.sh","title":"Add a Header"},{"location":"materials/software/part2-ex1-build-executable/#incorporate-arguments","text":"Let's say that you want to find the contexts for every occurrence of a character's name. Can you imagine trying to run this script for each name? It would be tedious to edit it for each one, especially since the name is used in multiple places in the script. And if you want to use this script to analyze other texts, you'd also have to change the filename. Instead of manually editing the get_contexts.sh for each time we want to run a different analysis, we should add arguments to the script to make it easy to reuse. Any information in a script or executable that is going to change or vary across jobs or analyses should likely be turned into an argument that is specified on the command line. In our example above, which pieces of the script are likely to change or vary? The name of the input file ( Alice_in_Wonderland.txt ) and the name to detect ( Dodo ) should be turned into arguments. Can you envision what our script should look like if we ran it with input arguments? Let's say we want to be able to run the following command: $ ./get_contexts.sh Alice_in_Wonderland.txt Dodo In order to get arguments from the command line into the script, you have to use special variables in the script. In bash, these are $1 (for the first argument), $2 (for the second argument) and so on. Try to figure out where these should go in our get_contexts.sh script. Other Languages Each language is going to have its own syntax for reading command line arguments into the script. In Python, sys.argv is a basic method, and more advanced libraries like argparse can be used. In R, the commandArgs() function can do this. Google \"command line arguments in ______\" to find the right syntax for your language of choice! A first pass at adding arguments might look like this: #!/bin/bash ./word-variations.py $1 $2 > variations.$2.txt ./word-contexts.py $1 variations.$2.txt > contexts.$2.txt Try running it as described above. Does it work? While we now have arguments, we have lost some of the readability of our script. The numbers $1 and $2 are not very meaningful in themselves! Let's rewrite the script to assign the arguments to meaningful variable names: #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > variations.${TARGET_WORD}.txt ./word-contexts.py ${FILENAME} variations.${TARGET_WORD}.txt > contexts.${TARGET_WORD}.txt Why curly brackets? You'll notice above that we started using curly brackets around our variables. While you technically don't need them ( $TARGET_WORD would also be fine), using them makes the name of the variable (compared to other text) completely clear. This is especially useful when combining variables with underscores, and in the case of specifying the numerical arguments ( $1 , $2 , etc.) the curly braces allow you to go above a value of 9 without error. There is one final place where we could optimize this script. The output filename variations.${TARGET_WORD}.txt is used in multiple places in the script. If we decide we want to change the naming convention of the output file, we'd have to remember to edit all the locations where that name is used, which is asking for typos. Instead, we should use a variable for referring to the output filename, so that we only ever need to change one spot in the file. We can do a similar thing for the second output file. After making these changes, the script will look like this: #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" VARIATIONS_OUTPUT=\"variations.${TARGET_WORD}.txt\" CONTEXTS_OUTPUT=\"contexts.${TARGET_WORD}.txt\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > ${VARIATIONS_OUTPUT} ./word-contexts.py ${FILENAME} ${VARIATIONS_OUTPUT} > ${CONTEXTS_OUTPUT} You may consider this optimization to be optimal and that's fine. But for longer scripts, being able to change \"customizable\" values in a single place helps prevent inconsistencies when making edits. Furthermore, this structure makes it easier to add additional arguments to the script. For example, if we wanted to always define custom names for the output files we could change the definitions at the top of script to read in the third and fourth arguments as the value, i.e., #!/bin/bash FILENAME=\"${1}\" TARGET_WORD=\"${2}\" VARIATIONS_OUTPUT=\"${3}\" CONTEXTS_OUTPUT=\"${4}\" ./word-variations.py ${FILENAME} ${TARGET_WORD} > ${VARIATIONS_OUTPUT} ./word-contexts.py ${FILENAME} ${VARIATIONS_OUTPUT} > ${CONTEXTS_OUTPUT}","title":"Incorporate Arguments"},{"location":"materials/software/part2-ex1-build-executable/#apply-to-your-work","text":"Are you using a scripting language where you could add a header to your main script? If so, what should it be? What items in your main code or commands are changing? Do you need to add arguments to your code? How could you use variables in your script to make it easier to pass arguments and make changes to your code?","title":"Apply to Your Work"},{"location":"materials/software/part3-ex1-apptainer-recipes/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 3.1: Create an Apptainer Definition File \u00b6 Objective : Describe each major section of an Apptainer Definition file. Why learn this? : When building your own containers, it is helpful to understand the basic options and syntax of the \"build\" or definition file. Section Bootstrap/From %files %post %env Where to start \u00b6 Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 A custom container always is always built on an existing container. It is common to use a container on Docker Hub, or in this case, hub.opensciencegrid.org. These lines tell Apptainer to pull the pre-existing image from the hub, and to use it as the base for the container that will be built using this definition file. When choosing a base container, try to find one that has most of what you need - for example, if you want to install R packages, try to find a container that already has R installed. Files needed for building or running \u00b6 %files source_code.tar.gz /opt install.R If you need specific files for the installation (like source code) or for the job to execute (like small data files or scripts), they can be copied into the container under the %files section. The first item on a line is what to copy (from your computer) and the optional second item is where it should be copied in the container. Normally the files being copied are in your local working directory where you run the build command. Commands to install \u00b6 %post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev install2.r tidyverse This is where most of the installation happens. You can use any shell command here that will work in the base container to install software. These commands might include: - Linux installation tools like apt or yum - Scripting specific installers like pip , conda or install.packages() - Shell commands like tar , configure , make Different distributions of Linux often have distinct sets of tools for installing software. The installers for various common Linux distributions are listed below: Ubuntu: apt or apt-get Debian: deb CentOS: yum A web search for \u201cinstall X on Y Linux\u201d is usually a good start for common software installation tasks. 1 When installing to a custom location, do not install to a home directory. This is likely to get overwritten when the container is run. Instead, /opt is the best directory for custom installations. Environment \u00b6 %environment PATH=/opt/mycode/bin:$PATH JAVA_HOME=/opt/java-1.8 To set environment variables (especially useful for software in a custom location), use the %environment section of the definition file. Apply to Your Work \u00b6 What is a good container to use as the base (specified in the From: line) for your container? Does your software need certain Linux libraries in order to work? Are there any environment variables that your software is expecting to use? If so, should you define them in the %environment section, or should you define them for each job? This text and previous list taken from Introduction to Docker \u21a9","title":"3.1 - Create an Apptainer Definition Files"},{"location":"materials/software/part3-ex1-apptainer-recipes/#software-exercise-31-create-an-apptainer-definition-file","text":"Objective : Describe each major section of an Apptainer Definition file. Why learn this? : When building your own containers, it is helpful to understand the basic options and syntax of the \"build\" or definition file. Section Bootstrap/From %files %post %env","title":"Software Exercise 3.1: Create an Apptainer Definition File"},{"location":"materials/software/part3-ex1-apptainer-recipes/#where-to-start","text":"Bootstrap: docker From: hub.opensciencegrid.org/htc/ubuntu:22.04 A custom container always is always built on an existing container. It is common to use a container on Docker Hub, or in this case, hub.opensciencegrid.org. These lines tell Apptainer to pull the pre-existing image from the hub, and to use it as the base for the container that will be built using this definition file. When choosing a base container, try to find one that has most of what you need - for example, if you want to install R packages, try to find a container that already has R installed.","title":"Where to start"},{"location":"materials/software/part3-ex1-apptainer-recipes/#files-needed-for-building-or-running","text":"%files source_code.tar.gz /opt install.R If you need specific files for the installation (like source code) or for the job to execute (like small data files or scripts), they can be copied into the container under the %files section. The first item on a line is what to copy (from your computer) and the optional second item is where it should be copied in the container. Normally the files being copied are in your local working directory where you run the build command.","title":"Files needed for building or running"},{"location":"materials/software/part3-ex1-apptainer-recipes/#commands-to-install","text":"%post apt-get update -y apt-get install -y \\ build-essential \\ cmake \\ g++ \\ r-base-dev install2.r tidyverse This is where most of the installation happens. You can use any shell command here that will work in the base container to install software. These commands might include: - Linux installation tools like apt or yum - Scripting specific installers like pip , conda or install.packages() - Shell commands like tar , configure , make Different distributions of Linux often have distinct sets of tools for installing software. The installers for various common Linux distributions are listed below: Ubuntu: apt or apt-get Debian: deb CentOS: yum A web search for \u201cinstall X on Y Linux\u201d is usually a good start for common software installation tasks. 1 When installing to a custom location, do not install to a home directory. This is likely to get overwritten when the container is run. Instead, /opt is the best directory for custom installations.","title":"Commands to install"},{"location":"materials/software/part3-ex1-apptainer-recipes/#environment","text":"%environment PATH=/opt/mycode/bin:$PATH JAVA_HOME=/opt/java-1.8 To set environment variables (especially useful for software in a custom location), use the %environment section of the definition file.","title":"Environment"},{"location":"materials/software/part3-ex1-apptainer-recipes/#apply-to-your-work","text":"What is a good container to use as the base (specified in the From: line) for your container? Does your software need certain Linux libraries in order to work? Are there any environment variables that your software is expecting to use? If so, should you define them in the %environment section, or should you define them for each job? This text and previous list taken from Introduction to Docker \u21a9","title":"Apply to Your Work"},{"location":"materials/software/part3-ex2-docker-build/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 3.2: Build Your Own Docker Container (Optional) \u00b6 Objective : Build a custom Docker container with numpy and use it in a job Why learn this? : Docker containers can be run on both your laptop and OSPool. DockerHub also provides a convenient platform for sharing containers. If you want to use a custom container, run across platforms, and/or share a container amongst a group, building in Docker first is a good approach. Python Script \u00b6 For this example, create a script called rand_array.py on the Access Point. import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library. This exercise will walk you through the steps to build your own Docker container based on Python, with the numpy Python library added on. Getting Set Up \u00b6 Before building your own Docker container, you need to go through the following set up steps: Install Docker Dekstop on your computer. Docker Desktop page You may need to create a Docker Hub user name to download Docker Desktop; if not created at that step, create a user name for Docker Hub now. (Optional): Once Docker is up and running on your computer, you are welcome to take some time to explore the basics of downloading and running a container, as shown in the initial sections of this Docker lesson: Introduction to Docker However, this isn't strictly necessary for building your own container. Building a Container \u00b6 In order to make our container reproducible, we will be using Docker's capability to build a container image from a specification file. First, create an empty build directory on your computer , not the Access Points. In the build directory, create a file called Dockerfile (no file extension!) with the following contents: # Start with this image as a \"base\". # It's as if all the commands that created that image were inserted here. # Always use a specific tag like \"4.10.3\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda3:4.10.3 # Use RUN to execute commands inside the image as it is being built up. RUN conda install --yes numpy # RUN multiple commands together. # Try to always \"clean up\" after yourself to reduce the final size of your image. RUN apt-get update \\ && apt-get --yes install --no-install-recommends graphviz \\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is our specification file and provides Docker with the information it needs to build our new container. There are other options besides FROM and RUN ; see the Docker documentation for more information. Note that our container is starting from an existing container continuumio/miniconda3:4.10.3 . This container is produced by the continuumio organization; the number 4.10.3 indicates the container version. When we create our new container, we will want to use a similar naming scheme of: USERNAME/CONTAINER:VERSIONTAG In what follows, you will want to replace USERNAME with your DockerHub user name. The CONTAINER name and VERSIONTAG are your choice; in what follows, we will use py3-numpy as the container name and 2024-08 as the version tag. To build and name the new container, open a command line window on your computer where you can run Docker commands. Use the cd command to change your working directory to the build directory with the Dockerfile inside. $ docker build -t USERNAME/py3-numpy:2024-08 . Note the . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside. Upload Container and Submit Job \u00b6 Right now the container image only exists on your computer. To use it in CHTC or elsewhere, it needs to be added to a public registry like Docker Hub. Anyone can access a public Docker container! Make sure that you do include any secret or privileged information in your container. To put your container image in Docker Hub, use the docker push command on the command line: $ docker push USERNAME/py3-numpy:2024-08 If the push doesn't work, you may need to run docker login first, enter your Docker Hub username and password and then try the push again. Once your container image is in DockerHub, you can use it in jobs as described in Exercise 1.3 . Apply to Your Work \u00b6 What is a good container to use as the base (specified in the FROM line) for your container? Does your software need certain Linux libraries in order to work? Thanks to Josh Karpel for providing the original sample Dockerfile !","title":"3.2 - Build Your Own Docker Container"},{"location":"materials/software/part3-ex2-docker-build/#software-exercise-32-build-your-own-docker-container-optional","text":"Objective : Build a custom Docker container with numpy and use it in a job Why learn this? : Docker containers can be run on both your laptop and OSPool. DockerHub also provides a convenient platform for sharing containers. If you want to use a custom container, run across platforms, and/or share a container amongst a group, building in Docker first is a good approach.","title":"Software Exercise 3.2: Build Your Own Docker Container (Optional)"},{"location":"materials/software/part3-ex2-docker-build/#python-script","text":"For this example, create a script called rand_array.py on the Access Point. import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library. This exercise will walk you through the steps to build your own Docker container based on Python, with the numpy Python library added on.","title":"Python Script"},{"location":"materials/software/part3-ex2-docker-build/#getting-set-up","text":"Before building your own Docker container, you need to go through the following set up steps: Install Docker Dekstop on your computer. Docker Desktop page You may need to create a Docker Hub user name to download Docker Desktop; if not created at that step, create a user name for Docker Hub now. (Optional): Once Docker is up and running on your computer, you are welcome to take some time to explore the basics of downloading and running a container, as shown in the initial sections of this Docker lesson: Introduction to Docker However, this isn't strictly necessary for building your own container.","title":"Getting Set Up"},{"location":"materials/software/part3-ex2-docker-build/#building-a-container","text":"In order to make our container reproducible, we will be using Docker's capability to build a container image from a specification file. First, create an empty build directory on your computer , not the Access Points. In the build directory, create a file called Dockerfile (no file extension!) with the following contents: # Start with this image as a \"base\". # It's as if all the commands that created that image were inserted here. # Always use a specific tag like \"4.10.3\", never \"latest\"! # The version referenced by \"latest\" can change, so the build will be # more stable when building from a specific version tag. FROM continuumio/miniconda3:4.10.3 # Use RUN to execute commands inside the image as it is being built up. RUN conda install --yes numpy # RUN multiple commands together. # Try to always \"clean up\" after yourself to reduce the final size of your image. RUN apt-get update \\ && apt-get --yes install --no-install-recommends graphviz \\ && apt-get --yes clean \\ && rm -rf /var/lib/apt/lists/* This is our specification file and provides Docker with the information it needs to build our new container. There are other options besides FROM and RUN ; see the Docker documentation for more information. Note that our container is starting from an existing container continuumio/miniconda3:4.10.3 . This container is produced by the continuumio organization; the number 4.10.3 indicates the container version. When we create our new container, we will want to use a similar naming scheme of: USERNAME/CONTAINER:VERSIONTAG In what follows, you will want to replace USERNAME with your DockerHub user name. The CONTAINER name and VERSIONTAG are your choice; in what follows, we will use py3-numpy as the container name and 2024-08 as the version tag. To build and name the new container, open a command line window on your computer where you can run Docker commands. Use the cd command to change your working directory to the build directory with the Dockerfile inside. $ docker build -t USERNAME/py3-numpy:2024-08 . Note the . at the end of the command! This indicates that we're using the current directory as our build environment, including the Dockerfile inside.","title":"Building a Container"},{"location":"materials/software/part3-ex2-docker-build/#upload-container-and-submit-job","text":"Right now the container image only exists on your computer. To use it in CHTC or elsewhere, it needs to be added to a public registry like Docker Hub. Anyone can access a public Docker container! Make sure that you do include any secret or privileged information in your container. To put your container image in Docker Hub, use the docker push command on the command line: $ docker push USERNAME/py3-numpy:2024-08 If the push doesn't work, you may need to run docker login first, enter your Docker Hub username and password and then try the push again. Once your container image is in DockerHub, you can use it in jobs as described in Exercise 1.3 .","title":"Upload Container and Submit Job"},{"location":"materials/software/part3-ex2-docker-build/#apply-to-your-work","text":"What is a good container to use as the base (specified in the FROM line) for your container? Does your software need certain Linux libraries in order to work? Thanks to Josh Karpel for providing the original sample Dockerfile !","title":"Apply to Your Work"},{"location":"materials/software/part4-ex1-download/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 4.1: Using a Pre-compiled Binary \u00b6 Objective : Identify software that can be downloaded; download it and use it to run a job. Why learn this? : Some software doesn't require much \"installation\" - you can just download it and run. Recognizing when this is possible can save you time. Our Software Example \u00b6 The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Note that the URL for this webpage starts with blast.ncbi.nlm.nih.gov . Check your sources! You should always be careful downloading software from the internet! Only download software from a trusted source and be on the lookout for forfeits. Feel free to ask a facilitator if you are not sure if you can trust a software source. Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code . This will either open a page in a web browser that looks like this: Or you will be asked to open the link in your file browser (choose the Connect as Guest option): In either case, you should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries. Before proceeding, look at the list of downloads and try to determine which one you want. Based on our operating system, we want to use the Linux binary, which is labelled with the x64-linux suffix. Right click on the filename and then click \"Copy link address\". All the other links are either for source code or other operating systems. What about the version number? While the image shows files for version 2.13.0 of BLAST, the version you'll see will almost certainly be newer. You can ignore the version number for the purpose of this exercise. On the Access Point, create a directory for this exercise. Then download the appropriate tar.gz file and un-tar/decompress it it. If you want to do this all from the command line, the sequence will look like this (using wget as the download command.) Use the link you copied from the previous step. user@login $ wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-linux.tar.gz user@login $ tar -xzf ncbi-blast-2.16.0+-x64-linux.tar.gz We're going to be using the blastx binary in our job. Where is it in the directory you just decompressed? Copy the Input Files \u00b6 To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: username@login $ osdf object get /ospool/uc-shared/public/school/2025/pdbaa.tar.gz ./ username@login $ osdf object get /ospool/uc-shared/public/school/2025/mouse.fa ./ Untar the pdbaa database: username@login $ tar -xzf pdbaa.tar.gz Submitting the Job \u00b6 We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. The form of a typical blastx command looks something like this: blastx -db <database_dir/prefix> -query <input_file> -out <output_file> Copy a submit file from one of the Day 1 exercises or previous software exercises to use for this exercise. Think about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? Do you have any idea how much memory or disk to request? Try to answer these questions and modify your submit file appropriately. Once you have done all you can, check your submit file against the lines below, which contain the exact components to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST directory. We need to use the arguments line in the submit file to express the rest of the command. executable = ncbi-blast-2.15.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . transfer_input_files = pdbaa, mouse.fa Let's assume that we've run this program before, and we know that 1GB of disk and 1GB of memory will be MORE than enough (the 'log' file will tell us how accurate we are, after the job runs): request_memory = 1GB request_disk = 1GB Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt . Apply to Your Work \u00b6 Is the software you want to use available as a standalone binary, like blastx ? How could you find out, and where could you get the necessary file? How could you adapt a submit file to use a binary executable file?","title":"4.1 - Download and Use Compiled Software"},{"location":"materials/software/part4-ex1-download/#software-exercise-41-using-a-pre-compiled-binary","text":"Objective : Identify software that can be downloaded; download it and use it to run a job. Why learn this? : Some software doesn't require much \"installation\" - you can just download it and run. Recognizing when this is possible can save you time.","title":"Software Exercise 4.1: Using a Pre-compiled Binary"},{"location":"materials/software/part4-ex1-download/#our-software-example","text":"The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Note that the URL for this webpage starts with blast.ncbi.nlm.nih.gov . Check your sources! You should always be careful downloading software from the internet! Only download software from a trusted source and be on the lookout for forfeits. Feel free to ask a facilitator if you are not sure if you can trust a software source. Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code . This will either open a page in a web browser that looks like this: Or you will be asked to open the link in your file browser (choose the Connect as Guest option): In either case, you should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use one of the pre-built binaries. Before proceeding, look at the list of downloads and try to determine which one you want. Based on our operating system, we want to use the Linux binary, which is labelled with the x64-linux suffix. Right click on the filename and then click \"Copy link address\". All the other links are either for source code or other operating systems. What about the version number? While the image shows files for version 2.13.0 of BLAST, the version you'll see will almost certainly be newer. You can ignore the version number for the purpose of this exercise. On the Access Point, create a directory for this exercise. Then download the appropriate tar.gz file and un-tar/decompress it it. If you want to do this all from the command line, the sequence will look like this (using wget as the download command.) Use the link you copied from the previous step. user@login $ wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-linux.tar.gz user@login $ tar -xzf ncbi-blast-2.16.0+-x64-linux.tar.gz We're going to be using the blastx binary in our job. Where is it in the directory you just decompressed?","title":"Our Software Example"},{"location":"materials/software/part4-ex1-download/#copy-the-input-files","text":"To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: username@login $ osdf object get /ospool/uc-shared/public/school/2025/pdbaa.tar.gz ./ username@login $ osdf object get /ospool/uc-shared/public/school/2025/mouse.fa ./ Untar the pdbaa database: username@login $ tar -xzf pdbaa.tar.gz","title":"Copy the Input Files"},{"location":"materials/software/part4-ex1-download/#submitting-the-job","text":"We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. The form of a typical blastx command looks something like this: blastx -db <database_dir/prefix> -query <input_file> -out <output_file> Copy a submit file from one of the Day 1 exercises or previous software exercises to use for this exercise. Think about which lines you will need to change or add to your submit file in order to submit the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? Do you have any idea how much memory or disk to request? Try to answer these questions and modify your submit file appropriately. Once you have done all you can, check your submit file against the lines below, which contain the exact components to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST directory. We need to use the arguments line in the submit file to express the rest of the command. executable = ncbi-blast-2.15.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . transfer_input_files = pdbaa, mouse.fa Let's assume that we've run this program before, and we know that 1GB of disk and 1GB of memory will be MORE than enough (the 'log' file will tell us how accurate we are, after the job runs): request_memory = 1GB request_disk = 1GB Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt .","title":"Submitting the Job"},{"location":"materials/software/part4-ex1-download/#apply-to-your-work","text":"Is the software you want to use available as a standalone binary, like blastx ? How could you find out, and where could you get the necessary file? How could you adapt a submit file to use a binary executable file?","title":"Apply to Your Work"},{"location":"materials/software/part4-ex2-wrapper/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 4.2: Writing a Wrapper Script \u00b6 Objective : Run downloaded software files via an intermediate, \"wrapper\" script. Why learn this? : This change is a good test of your general HTCondor knowledge and how to translate between executable and submit file. Using wrapper scripts is also a common practice for managing what happens in a job. Background \u00b6 Wrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps. A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the OSPool. For this exercise, we will write a wrapper script as an alternate way to run the same job as the previous exercise. Wrapper Script, part 1 \u00b6 Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise, make a file called run_blast.sh . The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below: #!/bin/bash ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt Submit File Changes \u00b6 We now need to make some changes to our submit file. Make a copy of your previous submit file and open it to edit. Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name of our wrapper script and comment out the arguments line. executable = run_blast.sh #arguments = Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of transferring just that program, we will transfer the original downloaded tar.gz file. To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: transfer_input_files = pdbaa.tar.gz, mouse.fa, ncbi-blast-2.15.0+-x64-linux.tar.gz If you really want to be on top of things, look at the log file for the last exercise, and update your memory and disk requests to be just slightly above the actual \"Usage\" values in the log. Before submitting, make sure to make the below additional changes to the wrapper script! Wrapper Script, part 2 \u00b6 Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the previous exercise if you're not sure what these commands looks like. In order to distinguish this job from our previous job, change the output file name to something besides results.txt . The completed script run_blast.sh should look like this: #!/bin/bash tar -xzf ncbi-blast-2.15.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: username@login $ chmod u+x run_blast.sh Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q . Apply to Your Work \u00b6 How could you use a wrapper script to execute multiple commands in one job? Would it be easier to manage the files for your job if you used compressed files like .tar.gz? What other tasks could you perform inside of the job as part of the wrapper script?","title":"4.2 - Use a Wrapper Script To Run Software"},{"location":"materials/software/part4-ex2-wrapper/#software-exercise-42-writing-a-wrapper-script","text":"Objective : Run downloaded software files via an intermediate, \"wrapper\" script. Why learn this? : This change is a good test of your general HTCondor knowledge and how to translate between executable and submit file. Using wrapper scripts is also a common practice for managing what happens in a job.","title":"Software Exercise 4.2: Writing a Wrapper Script"},{"location":"materials/software/part4-ex2-wrapper/#background","text":"Wrapper scripts are a useful tool for running software that can't be compiled into one piece, needs to be installed with every job, or just for running extra steps. A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the OSPool. For this exercise, we will write a wrapper script as an alternate way to run the same job as the previous exercise.","title":"Background"},{"location":"materials/software/part4-ex2-wrapper/#wrapper-script-part-1","text":"Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise, make a file called run_blast.sh . The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the script? Once you have an idea, check against the example below: #!/bin/bash ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt","title":"Wrapper Script, part 1"},{"location":"materials/software/part4-ex2-wrapper/#submit-file-changes","text":"We now need to make some changes to our submit file. Make a copy of your previous submit file and open it to edit. Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name of our wrapper script and comment out the arguments line. executable = run_blast.sh #arguments = Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of transferring just that program, we will transfer the original downloaded tar.gz file. To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: transfer_input_files = pdbaa.tar.gz, mouse.fa, ncbi-blast-2.15.0+-x64-linux.tar.gz If you really want to be on top of things, look at the log file for the last exercise, and update your memory and disk requests to be just slightly above the actual \"Usage\" values in the log. Before submitting, make sure to make the below additional changes to the wrapper script!","title":"Submit File Changes"},{"location":"materials/software/part4-ex2-wrapper/#wrapper-script-part-2","text":"Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the previous exercise if you're not sure what these commands looks like. In order to distinguish this job from our previous job, change the output file name to something besides results.txt . The completed script run_blast.sh should look like this: #!/bin/bash tar -xzf ncbi-blast-2.15.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: username@login $ chmod u+x run_blast.sh Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q .","title":"Wrapper Script, part 2"},{"location":"materials/software/part4-ex2-wrapper/#apply-to-your-work","text":"How could you use a wrapper script to execute multiple commands in one job? Would it be easier to manage the files for your job if you used compressed files like .tar.gz? What other tasks could you perform inside of the job as part of the wrapper script?","title":"Apply to Your Work"},{"location":"materials/software/part4-ex3-arguments/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 4.3: Passing Arguments Through the Wrapper Script \u00b6 Objective : Add arguments to a wrapper script to make it more flexible and modular Why learn this? : Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line, but in our world of job submission, the arguments will be listed in the submit file, in the arguments line. Identifying Potential Arguments \u00b6 In the same directory as the last exercise, make sure you're in the directory with your BLAST job submission. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the comparison database, the input file, and the output file. Modifying Files \u00b6 We are going to add three arguments to the wrapper script, controlling the database, input and output file. Make a copy of your last submit file and open it for editing. Add an arguments line, or uncomment the one that exists, and add the three input values mentioned above. The arguments line in your submit file should look like this: arguments = pdbaa mouse.fa results3.txt (We're using results3.txt ) to distinguish between the previous two runs.) For bash (the language of our current wrapper script), the variables $1 , $2 and $3 represent the first, second, and third arguments, respectively. Thus, in the main command of the script, replace the various names with these variables: ./ncbi-blast-2.15.0+/bin/blastx -db $1 / $1 -query $2 -out $3 If your wrapper script is in a different language, you should use that language's syntax for reading in variables from the command line. Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor. It is now easy to change the inputs for the job; we can write them into the arguments line of the submit file and they will be propagated to the command in the wrapper script. We can even turn the submit file arguments into their own variables when submitting multiple jobs at once. Readability with Variables \u00b6 One of the downsides of this approach, is that our command has become harder to read. The original script contains all the information at a glance: ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt But our new version is more cryptic -- what is $1 ?: ./ncbi-blast-2.15.0+/bin/blastx -db $1 -query $2 -out $3 One way to overcome this is to create our own variable names inside the wrapper script and assign the argument values to them. Here is an example for our BLAST script: #!/bin/bash DATABASE = $1 INFILE = $2 OUTFILE = $3 tar -xzf ncbi-blast-2.15.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ./ncbi-blast-2.15.0+/bin/blastx -db $DATABASE / $DATABASE -query $INFILE -out $OUTFILE Here, we are assigning the input arguments ( $1 , $2 and $3 ) to new variable names, and then using those names ( $DATABASE , $INFILE , and $OUTFILE ) in the command, which is easier to read. Edit your script to match the above syntax. Submit your jobs with condor_submit . When the job finishes, look at the job's standard output file to see how the variables printed. See also For more practice in writing scripts like the run_blast.sh script, see the earlier exercise: 2.1 Build an HTC-Friendly Executable . Apply to Your Work \u00b6 How can you use variables in your script to make the code more readable? What parts of your script would benefit from using a variable?","title":"4.3 - Using Arguments With Wrapper Scripts"},{"location":"materials/software/part4-ex3-arguments/#software-exercise-43-passing-arguments-through-the-wrapper-script","text":"Objective : Add arguments to a wrapper script to make it more flexible and modular Why learn this? : Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line, but in our world of job submission, the arguments will be listed in the submit file, in the arguments line.","title":"Software Exercise 4.3: Passing Arguments Through the Wrapper Script"},{"location":"materials/software/part4-ex3-arguments/#identifying-potential-arguments","text":"In the same directory as the last exercise, make sure you're in the directory with your BLAST job submission. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the comparison database, the input file, and the output file.","title":"Identifying Potential Arguments"},{"location":"materials/software/part4-ex3-arguments/#modifying-files","text":"We are going to add three arguments to the wrapper script, controlling the database, input and output file. Make a copy of your last submit file and open it for editing. Add an arguments line, or uncomment the one that exists, and add the three input values mentioned above. The arguments line in your submit file should look like this: arguments = pdbaa mouse.fa results3.txt (We're using results3.txt ) to distinguish between the previous two runs.) For bash (the language of our current wrapper script), the variables $1 , $2 and $3 represent the first, second, and third arguments, respectively. Thus, in the main command of the script, replace the various names with these variables: ./ncbi-blast-2.15.0+/bin/blastx -db $1 / $1 -query $2 -out $3 If your wrapper script is in a different language, you should use that language's syntax for reading in variables from the command line. Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor. It is now easy to change the inputs for the job; we can write them into the arguments line of the submit file and they will be propagated to the command in the wrapper script. We can even turn the submit file arguments into their own variables when submitting multiple jobs at once.","title":"Modifying Files"},{"location":"materials/software/part4-ex3-arguments/#readability-with-variables","text":"One of the downsides of this approach, is that our command has become harder to read. The original script contains all the information at a glance: ./ncbi-blast-2.15.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt But our new version is more cryptic -- what is $1 ?: ./ncbi-blast-2.15.0+/bin/blastx -db $1 -query $2 -out $3 One way to overcome this is to create our own variable names inside the wrapper script and assign the argument values to them. Here is an example for our BLAST script: #!/bin/bash DATABASE = $1 INFILE = $2 OUTFILE = $3 tar -xzf ncbi-blast-2.15.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ./ncbi-blast-2.15.0+/bin/blastx -db $DATABASE / $DATABASE -query $INFILE -out $OUTFILE Here, we are assigning the input arguments ( $1 , $2 and $3 ) to new variable names, and then using those names ( $DATABASE , $INFILE , and $OUTFILE ) in the command, which is easier to read. Edit your script to match the above syntax. Submit your jobs with condor_submit . When the job finishes, look at the job's standard output file to see how the variables printed. See also For more practice in writing scripts like the run_blast.sh script, see the earlier exercise: 2.1 Build an HTC-Friendly Executable .","title":"Readability with Variables"},{"location":"materials/software/part4-ex3-arguments/#apply-to-your-work","text":"How can you use variables in your script to make the code more readable? What parts of your script would benefit from using a variable?","title":"Apply to Your Work"},{"location":"materials/software/part5-ex1-prepackaged/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 5.1: Pre-package a Research Code \u00b6 Objective : Install software (HMMER) to a folder and run it in a job using a wrapper script. Why learn this? : If not using a container, this is a template for how to create a portable software installation using your own files, especially if the software is not available already compiled for Linux. Our Software Example \u00b6 For this exercise, we will be using the bioinformatics package HMMER. HMMER is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Create a directory for this exercise on the Access Point. Do an internet search to find the HMMER software downloads page and the installation instructions page. On the installation page, there are short instructions for how to install HMMER. There are two options shown for installation -- which should we use? For the purposes of this example, we are going to use the instructions under the \"Current version\" heading, with the \"Source\" link. Download the HMMER source using wget. Go back to the installation documentation page and look at the steps for compiling from source. This process should be similar to what was described in the lecture! Installation \u00b6 Normally, it is better to install software on a dedicated \"build\" server, but for this example, we are going to compile directly on the Access Point Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. username@host $ mkdir hmmer-build Now run the commands to unpack the source code: username@host $ tar -zxf hmmer-3.4.tar.gz username@host $ cd hmmer-3.4 Now we can follow the second set of installation instructions. For the prefix, we'll use the variable $PWD to capture the name of our current working directory and then a relative path to the hmmer-build directory we created in step 1: username@host $ ./configure --prefix = $PWD /../hmmer-build username@host $ make username@host $ make install Go back to the previous working directory : username@host $ cd .. and confirm that our installation procedure created bin , lib , and share directories in the hmmer-build folder: username@host $ ls hmmer-build bin share Now we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball. username@host $ tar -czf hmmer-build.tar.gz hmmer-build Note that we now have two tarballs in our directory -- the source tarball ( hmmer.tar.gz ), which we will no longer need and our newly built installation ( hmmer-build.tar.gz ) which is what we will actually be using to run jobs. Wrapper Script \u00b6 Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in a previous exercise . These steps should be performed back on the submit server ( ap1.facility.path-cc.io ). Create a script called run_hmmer.sh . The script will first need to untar our installation, so the script should start out like this: #!/bin/bash tar -xzf hmmer-build.tar.gz We're going to use the same $PWD trick from the installation in order to tell the computer how to find HMMER. We will do this by setting the PATH environment variable, to include the directory where HMMER is installed: export PATH = $PWD /hmmer-build/bin: $PATH Finally, the wrapper script needs to not only setup HMMER, but actually run the program. Add the following lines to your run_hmmer.sh wrapper script. hmmbuild globins4.hmm globins4.sto hmmsearch -o search-results.txt globins4.hmm globins45.fa Make sure the wrapper script has executable permissions: username@login $ chmod u+x run_hmmer.sh Run a HMMER job \u00b6 We're almost ready! We need two more pieces to run a HMMER job. We're going to use some of the tutorial files provided with the HMMER download to run the job. You already have these files back in the directory where you unpacked the source code: username@login $ ls hmmer-3.4/tutorial 7LESS_DROME fn3.hmm globins45.fa globins4.sto MADE1.hmm Pkinase.hmm dna_target.fa fn3.sto globins4.hmm HBB_HUMAN MADE1.sto Pkinase.sto If you don't see these files, you may want to redownload the hmmer.tar.gz file and untar it here. Our last step is to create a submit file for our HMMER job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from a previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: executable = run_hmmer.sh transfer_input_files = hmmer-build.tar.gz, hmmer-3.4/tutorial/ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . Note The / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. Submit the job with condor_submit . Once the job completes, it should produce a search-results.txt file. Note For a very similar compiling example, see this guide on how to compile samtools : Example Software Compilation Apply to Your Work \u00b6 What are the compilation instructions for your software? (If they aren't written down anywhere, now is a good time!) How portable is your software? Can you compile it on one machine and run it on another?","title":"5.1 - Compiling a Research Software"},{"location":"materials/software/part5-ex1-prepackaged/#software-exercise-51-pre-package-a-research-code","text":"Objective : Install software (HMMER) to a folder and run it in a job using a wrapper script. Why learn this? : If not using a container, this is a template for how to create a portable software installation using your own files, especially if the software is not available already compiled for Linux.","title":"Software Exercise 5.1: Pre-package a Research Code"},{"location":"materials/software/part5-ex1-prepackaged/#our-software-example","text":"For this exercise, we will be using the bioinformatics package HMMER. HMMER is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Create a directory for this exercise on the Access Point. Do an internet search to find the HMMER software downloads page and the installation instructions page. On the installation page, there are short instructions for how to install HMMER. There are two options shown for installation -- which should we use? For the purposes of this example, we are going to use the instructions under the \"Current version\" heading, with the \"Source\" link. Download the HMMER source using wget. Go back to the installation documentation page and look at the steps for compiling from source. This process should be similar to what was described in the lecture!","title":"Our Software Example"},{"location":"materials/software/part5-ex1-prepackaged/#installation","text":"Normally, it is better to install software on a dedicated \"build\" server, but for this example, we are going to compile directly on the Access Point Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. username@host $ mkdir hmmer-build Now run the commands to unpack the source code: username@host $ tar -zxf hmmer-3.4.tar.gz username@host $ cd hmmer-3.4 Now we can follow the second set of installation instructions. For the prefix, we'll use the variable $PWD to capture the name of our current working directory and then a relative path to the hmmer-build directory we created in step 1: username@host $ ./configure --prefix = $PWD /../hmmer-build username@host $ make username@host $ make install Go back to the previous working directory : username@host $ cd .. and confirm that our installation procedure created bin , lib , and share directories in the hmmer-build folder: username@host $ ls hmmer-build bin share Now we want to package up our installation, so we can use it in other jobs. We can do this by compressing any necessary directories into a single gzipped tarball. username@host $ tar -czf hmmer-build.tar.gz hmmer-build Note that we now have two tarballs in our directory -- the source tarball ( hmmer.tar.gz ), which we will no longer need and our newly built installation ( hmmer-build.tar.gz ) which is what we will actually be using to run jobs.","title":"Installation"},{"location":"materials/software/part5-ex1-prepackaged/#wrapper-script","text":"Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in a previous exercise . These steps should be performed back on the submit server ( ap1.facility.path-cc.io ). Create a script called run_hmmer.sh . The script will first need to untar our installation, so the script should start out like this: #!/bin/bash tar -xzf hmmer-build.tar.gz We're going to use the same $PWD trick from the installation in order to tell the computer how to find HMMER. We will do this by setting the PATH environment variable, to include the directory where HMMER is installed: export PATH = $PWD /hmmer-build/bin: $PATH Finally, the wrapper script needs to not only setup HMMER, but actually run the program. Add the following lines to your run_hmmer.sh wrapper script. hmmbuild globins4.hmm globins4.sto hmmsearch -o search-results.txt globins4.hmm globins45.fa Make sure the wrapper script has executable permissions: username@login $ chmod u+x run_hmmer.sh","title":"Wrapper Script"},{"location":"materials/software/part5-ex1-prepackaged/#run-a-hmmer-job","text":"We're almost ready! We need two more pieces to run a HMMER job. We're going to use some of the tutorial files provided with the HMMER download to run the job. You already have these files back in the directory where you unpacked the source code: username@login $ ls hmmer-3.4/tutorial 7LESS_DROME fn3.hmm globins45.fa globins4.sto MADE1.hmm Pkinase.hmm dna_target.fa fn3.sto globins4.hmm HBB_HUMAN MADE1.sto Pkinase.sto If you don't see these files, you may want to redownload the hmmer.tar.gz file and untar it here. Our last step is to create a submit file for our HMMER job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from a previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: executable = run_hmmer.sh transfer_input_files = hmmer-build.tar.gz, hmmer-3.4/tutorial/ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . Note The / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. Submit the job with condor_submit . Once the job completes, it should produce a search-results.txt file. Note For a very similar compiling example, see this guide on how to compile samtools : Example Software Compilation","title":"Run a HMMER job"},{"location":"materials/software/part5-ex1-prepackaged/#apply-to-your-work","text":"What are the compilation instructions for your software? (If they aren't written down anywhere, now is a good time!) How portable is your software? Can you compile it on one machine and run it on another?","title":"Apply to Your Work"},{"location":"materials/software/part5-ex2-python/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 5.2: Using Python, Pre-Built \u00b6 In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes. Background \u00b6 Objective : Install software (Python) to a folder and run it in a job using a wrapper script. Why learn this? : This is very similar to the previous exercise . Just need Python? If you just need Python and don't want to go through the process below, we recommend that you build a container that already has Python installed. For more information on containers, see this previous exercise . For instructions on building a Python container, see this OSPool guide . Pre-Building \u00b6 The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on the Access Point and cd into it. Download the Python source code from https://www.python.org/ . username@login $ wget https://www.python.org/ftp/python/3.10.5/Python-3.10.5.tgz First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball ( tar -xzf Python-3.10.5.tgz ) and look at the README.rst file in the Python-3.10.5 directory ( cd Python-3.10.5 ). You'll want to look for the \"Build Instructions\" header. What will the main installation steps be? What command is required for the final installation? Once you've tried to answer these questions, move to the next step. There are some basic installation instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: ./configure make make test sudo make install This three-stage process (configure, make, make install) is a common way to install many software packages. The default installation location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory so that we can compress that installation directory into a tarball. You can often use an option called -prefix with the configure script to change the default installation directory. Let's see if the Python configure script has this option by using the \"help\" option (as suggested in the README.rst file): username@host $ ./configure --help Sure enough, there's a list of all the different options that can be passed to the configure script, which includes --prefix . (To see the --prefix option, you may need to scroll towards the top of the output.) Therefore, we can use the $PWD command in order to set the path correctly to a custom installation directory. Now let's actually install Python! From the original working directory , create a directory to hold the installation. username@host $ cd ../ username@host $ mkdir python310 Move into the Python-3.10.5 directory and run the installation commands. These may take a few minutes each. username@host $ cd Python-3.10.5 username@host $ ./configure --prefix = $PWD /../python310 username@host $ make username@host $ make install Note The installation instructions in the README.rst file have a make test step between the make and make install steps. As this step isn't strictly necessary (and takes a long time), it's been omitted above. If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. username@host $ cd .. username@host $ ls python310/ bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! username@host $ tar -czf prebuilt_python.tar.gz python310/ We might want to know how we installed Python for later reference. Enter the following commands to save our history to a file: username@host $ history > python_install.txt Python Script \u00b6 Create a script with the following lines called fib.py . import sys import os if len ( sys . argv ) != 2 : print ( 'Usage: %s MAXIMUM' % ( os . path . basename ( sys . argv [ 0 ]))) sys . exit ( 1 ) maximum = int ( sys . argv [ 1 ]) n1 = n2 = 1 while n2 <= maximum : n1 , n2 = n2 , n1 + n2 print ( 'The greatest Fibonacci number up to %d is %d ' % ( maximum , n1 )) What command line arguments does this script take? Try running it on the submit server. Wrapper Script \u00b6 We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: #!/bin/bash tar -xzf prebuilt_python.tar.gz python310/bin/python3 fib.py 90 or #!/bin/bash tar -xzf prebuilt_python.tar.gz export PATH = $( pwd ) /python310/bin: $PATH python3 fib.py 90 Make sure your run_fib.sh script is executable. Submit File \u00b6 Make a copy of a previous submit file in your local directory (the submit file from the Use a Wrapper Script exercise might be a good candidate). What changes need to be made to run this Python job? Modify your submit file, then make sure you've included the key lines below: executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Submit the job using condor_submit . Check the .out file to see if the job completed. Apply to Your Work \u00b6 Do you need to compile Python with special options? What version of Python and what Python packages do you need?","title":"5.2 - Compiling Python and Running Jobs"},{"location":"materials/software/part5-ex2-python/#software-exercise-52-using-python-pre-built","text":"In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes.","title":"Software Exercise 5.2: Using Python, Pre-Built"},{"location":"materials/software/part5-ex2-python/#background","text":"Objective : Install software (Python) to a folder and run it in a job using a wrapper script. Why learn this? : This is very similar to the previous exercise . Just need Python? If you just need Python and don't want to go through the process below, we recommend that you build a container that already has Python installed. For more information on containers, see this previous exercise . For instructions on building a Python container, see this OSPool guide .","title":"Background"},{"location":"materials/software/part5-ex2-python/#pre-building","text":"The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on the Access Point and cd into it. Download the Python source code from https://www.python.org/ . username@login $ wget https://www.python.org/ftp/python/3.10.5/Python-3.10.5.tgz First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball ( tar -xzf Python-3.10.5.tgz ) and look at the README.rst file in the Python-3.10.5 directory ( cd Python-3.10.5 ). You'll want to look for the \"Build Instructions\" header. What will the main installation steps be? What command is required for the final installation? Once you've tried to answer these questions, move to the next step. There are some basic installation instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: ./configure make make test sudo make install This three-stage process (configure, make, make install) is a common way to install many software packages. The default installation location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory so that we can compress that installation directory into a tarball. You can often use an option called -prefix with the configure script to change the default installation directory. Let's see if the Python configure script has this option by using the \"help\" option (as suggested in the README.rst file): username@host $ ./configure --help Sure enough, there's a list of all the different options that can be passed to the configure script, which includes --prefix . (To see the --prefix option, you may need to scroll towards the top of the output.) Therefore, we can use the $PWD command in order to set the path correctly to a custom installation directory. Now let's actually install Python! From the original working directory , create a directory to hold the installation. username@host $ cd ../ username@host $ mkdir python310 Move into the Python-3.10.5 directory and run the installation commands. These may take a few minutes each. username@host $ cd Python-3.10.5 username@host $ ./configure --prefix = $PWD /../python310 username@host $ make username@host $ make install Note The installation instructions in the README.rst file have a make test step between the make and make install steps. As this step isn't strictly necessary (and takes a long time), it's been omitted above. If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. username@host $ cd .. username@host $ ls python310/ bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! username@host $ tar -czf prebuilt_python.tar.gz python310/ We might want to know how we installed Python for later reference. Enter the following commands to save our history to a file: username@host $ history > python_install.txt","title":"Pre-Building"},{"location":"materials/software/part5-ex2-python/#python-script","text":"Create a script with the following lines called fib.py . import sys import os if len ( sys . argv ) != 2 : print ( 'Usage: %s MAXIMUM' % ( os . path . basename ( sys . argv [ 0 ]))) sys . exit ( 1 ) maximum = int ( sys . argv [ 1 ]) n1 = n2 = 1 while n2 <= maximum : n1 , n2 = n2 , n1 + n2 print ( 'The greatest Fibonacci number up to %d is %d ' % ( maximum , n1 )) What command line arguments does this script take? Try running it on the submit server.","title":"Python Script"},{"location":"materials/software/part5-ex2-python/#wrapper-script","text":"We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: #!/bin/bash tar -xzf prebuilt_python.tar.gz python310/bin/python3 fib.py 90 or #!/bin/bash tar -xzf prebuilt_python.tar.gz export PATH = $( pwd ) /python310/bin: $PATH python3 fib.py 90 Make sure your run_fib.sh script is executable.","title":"Wrapper Script"},{"location":"materials/software/part5-ex2-python/#submit-file","text":"Make a copy of a previous submit file in your local directory (the submit file from the Use a Wrapper Script exercise might be a good candidate). What changes need to be made to run this Python job? Modify your submit file, then make sure you've included the key lines below: executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Submit the job using condor_submit . Check the .out file to see if the job completed.","title":"Submit File"},{"location":"materials/software/part5-ex2-python/#apply-to-your-work","text":"Do you need to compile Python with special options? What version of Python and what Python packages do you need?","title":"Apply to Your Work"},{"location":"materials/software/part5-ex3-conda/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Software Exercise 5.3: Using Conda Environments \u00b6 Objective : Create a portable conda environment and use it in a job. Why learn this? : If you normally use conda to manage your Python environments, this method of software portability offers great similarity to your usual practices. Introduction \u00b6 Many Python users manage their Python installation and environments with either the Anaconda or miniconda distributions. These distribution tools are great for creating portable Python installations and can be used on HTC systems with some help from a tool called conda pack . Sample Script \u00b6 For this example, create a script called rand_array.py on the Access Point. import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library. Create and Pack a Conda Environment \u00b6 (For a generic version of these instructions, see the CHTC User Guide ) Our first step is to create a miniconda installation on the submit server. You should be logged into whichever server you made the rand_array.py script on. Download the latest Linux miniconda installer user@login $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Run the installer to install miniconda; you'll need to accept the license terms and you can use the default installation location: [user@login]$ sh Miniconda3-latest-Linux-x86_64.sh At the end, you can choose whether or not to \"initialize Miniconda3 by running conda init?\" The default is no; you would then run the eval command listed by the installer to \"activate\" Miniconda. If you choose \"no\" you'll want to save this command so that you can reactivate the Miniconda installation when needed in the future. Next we'll create our conda \"environment\" with numpy (we've called the environment \"py3-numpy\"): (base) [user@login]$ conda create -n py3-numpy (base) [user@login]$ conda activate py3-numpy (py3-numpy) [user@login]$ conda install -c conda-forge numpy Once everything is installed, deactivate the environment to go back to the Miniconda \"base\" environment. (py3-numpy) [user@login]$ conda deactivate We'll now install a tool that will pack up the just created conda environment so we can run it elsewhere. Make sure that your job's Miniconda environment is created, but deactivated, so that you're in the \"base\" Miniconda environment, then run: (base) [user@login]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, we will run the conda pack command, which will automatically create a tar.gz file with our environment: (base) [user@login]$ conda pack -n py3-numpy Submit a Job \u00b6 The executable for this job will need to be a wrapper script. What steps do you think need to be included? Write down a rough draft, then compare with the following script. Create a wrapper script like the following: #!/bin/bash set -e export PATH mkdir py3-numpy tar -xzf py3-numpy.tar.gz -C py3-numpy . py3-numpy/bin/activate python3 rand_array.py What needs to be included in your submit file for the job to run successfully? Try yourself and then check the suggestions in the next point. In your submit file, make sure to have the following: Your executable should be the the bash script you created in the previous step. Remember to transfer your Python script and the environment tar.gz file via transfer_input_files . Submit the job and see what happens! When conda pack fails... As useful as the conda pack utility is, it does not work with all Conda or Python packages. If your job fails with an obscure error, you may have encountered such a situation. For more information on containers, see this previous exercise . For instructions on building a Conda container, see this OSPool guide . Apply to Your Work \u00b6 Do you use Conda for installing your software? What are the packages you need? Do you have an environment.yaml file that defines your Conda environment? How could you incorporate that in the above instructions?","title":"5.3 - Using Conda Environments"},{"location":"materials/software/part5-ex3-conda/#software-exercise-53-using-conda-environments","text":"Objective : Create a portable conda environment and use it in a job. Why learn this? : If you normally use conda to manage your Python environments, this method of software portability offers great similarity to your usual practices.","title":"Software Exercise 5.3: Using Conda Environments"},{"location":"materials/software/part5-ex3-conda/#introduction","text":"Many Python users manage their Python installation and environments with either the Anaconda or miniconda distributions. These distribution tools are great for creating portable Python installations and can be used on HTC systems with some help from a tool called conda pack .","title":"Introduction"},{"location":"materials/software/part5-ex3-conda/#sample-script","text":"For this example, create a script called rand_array.py on the Access Point. import numpy as np #numpy array with random values a = np.random.rand(4,2,3) print(a) To run this script, we will need a copy of Python with the numpy library.","title":"Sample Script"},{"location":"materials/software/part5-ex3-conda/#create-and-pack-a-conda-environment","text":"(For a generic version of these instructions, see the CHTC User Guide ) Our first step is to create a miniconda installation on the submit server. You should be logged into whichever server you made the rand_array.py script on. Download the latest Linux miniconda installer user@login $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Run the installer to install miniconda; you'll need to accept the license terms and you can use the default installation location: [user@login]$ sh Miniconda3-latest-Linux-x86_64.sh At the end, you can choose whether or not to \"initialize Miniconda3 by running conda init?\" The default is no; you would then run the eval command listed by the installer to \"activate\" Miniconda. If you choose \"no\" you'll want to save this command so that you can reactivate the Miniconda installation when needed in the future. Next we'll create our conda \"environment\" with numpy (we've called the environment \"py3-numpy\"): (base) [user@login]$ conda create -n py3-numpy (base) [user@login]$ conda activate py3-numpy (py3-numpy) [user@login]$ conda install -c conda-forge numpy Once everything is installed, deactivate the environment to go back to the Miniconda \"base\" environment. (py3-numpy) [user@login]$ conda deactivate We'll now install a tool that will pack up the just created conda environment so we can run it elsewhere. Make sure that your job's Miniconda environment is created, but deactivated, so that you're in the \"base\" Miniconda environment, then run: (base) [user@login]$ conda install -c conda-forge conda-pack Enter y when it asks you to install. Finally, we will run the conda pack command, which will automatically create a tar.gz file with our environment: (base) [user@login]$ conda pack -n py3-numpy","title":"Create and Pack a Conda Environment"},{"location":"materials/software/part5-ex3-conda/#submit-a-job","text":"The executable for this job will need to be a wrapper script. What steps do you think need to be included? Write down a rough draft, then compare with the following script. Create a wrapper script like the following: #!/bin/bash set -e export PATH mkdir py3-numpy tar -xzf py3-numpy.tar.gz -C py3-numpy . py3-numpy/bin/activate python3 rand_array.py What needs to be included in your submit file for the job to run successfully? Try yourself and then check the suggestions in the next point. In your submit file, make sure to have the following: Your executable should be the the bash script you created in the previous step. Remember to transfer your Python script and the environment tar.gz file via transfer_input_files . Submit the job and see what happens! When conda pack fails... As useful as the conda pack utility is, it does not work with all Conda or Python packages. If your job fails with an obscure error, you may have encountered such a situation. For more information on containers, see this previous exercise . For instructions on building a Conda container, see this OSPool guide .","title":"Submit a Job"},{"location":"materials/software/part5-ex3-conda/#apply-to-your-work","text":"Do you use Conda for installing your software? What are the packages you need? Do you have an environment.yaml file that defines your Conda environment? How could you incorporate that in the above instructions?","title":"Apply to Your Work"},{"location":"materials/software/part5-ex4-compiling/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Software Exercise 5.4: Compile Statically Linked Code \u00b6 Objective : Compile code using static linking, explain why this can be useful. Why learn this? : When code is compiled, it is usually linked to other pieces of code on the computer. This can cause it to not work when moved to other computers. Static linking means that all the needed references are included in the compiled code, meaning that it can run almost anywhere. Our Software Example \u00b6 For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the Access Point. Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . #include \"stdio.h\" long factorial ( int ); int main () { int i , n , c ; n = 7 ; for ( i = 0 ; i < n ; i ++ ){ for ( c = 0 ; c <= ( n - i - 2 ); c ++ ) printf ( \" \" ); for ( c = 0 ; c <= i ; c ++ ) printf ( \"%ld \" , factorial ( i ) / ( factorial ( c ) * factorial ( i - c ))); printf ( \" \\n \" ); } return 0 ; } long factorial ( int n ) { int c ; long result = 1 ; for ( c = 1 ; c <= n ; c ++ ) result = result * c ; return result ; } Compiling \u00b6 In order to use this code in a job, we will first need to statically compile the code. Most linux servers (including our Access Point) have the gcc (GNU compiler collection) installed, so we already have a compiler on the Access Point. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the Access Point Compile the code, using the command: username@login $ gcc -static pascal.c -o pascal Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. Verify that the compiled binary was statically linked: username@login $ file pascal The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: username@host $ file pascal pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.18, not stripped The output clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files) Submit the Job \u00b6 Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? Based on what you thought about in 1., find a submit file from earlier that you can modify to run our compiled pascal code. Copy it to the directory with the pascal binary and make those changes. Submit the job using condor_submit . Once the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the .out file created by the job. Apply to Your Work \u00b6 Do you write and compile your own code like in this exercise? How portable is your compiled code? Do you have any assumptions about what libraries are available at runtime?","title":"5.4 - Compiling and Running a Simple Code"},{"location":"materials/software/part5-ex4-compiling/#software-exercise-54-compile-statically-linked-code","text":"Objective : Compile code using static linking, explain why this can be useful. Why learn this? : When code is compiled, it is usually linked to other pieces of code on the computer. This can cause it to not work when moved to other computers. Static linking means that all the needed references are included in the compiled code, meaning that it can run almost anywhere.","title":"Software Exercise 5.4: Compile Statically Linked Code"},{"location":"materials/software/part5-ex4-compiling/#our-software-example","text":"For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the Access Point. Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . #include \"stdio.h\" long factorial ( int ); int main () { int i , n , c ; n = 7 ; for ( i = 0 ; i < n ; i ++ ){ for ( c = 0 ; c <= ( n - i - 2 ); c ++ ) printf ( \" \" ); for ( c = 0 ; c <= i ; c ++ ) printf ( \"%ld \" , factorial ( i ) / ( factorial ( c ) * factorial ( i - c ))); printf ( \" \\n \" ); } return 0 ; } long factorial ( int n ) { int c ; long result = 1 ; for ( c = 1 ; c <= n ; c ++ ) result = result * c ; return result ; }","title":"Our Software Example"},{"location":"materials/software/part5-ex4-compiling/#compiling","text":"In order to use this code in a job, we will first need to statically compile the code. Most linux servers (including our Access Point) have the gcc (GNU compiler collection) installed, so we already have a compiler on the Access Point. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the Access Point Compile the code, using the command: username@login $ gcc -static pascal.c -o pascal Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. Verify that the compiled binary was statically linked: username@login $ file pascal The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: username@host $ file pascal pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.18, not stripped The output clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files)","title":"Compiling"},{"location":"materials/software/part5-ex4-compiling/#submit-the-job","text":"Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? Based on what you thought about in 1., find a submit file from earlier that you can modify to run our compiled pascal code. Copy it to the directory with the pascal binary and make those changes. Submit the job using condor_submit . Once the job has run and left the queue, you should be able to see the results (seven rows of Pascal's triangle) in the .out file created by the job.","title":"Submit the Job"},{"location":"materials/software/part5-ex4-compiling/#apply-to-your-work","text":"Do you write and compile your own code like in this exercise? How portable is your compiled code? Do you have any assumptions about what libraries are available at runtime?","title":"Apply to Your Work"},{"location":"materials/special/part1-ex1-gpus/","text":"Exercise 1.1: GPUs \u00b6 Exploring Availability \u00b6 For this exercise, we will use the ap40.uw.osg-htc.org access point. Log in: $ ssh <USERNAME>@ap40.uw.osg-htc.org Let's first explore what GPUs are available in the OSPool. Remember that the pool is dynamic - resources are beeing added and removed all the time - but we can at least find out what the current set of GPUs are there. Run: user@ap40 $ condor_status -const 'GPUs > 0' Once you have that list, pick one of the resources and look at the classad using the -l flag. For example: user@ap40 $ condor_status -l [ MACHINE ] Using the -autoformat flag, explore the different attributes of the GPUs. Some interesting attributes might be GPUs_DeviceName , GPUs_Capability , GLIDEIN_Site and GLIDEIN_ResourceName . Compare the Mips number of a GPU slot with a regular slot. Does the Mips number indicate that GPUs can be much faster than CPUs? Why/why not? A sample GPU job \u00b6 Create a file named mytf.py and chmod it to be executable. The content is a sample TensorFlow code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/python3 # http://learningtensorflow.com/lesson10/ import sys import numpy as np import tensorflow as tf from datetime import datetime tf . debugging . set_log_device_placement ( True ) # Create some tensors a = tf . constant ([[ 1.0 , 2.0 , 3.0 ], [ 4.0 , 5.0 , 6.0 ]]) b = tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ], [ 5.0 , 6.0 ]]) c = tf . matmul ( a , b ) print ( c ) Then, create a submit file to run the code on a GPU, using a TensorFlow container image. The new bits of the submit file is provided below, but you will have to fill in the rest from what you have learnt earlier in the User School. universe = container container_image = /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 executable = mytf.py request_gpus = 1 Note that TensorFlow also require the AVX2 CPU extensions. Remember that AVX2 is available in the x86_64-v3 and x86_64-v4 micro architectures. Add a requirements line stating that Microarch has to be one of those two (the operand for or in the classad experssions is || ) Submit the job and watch the queue. Did the job start running as quickly as when we ran CPU jobs? Why/why not? Examine the out/err files. Does it indicate somewhere that the job was mapped to a GPU? (Hint: search for Created TensorFlow device ) Keep a copy of the out/err. Modify the submit file to not run on a GPU, and the try the job again. Did the job work? Does the err from the CPU job look anything like the GPU err?","title":"1.1 - GPUs"},{"location":"materials/special/part1-ex1-gpus/#exercise-11-gpus","text":"","title":"Exercise 1.1: GPUs"},{"location":"materials/special/part1-ex1-gpus/#exploring-availability","text":"For this exercise, we will use the ap40.uw.osg-htc.org access point. Log in: $ ssh <USERNAME>@ap40.uw.osg-htc.org Let's first explore what GPUs are available in the OSPool. Remember that the pool is dynamic - resources are beeing added and removed all the time - but we can at least find out what the current set of GPUs are there. Run: user@ap40 $ condor_status -const 'GPUs > 0' Once you have that list, pick one of the resources and look at the classad using the -l flag. For example: user@ap40 $ condor_status -l [ MACHINE ] Using the -autoformat flag, explore the different attributes of the GPUs. Some interesting attributes might be GPUs_DeviceName , GPUs_Capability , GLIDEIN_Site and GLIDEIN_ResourceName . Compare the Mips number of a GPU slot with a regular slot. Does the Mips number indicate that GPUs can be much faster than CPUs? Why/why not?","title":"Exploring Availability"},{"location":"materials/special/part1-ex1-gpus/#a-sample-gpu-job","text":"Create a file named mytf.py and chmod it to be executable. The content is a sample TensorFlow code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/usr/bin/python3 # http://learningtensorflow.com/lesson10/ import sys import numpy as np import tensorflow as tf from datetime import datetime tf . debugging . set_log_device_placement ( True ) # Create some tensors a = tf . constant ([[ 1.0 , 2.0 , 3.0 ], [ 4.0 , 5.0 , 6.0 ]]) b = tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ], [ 5.0 , 6.0 ]]) c = tf . matmul ( a , b ) print ( c ) Then, create a submit file to run the code on a GPU, using a TensorFlow container image. The new bits of the submit file is provided below, but you will have to fill in the rest from what you have learnt earlier in the User School. universe = container container_image = /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:2.2-cuda-10.1 executable = mytf.py request_gpus = 1 Note that TensorFlow also require the AVX2 CPU extensions. Remember that AVX2 is available in the x86_64-v3 and x86_64-v4 micro architectures. Add a requirements line stating that Microarch has to be one of those two (the operand for or in the classad experssions is || ) Submit the job and watch the queue. Did the job start running as quickly as when we ran CPU jobs? Why/why not? Examine the out/err files. Does it indicate somewhere that the job was mapped to a GPU? (Hint: search for Created TensorFlow device ) Keep a copy of the out/err. Modify the submit file to not run on a GPU, and the try the job again. Did the job work? Does the err from the CPU job look anything like the GPU err?","title":"A sample GPU job"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/","text":"OSG Exercise 2.1: Troubleshooting Jobs \u00b6 The goal of this exercise is to practice troubleshooting some common problems that you may encounter when submitting jobs using HTCondor. Note: This exercise is a little harder than some others. To complete it, you will have to find and fix several issues. Be patient, keep trying, but if you really get stuck, you can ask for help or look at the very bottom of this page for a link to answers. But try not to look at the answers! Acquiring the Materials \u00b6 We have prepared some Python code, data, and submit files for this exercise: Log into an Access Point (ap40.uw.osg-htc.org) Download a tarball of the materials: user@server $ cp /ospool/ap40/osg-staff/osgus25/troubleshooting.tar.gz . Extract the tarball using the commands that you learned earlier Change into the newly extracted directory and explore its contents \u2014 resist the temptation to fix things right away! Solving a Project Euler Problem \u00b6 The contents of the tarball contain a series of submit files, Python scripts, and an input file that are designed to solve Project Euler problem 98 : By replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 = 36^2. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square number: 9216 = 96^2. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes are not permitted, neither may a different letter have the same digital value as another letter. Using p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square anagram word pairs (a palindromic word is NOT considered to be an anagram of itself). What is the largest square number formed by any member of such a pair? NOTE: All anagrams formed must be contained in the given text file. Unfortunately, there are many issues with the submit files that you will have to work through before you can obtain the solution to the problem! The code in the Python scripts themselves is, in theory, free of bugs. Finding anagrams \u00b6 The first step in our workflow takes an input file with a list of words ( p098_words.txt ) and extracts all of the anagrams using the find_anagrams.py script. Naturally, we want to run this as an HTCondor job, so: Submit the accompanying find-anagrams.sub file from the tarball. Resolve any issues that you encounter until the job returns pairs of anagrams as its output. Once you have satisfactory output, move onto the next section. Please be polite Access points are shared resources, so you should clean up after yourself. If you discover any jobs in the Hold state, and after you are done troubleshooting them, remove them with the following command: user@server $ condor_rm -const 'JobStatus =?= 5' <JOB FILTER> Where replacing <JOB FILTER> with... Will remove... Your username (e.g. blin ) All of your held jobs A cluster ID (e.g. 74078 ) All held jobs matching the given cluster ID A job ID (e.g. 97932.30 ) That specific held job Finding the largest square \u00b6 The next step in the workflow uses the max_square.py script to find the largest square number, if any, for a given anagram word pair. Let's submit jobs that run max_square.py for all of the anagram word pairs (i.e., one job per word pair) that you found in the previous section: Submit the accompanying squares.sub file from the tarball Resolve any issues that you encounter until you receive output for each job. Note that some jobs may have empty output since not all anagram word pairs are square anagram word pairs. Next, you can find the largest square among your output by directly using the command line. For example, if all of your job output has been placed in the squares directory and are named square-1.out , square-2.out , etc., then you could run the following command to find the largest square: user@server $ cat squares/square-*.out | sort -n | tail -n 1 You can check if you have the right answer with any of the OSG staff or by submitting the answer to Project Euler (requires an account). Answer Key \u00b6 There is also a working solution that can be retrieved with user@server $ osdf object get /ospool/uc-shared/public/school/2025/troubleshooting-key.tar.gz . It contains comments labeled SOLUTION that you can consult in case you get stuck. Like any answer key, it is mainly useful as a verification tool, so try to only use it as a last resort or for detailed explanations to improve your understanding.","title":"1.1 - Troubleshooting Jobs"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#osg-exercise-21-troubleshooting-jobs","text":"The goal of this exercise is to practice troubleshooting some common problems that you may encounter when submitting jobs using HTCondor. Note: This exercise is a little harder than some others. To complete it, you will have to find and fix several issues. Be patient, keep trying, but if you really get stuck, you can ask for help or look at the very bottom of this page for a link to answers. But try not to look at the answers!","title":"OSG Exercise 2.1: Troubleshooting Jobs"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#acquiring-the-materials","text":"We have prepared some Python code, data, and submit files for this exercise: Log into an Access Point (ap40.uw.osg-htc.org) Download a tarball of the materials: user@server $ cp /ospool/ap40/osg-staff/osgus25/troubleshooting.tar.gz . Extract the tarball using the commands that you learned earlier Change into the newly extracted directory and explore its contents \u2014 resist the temptation to fix things right away!","title":"Acquiring the Materials"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#solving-a-project-euler-problem","text":"The contents of the tarball contain a series of submit files, Python scripts, and an input file that are designed to solve Project Euler problem 98 : By replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 = 36^2. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square number: 9216 = 96^2. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes are not permitted, neither may a different letter have the same digital value as another letter. Using p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square anagram word pairs (a palindromic word is NOT considered to be an anagram of itself). What is the largest square number formed by any member of such a pair? NOTE: All anagrams formed must be contained in the given text file. Unfortunately, there are many issues with the submit files that you will have to work through before you can obtain the solution to the problem! The code in the Python scripts themselves is, in theory, free of bugs.","title":"Solving a Project Euler Problem"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#finding-anagrams","text":"The first step in our workflow takes an input file with a list of words ( p098_words.txt ) and extracts all of the anagrams using the find_anagrams.py script. Naturally, we want to run this as an HTCondor job, so: Submit the accompanying find-anagrams.sub file from the tarball. Resolve any issues that you encounter until the job returns pairs of anagrams as its output. Once you have satisfactory output, move onto the next section. Please be polite Access points are shared resources, so you should clean up after yourself. If you discover any jobs in the Hold state, and after you are done troubleshooting them, remove them with the following command: user@server $ condor_rm -const 'JobStatus =?= 5' <JOB FILTER> Where replacing <JOB FILTER> with... Will remove... Your username (e.g. blin ) All of your held jobs A cluster ID (e.g. 74078 ) All held jobs matching the given cluster ID A job ID (e.g. 97932.30 ) That specific held job","title":"Finding anagrams"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#finding-the-largest-square","text":"The next step in the workflow uses the max_square.py script to find the largest square number, if any, for a given anagram word pair. Let's submit jobs that run max_square.py for all of the anagram word pairs (i.e., one job per word pair) that you found in the previous section: Submit the accompanying squares.sub file from the tarball Resolve any issues that you encounter until you receive output for each job. Note that some jobs may have empty output since not all anagram word pairs are square anagram word pairs. Next, you can find the largest square among your output by directly using the command line. For example, if all of your job output has been placed in the squares directory and are named square-1.out , square-2.out , etc., then you could run the following command to find the largest square: user@server $ cat squares/square-*.out | sort -n | tail -n 1 You can check if you have the right answer with any of the OSG staff or by submitting the answer to Project Euler (requires an account).","title":"Finding the largest square"},{"location":"materials/troubleshooting/part1-ex1-troubleshooting/#answer-key","text":"There is also a working solution that can be retrieved with user@server $ osdf object get /ospool/uc-shared/public/school/2025/troubleshooting-key.tar.gz . It contains comments labeled SOLUTION that you can consult in case you get stuck. Like any answer key, it is mainly useful as a verification tool, so try to only use it as a last resort or for detailed explanations to improve your understanding.","title":"Answer Key"},{"location":"materials/troubleshooting/part1-ex2-job-retry/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Exercise 1.2: Retries \u00b6 The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises. Bad Job \u00b6 Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a Python script that fails once in a while. We will not fix it, but instead use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python3 # murphy.py simulates a real program with real problems import random import sys import time # For one out of every three attempts, simulate a runtime error if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py . Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk. Submit the file, note the ClusterId, and wait for the jobs to finish. What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: user@server $ condor_history <CLUSTER> -af:h ProcId ExitCode (Be sure to replace <cluster> with your actual cluster ID. The command may take a minute or so to complete.) How many of the jobs succeeded? How many failed? Retrying Failed Jobs \u00b6 Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. HTcondor has a feature named max_retries that allows to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Try implementing this feature. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode ; and what about the ExitCode from earlier execution attempts? A (Too) Long Running Job \u00b6 Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python3 # murphy.py simulate a real program with real problems import random import sys import time # For one out of every three attempts, simulate an \"infinite\" loop if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output time . sleep ( 3600 ) sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. Save the script to a new file named murphy2.py . Copy your previous submit file to a new name and change the executable to murphy2.py . If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file periodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 ) Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think JobStatus == 2 corresponds to? Bonus Exercise \u00b6 If you have time, edit your submit file so that instead of removing long running jobs, HTCondor will automatically put the long-running job on hold, and then automatically release it.","title":"1.2 - Job Retry"},{"location":"materials/troubleshooting/part1-ex2-job-retry/#exercise-12-retries","text":"The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises.","title":"Exercise 1.2: Retries"},{"location":"materials/troubleshooting/part1-ex2-job-retry/#bad-job","text":"Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a Python script that fails once in a while. We will not fix it, but instead use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python3 # murphy.py simulates a real program with real problems import random import sys import time # For one out of every three attempts, simulate a runtime error if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py . Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk. Submit the file, note the ClusterId, and wait for the jobs to finish. What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: user@server $ condor_history <CLUSTER> -af:h ProcId ExitCode (Be sure to replace <cluster> with your actual cluster ID. The command may take a minute or so to complete.) How many of the jobs succeeded? How many failed?","title":"Bad Job"},{"location":"materials/troubleshooting/part1-ex2-job-retry/#retrying-failed-jobs","text":"Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. HTcondor has a feature named max_retries that allows to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Try implementing this feature. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode ; and what about the ExitCode from earlier execution attempts?","title":"Retrying Failed Jobs"},{"location":"materials/troubleshooting/part1-ex2-job-retry/#a-too-long-running-job","text":"Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code, and it may just need to be re-run (or run on a different execute server) to complete without getting stuck. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python3 # murphy.py simulate a real program with real problems import random import sys import time # For one out of every three attempts, simulate an \"infinite\" loop if random . randint ( 0 , 2 ) == 0 : # Intentionally don't print any output time . sleep ( 3600 ) sys . exit ( 15 ) else : time . sleep ( 3 ) print ( \"All work done correctly\" ) # By convention, zero exit code means success sys . exit ( 0 ) Let\u2019s see what happens when a program like this one is run in HTCondor. Save the script to a new file named murphy2.py . Copy your previous submit file to a new name and change the executable to murphy2.py . If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file periodic_remove = (JobStatus == 2) && ( (CurrentTime - EnteredCurrentStatus) > 60 ) Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done? Which job status (i.e. idle, held, running) do you think JobStatus == 2 corresponds to?","title":"A (Too) Long Running Job"},{"location":"materials/troubleshooting/part1-ex2-job-retry/#bonus-exercise","text":"If you have time, edit your submit file so that instead of removing long running jobs, HTCondor will automatically put the long-running job on hold, and then automatically release it.","title":"Bonus Exercise"},{"location":"materials/workflows/part1-ex1-simple-dag/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.1: Coordinating a Set of Jobs With a Simple DAG \u00b6 The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. What is DAGMan? \u00b6 In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the HTCondor manual . Submitting a Simple DAG \u00b6 For our job, we will return briefly to the sleep program, name it job.sub executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . JOB Simple job.sub In your first window, submit the DAG: username@ap40 $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, check the queue (what you see may be slightly different): username@ap40 $ condor_q -nobatch -wide:80 -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended In the third window, watch what DAGMan does (what you see may be slightly different): username@ap40 $ tail -f --lines = 500 simple.dag.dagman.out 08/02/24 15:44:57 ****************************************************** 08/02/24 15:44:57 ** condor_scheduniv_exec.271100.0 (CONDOR_DAGMAN) STARTING UP 08/02/24 15:44:57 ** /usr/bin/condor_dagman 08/02/24 15:44:57 ** SubsystemInfo: name=DAGMAN type=DAGMAN(9) class=CLIENT(2) 08/02/24 15:44:57 ** Configuration: subsystem:DAGMAN local:<NONE> class:CLIENT 08/02/24 15:44:57 ** $CondorVersion: 10.7.0 2024-07-10 BuildID: 659788 PackageID: 10.7.0-0.659788 RC $ 08/02/24 15:44:57 ** $CondorPlatform: x86_64_AlmaLinux8 $ 08/02/24 15:44:57 ** PID = 2340103 08/02/24 15:44:57 ** Log last touched time unavailable (No such file or directory) 08/02/24 15:44:57 ****************************************************** 08/02/24 15:44:57 Daemon Log is logging: D_ALWAYS D_ERROR D_STATUS 08/02/24 15:44:57 DaemonCore: No command port requested. 08/02/24 15:44:57 DAGMAN_USE_STRICT setting: 1 08/02/24 15:44:57 DAGMAN_VERBOSITY setting: 3 08/02/24 15:44:57 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 08/02/24 15:44:57 DAGMAN_DEBUG_CACHE_ENABLE setting: False 08/02/24 15:44:57 DAGMAN_SUBMIT_DELAY setting: 0 08/02/24 15:44:57 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 08/02/24 15:44:57 DAGMAN_STARTUP_CYCLE_DETECT setting: False 08/02/24 15:44:57 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 100 08/02/24 15:44:57 DAGMAN_AGGRESSIVE_SUBMIT setting: False 08/02/24 15:44:57 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 08/02/24 15:44:57 DAGMAN_QUEUE_UPDATE_INTERVAL setting: 300 08/02/24 15:44:57 DAGMAN_DEFAULT_PRIORITY setting: 0 08/02/24 15:44:57 DAGMAN_SUPPRESS_NOTIFICATION setting: True 08/02/24 15:44:57 allow_events (DAGMAN_ALLOW_EVENTS) setting: 114 08/02/24 15:44:57 DAGMAN_RETRY_SUBMIT_FIRST setting: True 08/02/24 15:44:57 DAGMAN_RETRY_NODE_FIRST setting: False 08/02/24 15:44:57 DAGMAN_MAX_JOBS_IDLE setting: 1000 08/02/24 15:44:57 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 08/02/24 15:44:57 DAGMAN_MAX_PRE_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MAX_POST_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MAX_HOLD_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MUNGE_NODE_NAMES setting: True 08/02/24 15:44:57 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 08/02/24 15:44:57 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 08/02/24 15:44:57 DAGMAN_ALWAYS_RUN_POST setting: False 08/02/24 15:44:57 DAGMAN_CONDOR_SUBMIT_EXE setting: /usr/bin/condor_submit 08/02/24 15:44:57 DAGMAN_USE_DIRECT_SUBMIT setting: True 08/02/24 15:44:57 DAGMAN_DEFAULT_APPEND_VARS setting: False 08/02/24 15:44:57 DAGMAN_ABORT_DUPLICATES setting: True 08/02/24 15:44:57 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 08/02/24 15:44:57 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 08/02/24 15:44:57 DAGMAN_AUTO_RESCUE setting: True 08/02/24 15:44:57 DAGMAN_MAX_RESCUE_NUM setting: 100 08/02/24 15:44:57 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 08/02/24 15:44:57 DAGMAN_DEFAULT_NODE_LOG setting: @(DAG_DIR)/@(DAG_FILE).nodes.log 08/02/24 15:44:57 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 08/02/24 15:44:57 DAGMAN_MAX_JOB_HOLDS setting: 100 08/02/24 15:44:57 DAGMAN_HOLD_CLAIM_TIME setting: 20 08/02/24 15:44:57 ALL_DEBUG setting: 08/02/24 15:44:57 DAGMAN_DEBUG setting: 08/02/24 15:44:57 DAGMAN_SUPPRESS_JOB_LOGS setting: False 08/02/24 15:44:57 DAGMAN_REMOVE_NODE_JOBS setting: True 08/02/24 15:44:57 DAGMAN will adjust edges after parsing 08/02/24 15:44:57 argv[0] == \"condor_scheduniv_exec.271100.0\" 08/02/24 15:44:57 argv[1] == \"-Lockfile\" 08/02/24 15:44:57 argv[2] == \"simple.dag.lock\" 08/02/24 15:44:57 argv[3] == \"-AutoRescue\" 08/02/24 15:44:57 argv[4] == \"1\" 08/02/24 15:44:57 argv[5] == \"-DoRescueFrom\" 08/02/24 15:44:57 argv[6] == \"0\" 08/02/24 15:44:57 argv[7] == \"-Dag\" 08/02/24 15:44:57 argv[8] == \"simple.dag\" 08/02/24 15:44:57 argv[9] == \"-Suppress_notification\" 08/02/24 15:44:57 argv[10] == \"-CsdVersion\" 08/02/24 15:44:57 argv[11] == \"$CondorVersion: 10.7.0 2024-07-10 BuildID: 659788 PackageID: 10.7.0-0.659788 RC $\" 08/02/24 15:44:57 argv[12] == \"-Dagman\" 08/02/24 15:44:57 argv[13] == \"/usr/bin/condor_dagman\" 08/02/24 15:44:57 Default node log file is: </home/mats.rynge/dagman-1/./simple.dag.nodes.log> 08/02/24 15:44:57 DAG Lockfile will be written to simple.dag.lock 08/02/24 15:44:57 DAG Input file is simple.dag 08/02/24 15:44:57 Parsing 1 dagfiles 08/02/24 15:44:57 Parsing simple.dag ... 08/02/24 15:44:57 Adjusting edges 08/02/24 15:44:57 Dag contains 1 total jobs 08/02/24 15:44:57 Bootstrapping... 08/02/24 15:44:57 Number of pre-completed nodes: 0 08/02/24 15:44:57 MultiLogFiles: truncating log file /home/mats.rynge/dagman-1/./simple.dag.nodes.log 08/02/24 15:44:57 DAG status: 0 (DAG_STATUS_OK) 08/02/24 15:44:57 Of 1 nodes total: 08/02/24 15:44:57 Done Pre Queued Post Ready Un-Ready Failed Futile 08/02/24 15:44:57 === === === === === === === === 08/02/24 15:44:57 0 0 0 0 1 0 0 0 08/02/24 15:44:57 0 job proc(s) currently held 08/02/24 15:44:57 Registering condor_event_timer... 08/02/24 15:44:58 Submitting HTCondor Node Simple job(s)... Here's where the job is submitted 08/02/24 15:44:58 Submitting HTCondor Node Simple job(s)... 08/02/24 15:44:58 Submitting node Simple from file job.sub using direct job submission 08/02/24 15:44:58 assigned HTCondor ID (271101.0.0) 08/02/24 15:44:58 Just submitted 1 job this cycle... Here's where DAGMan noticed that the job is running 08/02/24 15:45:18 Event: ULOG_EXECUTE for HTCondor Node Simple (271101.0.0) {08/02/24 15:45:14} 08/02/24 15:45:18 Number of idle job procs: 0 Here's where DAGMan noticed that the job finished. 08/02/24 15:45:23 Event: ULOG_JOB_TERMINATED for HTCondor Node Simple (271101.0.0) {08/02/24 15:45:19} 08/02/24 15:45:23 Number of idle job procs: 0 08/02/24 15:45:23 Node Simple job proc (271101.0.0) completed successfully. 08/02/24 15:45:23 Node Simple job completed 08/02/24 15:45:23 DAG status: 0 (DAG_STATUS_OK) 08/02/24 15:45:23 Of 1 nodes total: 08/02/24 15:45:23 Done Pre Queued Post Ready Un-Ready Failed Futile 08/02/24 15:45:23 === === === === === === === === 08/02/24 15:45:23 1 0 0 0 0 0 0 0 Here's where DAGMan noticed that all the work is done. 08/02/24 15:45:23 All jobs Completed! 08/02/24 15:45:23 Note: 0 total job deferrals because of -MaxJobs limit (0) 08/02/24 15:45:23 Note: 0 total job deferrals because of -MaxIdle limit (1000) 08/02/24 15:45:23 Note: 0 total job deferrals because of node category throttles 08/02/24 15:45:23 Note: 0 total PRE script deferrals because of -MaxPre limit (20) or DEFER 08/02/24 15:45:23 Note: 0 total POST script deferrals because of -MaxPost limit (20) or DEFER 08/02/24 15:45:23 Note: 0 total HOLD script deferrals because of -MaxHold limit (20) or DEFER Now verify your results: username@ap40 $ cat simple.log 000 (271101.000.000) 2024-08-02 15:44:58 Job submitted from host: <128.105.68.92:9618?addrs=128.105.68.92-9618+[2607-f388-2200-100-eaeb-d3ff-fe40-111c]-9618&alias=ap40.uw.osg-htc.org&noUDP&sock=schedd_35391_dc5c> DAG Node: Simple ... 040 (271101.000.000) 2024-08-02 15:45:13 Started transferring input files Transferring to host: <10.136.81.233:37425?CCBID=128.104.103.162:9618%3faddrs%3d128.104.103.162-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector4#23067238%20192.170.231.9:9618%3faddrs%3d192.170.231.9-9618+[fd85-ee78-d8a6-8607--1-73b6]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector10#1512850&PrivNet=comp-cc-0463.gwave.ics.psu.edu&addrs=10.136.81.233-37425&alias=comp-cc-0463.gwave.ics.psu.edu&noUDP> ... 040 (271101.000.000) 2024-08-02 15:45:13 Finished transferring input files ... 021 (271101.000.000) 2024-08-02 15:45:14 Warning from starter on slot1_4@glidein_2635188_104012775@comp-cc-0463.gwave.ics.psu.edu: PREPARE_JOB (prepare-hook) succeeded (reported status 000): Using default Singularity image /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 ... 001 (271101.000.000) 2024-08-02 15:45:14 Job executing on host: <10.136.81.233:39645?CCBID=128.104.103.162:9618%3faddrs%3d128.104.103.162-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector10#1506459%20192.170.231.9:9618%3faddrs%3d192.170.231.9-9618+[fd85-ee78-d8a6-8607--1-73b4]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector10#1506644&PrivNet=comp-cc-0463.gwave.ics.psu.edu&addrs=10.136.81.233-39645&alias=comp-cc-0463.gwave.ics.psu.edu&noUDP> SlotName: slot1_4@comp-cc-0463.gwave.ics.psu.edu CondorScratchDir = \"/localscratch/condor/execute/dir_2635172/glide_uZ6qXM/execute/dir_3252113\" Cpus = 1 Disk = 2699079 GLIDEIN_ResourceName = \"PSU-LIGO\" Memory = 1024 ... 006 (271101.000.000) 2024-08-02 15:45:19 Image size of job updated: 2296464 47 - MemoryUsage of job (MB) 47684 - ResidentSetSize of job (KB) ... 040 (271101.000.000) 2024-08-02 15:45:19 Started transferring output files ... 040 (271101.000.000) 2024-08-02 15:45:19 Finished transferring output files ... 005 (271101.000.000) 2024-08-02 15:45:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 38416 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 38416 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 149 1048576 2699079 Memory (MB) : 47 1024 1024 Job terminated of its own accord at 2024-08-02T20:45:19Z with exit-code 0. ... Looking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job). username@ap40 $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out username@ap40 $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = CONDOR_CONFIG,_CONDOR_*,PATH,PYTHONPATH,PERL*,PEGASUS_*,TZ,HOME,USER,LANG,LC_ALL output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId =?= $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal = ? = 11 || ( ExitCode = ! = UNDEFINED && ExitCode > = 0 && ExitCode < = 2 )) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed ( e.g., during a reboot ) . on_exit_remove = (ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-p 0 -f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -Suppress_notification -CsdVersion $CondorVersion:' '10.7.0' '2024-07-10' 'BuildID:' '659788' 'PackageID:' '10.7.0-0.659788' 'RC' '$ -Dagman /usr/bin/condor_dagman\" environment = \"_CONDOR_DAGMAN_LOG=simple.dag.dagman.out _CONDOR_MAX_DAGMAN_LOG=0 _CONDOR_SCHEDD_ADDRESS_FILE=/var/lib/condor/spool/.schedd_address _CONDOR_SCHEDD_DAEMON_AD_FILE=/var/lib/condor/spool/.schedd_classad\" queue If you want to clean up some of these files (you may not want to, at least not yet), run: username@ap40 $ rm simple.dag.* Challenge \u00b6 What is the scheduler universe? Why does DAGMan use it? Show hint HTCondor has several universes What would happen to your DAGMan workflow if the access point has to be rebooted? Jobs in the HTCondor queue are \"managed\" - they are always tracked, and restarted automatically if needed","title":"1.1 - A simple DAG"},{"location":"materials/workflows/part1-ex1-simple-dag/#workflows-exercise-11-coordinating-a-set-of-jobs-with-a-simple-dag","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Workflows Exercise 1.1: Coordinating a Set of Jobs With a Simple DAG"},{"location":"materials/workflows/part1-ex1-simple-dag/#what-is-dagman","text":"In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the HTCondor manual .","title":"What is DAGMan?"},{"location":"materials/workflows/part1-ex1-simple-dag/#submitting-a-simple-dag","text":"For our job, we will return briefly to the sleep program, name it job.sub executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . JOB Simple job.sub In your first window, submit the DAG: username@ap40 $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, check the queue (what you see may be slightly different): username@ap40 $ condor_q -nobatch -wide:80 -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended In the third window, watch what DAGMan does (what you see may be slightly different): username@ap40 $ tail -f --lines = 500 simple.dag.dagman.out 08/02/24 15:44:57 ****************************************************** 08/02/24 15:44:57 ** condor_scheduniv_exec.271100.0 (CONDOR_DAGMAN) STARTING UP 08/02/24 15:44:57 ** /usr/bin/condor_dagman 08/02/24 15:44:57 ** SubsystemInfo: name=DAGMAN type=DAGMAN(9) class=CLIENT(2) 08/02/24 15:44:57 ** Configuration: subsystem:DAGMAN local:<NONE> class:CLIENT 08/02/24 15:44:57 ** $CondorVersion: 10.7.0 2024-07-10 BuildID: 659788 PackageID: 10.7.0-0.659788 RC $ 08/02/24 15:44:57 ** $CondorPlatform: x86_64_AlmaLinux8 $ 08/02/24 15:44:57 ** PID = 2340103 08/02/24 15:44:57 ** Log last touched time unavailable (No such file or directory) 08/02/24 15:44:57 ****************************************************** 08/02/24 15:44:57 Daemon Log is logging: D_ALWAYS D_ERROR D_STATUS 08/02/24 15:44:57 DaemonCore: No command port requested. 08/02/24 15:44:57 DAGMAN_USE_STRICT setting: 1 08/02/24 15:44:57 DAGMAN_VERBOSITY setting: 3 08/02/24 15:44:57 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 08/02/24 15:44:57 DAGMAN_DEBUG_CACHE_ENABLE setting: False 08/02/24 15:44:57 DAGMAN_SUBMIT_DELAY setting: 0 08/02/24 15:44:57 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 08/02/24 15:44:57 DAGMAN_STARTUP_CYCLE_DETECT setting: False 08/02/24 15:44:57 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 100 08/02/24 15:44:57 DAGMAN_AGGRESSIVE_SUBMIT setting: False 08/02/24 15:44:57 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 08/02/24 15:44:57 DAGMAN_QUEUE_UPDATE_INTERVAL setting: 300 08/02/24 15:44:57 DAGMAN_DEFAULT_PRIORITY setting: 0 08/02/24 15:44:57 DAGMAN_SUPPRESS_NOTIFICATION setting: True 08/02/24 15:44:57 allow_events (DAGMAN_ALLOW_EVENTS) setting: 114 08/02/24 15:44:57 DAGMAN_RETRY_SUBMIT_FIRST setting: True 08/02/24 15:44:57 DAGMAN_RETRY_NODE_FIRST setting: False 08/02/24 15:44:57 DAGMAN_MAX_JOBS_IDLE setting: 1000 08/02/24 15:44:57 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 08/02/24 15:44:57 DAGMAN_MAX_PRE_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MAX_POST_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MAX_HOLD_SCRIPTS setting: 20 08/02/24 15:44:57 DAGMAN_MUNGE_NODE_NAMES setting: True 08/02/24 15:44:57 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 08/02/24 15:44:57 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 08/02/24 15:44:57 DAGMAN_ALWAYS_RUN_POST setting: False 08/02/24 15:44:57 DAGMAN_CONDOR_SUBMIT_EXE setting: /usr/bin/condor_submit 08/02/24 15:44:57 DAGMAN_USE_DIRECT_SUBMIT setting: True 08/02/24 15:44:57 DAGMAN_DEFAULT_APPEND_VARS setting: False 08/02/24 15:44:57 DAGMAN_ABORT_DUPLICATES setting: True 08/02/24 15:44:57 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 08/02/24 15:44:57 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 08/02/24 15:44:57 DAGMAN_AUTO_RESCUE setting: True 08/02/24 15:44:57 DAGMAN_MAX_RESCUE_NUM setting: 100 08/02/24 15:44:57 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 08/02/24 15:44:57 DAGMAN_DEFAULT_NODE_LOG setting: @(DAG_DIR)/@(DAG_FILE).nodes.log 08/02/24 15:44:57 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 08/02/24 15:44:57 DAGMAN_MAX_JOB_HOLDS setting: 100 08/02/24 15:44:57 DAGMAN_HOLD_CLAIM_TIME setting: 20 08/02/24 15:44:57 ALL_DEBUG setting: 08/02/24 15:44:57 DAGMAN_DEBUG setting: 08/02/24 15:44:57 DAGMAN_SUPPRESS_JOB_LOGS setting: False 08/02/24 15:44:57 DAGMAN_REMOVE_NODE_JOBS setting: True 08/02/24 15:44:57 DAGMAN will adjust edges after parsing 08/02/24 15:44:57 argv[0] == \"condor_scheduniv_exec.271100.0\" 08/02/24 15:44:57 argv[1] == \"-Lockfile\" 08/02/24 15:44:57 argv[2] == \"simple.dag.lock\" 08/02/24 15:44:57 argv[3] == \"-AutoRescue\" 08/02/24 15:44:57 argv[4] == \"1\" 08/02/24 15:44:57 argv[5] == \"-DoRescueFrom\" 08/02/24 15:44:57 argv[6] == \"0\" 08/02/24 15:44:57 argv[7] == \"-Dag\" 08/02/24 15:44:57 argv[8] == \"simple.dag\" 08/02/24 15:44:57 argv[9] == \"-Suppress_notification\" 08/02/24 15:44:57 argv[10] == \"-CsdVersion\" 08/02/24 15:44:57 argv[11] == \"$CondorVersion: 10.7.0 2024-07-10 BuildID: 659788 PackageID: 10.7.0-0.659788 RC $\" 08/02/24 15:44:57 argv[12] == \"-Dagman\" 08/02/24 15:44:57 argv[13] == \"/usr/bin/condor_dagman\" 08/02/24 15:44:57 Default node log file is: </home/mats.rynge/dagman-1/./simple.dag.nodes.log> 08/02/24 15:44:57 DAG Lockfile will be written to simple.dag.lock 08/02/24 15:44:57 DAG Input file is simple.dag 08/02/24 15:44:57 Parsing 1 dagfiles 08/02/24 15:44:57 Parsing simple.dag ... 08/02/24 15:44:57 Adjusting edges 08/02/24 15:44:57 Dag contains 1 total jobs 08/02/24 15:44:57 Bootstrapping... 08/02/24 15:44:57 Number of pre-completed nodes: 0 08/02/24 15:44:57 MultiLogFiles: truncating log file /home/mats.rynge/dagman-1/./simple.dag.nodes.log 08/02/24 15:44:57 DAG status: 0 (DAG_STATUS_OK) 08/02/24 15:44:57 Of 1 nodes total: 08/02/24 15:44:57 Done Pre Queued Post Ready Un-Ready Failed Futile 08/02/24 15:44:57 === === === === === === === === 08/02/24 15:44:57 0 0 0 0 1 0 0 0 08/02/24 15:44:57 0 job proc(s) currently held 08/02/24 15:44:57 Registering condor_event_timer... 08/02/24 15:44:58 Submitting HTCondor Node Simple job(s)... Here's where the job is submitted 08/02/24 15:44:58 Submitting HTCondor Node Simple job(s)... 08/02/24 15:44:58 Submitting node Simple from file job.sub using direct job submission 08/02/24 15:44:58 assigned HTCondor ID (271101.0.0) 08/02/24 15:44:58 Just submitted 1 job this cycle... Here's where DAGMan noticed that the job is running 08/02/24 15:45:18 Event: ULOG_EXECUTE for HTCondor Node Simple (271101.0.0) {08/02/24 15:45:14} 08/02/24 15:45:18 Number of idle job procs: 0 Here's where DAGMan noticed that the job finished. 08/02/24 15:45:23 Event: ULOG_JOB_TERMINATED for HTCondor Node Simple (271101.0.0) {08/02/24 15:45:19} 08/02/24 15:45:23 Number of idle job procs: 0 08/02/24 15:45:23 Node Simple job proc (271101.0.0) completed successfully. 08/02/24 15:45:23 Node Simple job completed 08/02/24 15:45:23 DAG status: 0 (DAG_STATUS_OK) 08/02/24 15:45:23 Of 1 nodes total: 08/02/24 15:45:23 Done Pre Queued Post Ready Un-Ready Failed Futile 08/02/24 15:45:23 === === === === === === === === 08/02/24 15:45:23 1 0 0 0 0 0 0 0 Here's where DAGMan noticed that all the work is done. 08/02/24 15:45:23 All jobs Completed! 08/02/24 15:45:23 Note: 0 total job deferrals because of -MaxJobs limit (0) 08/02/24 15:45:23 Note: 0 total job deferrals because of -MaxIdle limit (1000) 08/02/24 15:45:23 Note: 0 total job deferrals because of node category throttles 08/02/24 15:45:23 Note: 0 total PRE script deferrals because of -MaxPre limit (20) or DEFER 08/02/24 15:45:23 Note: 0 total POST script deferrals because of -MaxPost limit (20) or DEFER 08/02/24 15:45:23 Note: 0 total HOLD script deferrals because of -MaxHold limit (20) or DEFER Now verify your results: username@ap40 $ cat simple.log 000 (271101.000.000) 2024-08-02 15:44:58 Job submitted from host: <128.105.68.92:9618?addrs=128.105.68.92-9618+[2607-f388-2200-100-eaeb-d3ff-fe40-111c]-9618&alias=ap40.uw.osg-htc.org&noUDP&sock=schedd_35391_dc5c> DAG Node: Simple ... 040 (271101.000.000) 2024-08-02 15:45:13 Started transferring input files Transferring to host: <10.136.81.233:37425?CCBID=128.104.103.162:9618%3faddrs%3d128.104.103.162-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector4#23067238%20192.170.231.9:9618%3faddrs%3d192.170.231.9-9618+[fd85-ee78-d8a6-8607--1-73b6]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector10#1512850&PrivNet=comp-cc-0463.gwave.ics.psu.edu&addrs=10.136.81.233-37425&alias=comp-cc-0463.gwave.ics.psu.edu&noUDP> ... 040 (271101.000.000) 2024-08-02 15:45:13 Finished transferring input files ... 021 (271101.000.000) 2024-08-02 15:45:14 Warning from starter on slot1_4@glidein_2635188_104012775@comp-cc-0463.gwave.ics.psu.edu: PREPARE_JOB (prepare-hook) succeeded (reported status 000): Using default Singularity image /cvmfs/singularity.opensciencegrid.org/htc/rocky:8-cuda-11.0.3 ... 001 (271101.000.000) 2024-08-02 15:45:14 Job executing on host: <10.136.81.233:39645?CCBID=128.104.103.162:9618%3faddrs%3d128.104.103.162-9618%26alias%3dospool-ccb.osg.chtc.io%26noUDP%26sock%3dcollector10#1506459%20192.170.231.9:9618%3faddrs%3d192.170.231.9-9618+[fd85-ee78-d8a6-8607--1-73b4]-9618%26alias%3dospool-ccb.osgprod.tempest.chtc.io%26noUDP%26sock%3dcollector10#1506644&PrivNet=comp-cc-0463.gwave.ics.psu.edu&addrs=10.136.81.233-39645&alias=comp-cc-0463.gwave.ics.psu.edu&noUDP> SlotName: slot1_4@comp-cc-0463.gwave.ics.psu.edu CondorScratchDir = \"/localscratch/condor/execute/dir_2635172/glide_uZ6qXM/execute/dir_3252113\" Cpus = 1 Disk = 2699079 GLIDEIN_ResourceName = \"PSU-LIGO\" Memory = 1024 ... 006 (271101.000.000) 2024-08-02 15:45:19 Image size of job updated: 2296464 47 - MemoryUsage of job (MB) 47684 - ResidentSetSize of job (KB) ... 040 (271101.000.000) 2024-08-02 15:45:19 Started transferring output files ... 040 (271101.000.000) 2024-08-02 15:45:19 Finished transferring output files ... 005 (271101.000.000) 2024-08-02 15:45:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 38416 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 38416 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 149 1048576 2699079 Memory (MB) : 47 1024 1024 Job terminated of its own accord at 2024-08-02T20:45:19Z with exit-code 0. ... Looking at DAGMan's various files, we see that DAGMan itself ran as a job (specifically, a \"scheduler\" universe job). username@ap40 $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out username@ap40 $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = CONDOR_CONFIG,_CONDOR_*,PATH,PYTHONPATH,PERL*,PEGASUS_*,TZ,HOME,USER,LANG,LC_ALL output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId =?= $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal = ? = 11 || ( ExitCode = ! = UNDEFINED && ExitCode > = 0 && ExitCode < = 2 )) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed ( e.g., during a reboot ) . on_exit_remove = (ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-p 0 -f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -Suppress_notification -CsdVersion $CondorVersion:' '10.7.0' '2024-07-10' 'BuildID:' '659788' 'PackageID:' '10.7.0-0.659788' 'RC' '$ -Dagman /usr/bin/condor_dagman\" environment = \"_CONDOR_DAGMAN_LOG=simple.dag.dagman.out _CONDOR_MAX_DAGMAN_LOG=0 _CONDOR_SCHEDD_ADDRESS_FILE=/var/lib/condor/spool/.schedd_address _CONDOR_SCHEDD_DAEMON_AD_FILE=/var/lib/condor/spool/.schedd_classad\" queue If you want to clean up some of these files (you may not want to, at least not yet), run: username@ap40 $ rm simple.dag.*","title":"Submitting a Simple DAG"},{"location":"materials/workflows/part1-ex1-simple-dag/#challenge","text":"What is the scheduler universe? Why does DAGMan use it? Show hint HTCondor has several universes What would happen to your DAGMan workflow if the access point has to be rebooted? Jobs in the HTCondor queue are \"managed\" - they are always tracked, and restarted automatically if needed","title":"Challenge"},{"location":"materials/workflows/part1-ex2-mandelbrot/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.2: A Brief Detour Through the Mandelbrot Set \u00b6 Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot . Before beginning, ensure that you are connected to ap40.uw.osg-htc.org . Create a directory for this exercise and cd into it. Running goatbrot From the Command Line \u00b6 You can generate the Mandelbrot set as a quick test with two simple commands. Download the goatbrot executable: username@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/goatbrot ./ username@ap40 $ chmod a+x goatbrot Generate a PPM image of the Mandelbrot set: username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000000.ppm -c 0 ,0 -w 3 -s 1000 ,1000 The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Convert the image to the JPEG format (using a built-in program called convert ): username@ap40 $ convert tile_000000_000000.ppm mandel.jpg Dividing the Work into Smaller Pieces \u00b6 The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times: username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000001.ppm -c 0 .75,0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000001_000001.ppm -c 0 .75,-0.75 -w 1 .5 -s 500 ,500 Stitch the small images together into the complete image (in JPEG format): username@ap40 $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The built-in montage program stitches the files together and writes out the final image in JPEG format. View the Image! \u00b6 Run the commands above so that you have the Mandelbrot image. When you create the image, you might wonder how you can view it. Use scp or sftp to copy the mandel.jpg back to your computer to view it.","title":"1.2 - A brief detour through the Mandelbrot set"},{"location":"materials/workflows/part1-ex2-mandelbrot/#workflows-exercise-12-a-brief-detour-through-the-mandelbrot-set","text":"Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot . Before beginning, ensure that you are connected to ap40.uw.osg-htc.org . Create a directory for this exercise and cd into it.","title":"Workflows Exercise 1.2: A Brief Detour Through the Mandelbrot Set"},{"location":"materials/workflows/part1-ex2-mandelbrot/#running-goatbrot-from-the-command-line","text":"You can generate the Mandelbrot set as a quick test with two simple commands. Download the goatbrot executable: username@ap40 $ osdf object get /ospool/uc-shared/public/school/2025/goatbrot ./ username@ap40 $ chmod a+x goatbrot Generate a PPM image of the Mandelbrot set: username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000000.ppm -c 0 ,0 -w 3 -s 1000 ,1000 The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Convert the image to the JPEG format (using a built-in program called convert ): username@ap40 $ convert tile_000000_000000.ppm mandel.jpg","title":"Running goatbrot From the Command Line"},{"location":"materials/workflows/part1-ex2-mandelbrot/#dividing-the-work-into-smaller-pieces","text":"The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations (an HTC approach!) then stitched them together? Once we do that, we can run each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times: username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000000_000001.ppm -c 0 .75,0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1 .5 -s 500 ,500 username@ap40 $ ./goatbrot -i 1000 -o tile_000001_000001.ppm -c 0 .75,-0.75 -w 1 .5 -s 500 ,500 Stitch the small images together into the complete image (in JPEG format): username@ap40 $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The built-in montage program stitches the files together and writes out the final image in JPEG format.","title":"Dividing the Work into Smaller Pieces"},{"location":"materials/workflows/part1-ex2-mandelbrot/#view-the-image","text":"Run the commands above so that you have the Mandelbrot image. When you create the image, you might wonder how you can view it. Use scp or sftp to copy the mandel.jpg back to your computer to view it.","title":"View the Image!"},{"location":"materials/workflows/part1-ex3-complex-dag/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Workflows Exercise 1.3: A More Complex DAG \u00b6 The objective of this exercise is to run a real set of jobs with DAGMan. Make Your Job Submission Files \u00b6 We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files. goatbrot1.sub \u00b6 executable = goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot2.sub \u00b6 executable = goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot3.sub \u00b6 executable = goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot4.sub \u00b6 executable = goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue montage.sub \u00b6 You should notice that the transfer_input_files statement refers to the files created by the other jobs. +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\" executable = montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Notice that the job specified by montage.sub uses a container image, as indicated by the +SingularityImage flag. This is because montage uses libraries that are not installed on the execution nodes. We use a container with montage installed and call it using the executable montage.sh ; thus we will need to create the file montage.sh . #!/bin/bash # Pass all arguments to montage montage \"$@\" Make your DAG \u00b6 In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job? Running the DAG \u00b6 Submit your DAG: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. ----------------------------------------------------------------------- Watch Your DAG \u00b6 Let\u2019s follow the progress of the whole DAG: Use the condor_watch_q command to keep an eye on the running jobs. See more information about this tool here . username@learn $ condor_watch_q If you're quick enough, you may have seen DAGMan running as the lone job, before it submitted additional job nodes: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 - 1 222059.0 [=============================================================================] Total: 1 jobs; 1 running Updated at 2024-07-28 13:52:57 DAGMan has submitted the goatbrot jobs, but they haven't started running yet BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 4 1 - 5 222059.0 ... 222063.0 [===============--------------------------------------------------------------] Total: 5 jobs; 4 idle, 1 running Updated at 2024-07-28 13:53:53 They're running BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 5 - 5 222059.0 ... 222063.0 [=============================================================================] Total: 5 jobs; 5 running Updated at 2024-07-28 13:54:33 They finished, but DAGMan hasn't noticed yet. It only checks periodically: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 4 - 5 222059.0 ... 222063.0 [##############################################################===============] Total: 5 jobs; 4 completed, 1 running Updated at 2024-07-28 13:55:13 Eventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue. Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. username@learn $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, transfer the resulting mandel-from-dag.jpg to your computer so that you can view the image. Does the image look correct? Clean up your results by removing all of the goatbrot.dag.* files if you like. Be careful to not delete the goatbrot.dag file. Bonus Challenge \u00b6 Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"1.3 - A more complex DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#workflows-exercise-13-a-more-complex-dag","text":"The objective of this exercise is to run a real set of jobs with DAGMan.","title":"Workflows Exercise 1.3: A More Complex DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#make-your-job-submission-files","text":"We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files.","title":"Make Your Job Submission Files"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot1sub","text":"executable = goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot1.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot2sub","text":"executable = goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot2.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot3sub","text":"executable = goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot3.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#goatbrot4sub","text":"executable = goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot4.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#montagesub","text":"You should notice that the transfer_input_files statement refers to the files created by the other jobs. +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/htc/rocky:9\" executable = montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Notice that the job specified by montage.sub uses a container image, as indicated by the +SingularityImage flag. This is because montage uses libraries that are not installed on the execution nodes. We use a container with montage installed and call it using the executable montage.sh ; thus we will need to create the file montage.sh . #!/bin/bash # Pass all arguments to montage montage \"$@\"","title":"montage.sub"},{"location":"materials/workflows/part1-ex3-complex-dag/#make-your-dag","text":"In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job?","title":"Make your DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#running-the-dag","text":"Submit your DAG: username@learn $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. -----------------------------------------------------------------------","title":"Running the DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#watch-your-dag","text":"Let\u2019s follow the progress of the whole DAG: Use the condor_watch_q command to keep an eye on the running jobs. See more information about this tool here . username@learn $ condor_watch_q If you're quick enough, you may have seen DAGMan running as the lone job, before it submitted additional job nodes: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 - 1 222059.0 [=============================================================================] Total: 1 jobs; 1 running Updated at 2024-07-28 13:52:57 DAGMan has submitted the goatbrot jobs, but they haven't started running yet BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 4 1 - 5 222059.0 ... 222063.0 [===============--------------------------------------------------------------] Total: 5 jobs; 4 idle, 1 running Updated at 2024-07-28 13:53:53 They're running BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 5 - 5 222059.0 ... 222063.0 [=============================================================================] Total: 5 jobs; 5 running Updated at 2024-07-28 13:54:33 They finished, but DAGMan hasn't noticed yet. It only checks periodically: BATCH IDLE RUN DONE TOTAL JOB_IDS goatbrot.dag+222059 - 1 4 - 5 222059.0 ... 222063.0 [##############################################################===============] Total: 5 jobs; 4 completed, 1 running Updated at 2024-07-28 13:55:13 Eventually, you'll see the montage job submitted, then running, then leave the queue, and then DAGMan will leave the queue. Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. username@learn $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, transfer the resulting mandel-from-dag.jpg to your computer so that you can view the image. Does the image look correct? Clean up your results by removing all of the goatbrot.dag.* files if you like. Be careful to not delete the goatbrot.dag file.","title":"Watch Your DAG"},{"location":"materials/workflows/part1-ex3-complex-dag/#bonus-challenge","text":"Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"Bonus Challenge"},{"location":"materials/workflows/part1-ex4-failed-dag/","text":"Workflows Exercise 1.4: Handling a DAG That Fails \u00b6 The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. Background \u00b6 DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed. Breaking Things \u00b6 Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this, with the -h at the beginning of the highlighted line: executable = /usr/bin/montage arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Submit the DAG again: username@ap40 $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/24 17:57:41 Setting maximum accepts per cycle 8. 06/22/24 17:57:41 ****************************************************** 06/22/24 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/24 17:57:41 ** /usr/bin/condor_dagman 06/22/24 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/24 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/24 17:57:41 ** $CondorVersion: 23.9.0 2024-07-02 BuildID: 742617 PackageID: 23.9.0-0.742617 GitSHA: 5acb07ea RC $ 06/22/24 17:57:41 ** $CondorPlatform: x86_64_AlmaLinux9 $ 06/22/24 17:57:41 ** PID = 26867 06/22/24 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/24 17:57:41 ****************************************************** 06/22/24 17:57:41 Using config source: /etc/condor/condor_config 06/22/24 17:57:41 Using local config sources: 06/22/24 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/24 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/24 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/24 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/24 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/24 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/24 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/24 17:57:41 /etc/condor/condor_config.local Below is where DAGMan realizes that the montage node failed: 06/22/24 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/24 18:08:42 Number of idle job procs: 0 06/22/24 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/24 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/24 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/24 18:08:42 Number of idle job procs: 0 06/22/24 18:08:42 Of 5 nodes total: 06/22/24 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/24 18:08:42 === === === === === === === 06/22/24 18:08:42 4 0 0 0 0 0 1 06/22/24 18:08:42 0 job proc(s) currently held 06/22/24 18:08:42 Aborting DAG... 06/22/24 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/24 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/24 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/24 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/24 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/24 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/24 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because its exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the rescue DAG? Look at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed. username@ap40 $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6 /22/2024 23 :08:42 UTC # Rescue DAG version: 2 .0.1 ( partial ) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG. username@ap40 $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- username@ap40 $ tail -f goatbrot.dag.dagman.out 06/23/24 11:30:53 ****************************************************** 06/23/24 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/24 11:30:53 ** /usr/bin/condor_dagman 06/23/24 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/24 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/24 11:30:53 ** $CondorVersion: 23.9.0 2024-07-02 BuildID: 742617 PackageID: 23.9.0-0.742617 GitSHA: 5acb07ea RC $ 06/23/24 11:30:53 ** $CondorPlatform: x86_64_AlmaLinux9 $ 06/23/24 11:30:53 ** PID = 28576 06/23/24 11:30:53 ** Log last touched 6/22 18:08:42 06/23/24 11:30:53 ****************************************************** 06/23/24 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG 06/23/24 11:30:53 Parsing 1 dagfiles 06/23/24 11:30:53 Parsing goatbrot.dag ... 06/23/24 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/24 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/24 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/24 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished. 06/23/24 11:31:05 Bootstrapping... 06/23/24 11:31:05 Number of pre-completed nodes: 4 06/23/24 11:31:05 Registering condor_event_timer... 06/23/24 11:31:06 Sleeping for one second for log file consistency 06/23/24 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete. 06/23/24 11:31:07 Submitting Condor Node montage job(s)... 06/23/24 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/24 11:31:07 From submit: Submitting job(s). 06/23/24 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/24 11:31:07 assigned Condor ID (84.0.0) 06/23/24 11:31:07 Just submitted 1 job this cycle... 06/23/24 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/24 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/24 11:31:07 Number of idle job procs: 1 06/23/24 11:31:07 Of 5 nodes total: 06/23/24 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/24 11:31:07 === === === === === === === 06/23/24 11:31:07 4 0 1 0 0 0 0 06/23/24 11:31:07 0 job proc(s) currently held 06/23/24 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/24 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/24 11:40:22 Number of idle job procs: 0 06/23/24 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/24 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished. 06/23/24 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/24 11:40:22 Node montage job completed 06/23/24 11:40:22 Number of idle job procs: 0 06/23/24 11:40:22 Of 5 nodes total: 06/23/24 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/24 11:40:22 === === === === === === === 06/23/24 11:40:22 5 0 0 0 0 0 0 06/23/24 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done. 06/23/24 11:40:22 All jobs Completed! 06/23/24 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/24 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/24 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/24 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/24 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/24 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up. Bonus Challenge \u00b6 If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the HTCondor manual to see how to describe your post script.","title":"1.4 - Handling jobs that fail with DAGMan"},{"location":"materials/workflows/part1-ex4-failed-dag/#workflows-exercise-14-handling-a-dag-that-fails","text":"The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures.","title":"Workflows Exercise 1.4: Handling a DAG That Fails"},{"location":"materials/workflows/part1-ex4-failed-dag/#background","text":"DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.","title":"Background"},{"location":"materials/workflows/part1-ex4-failed-dag/#breaking-things","text":"Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this, with the -h at the beginning of the highlighted line: executable = /usr/bin/montage arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Submit the DAG again: username@ap40 $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/24 17:57:41 Setting maximum accepts per cycle 8. 06/22/24 17:57:41 ****************************************************** 06/22/24 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/24 17:57:41 ** /usr/bin/condor_dagman 06/22/24 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/24 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/24 17:57:41 ** $CondorVersion: 23.9.0 2024-07-02 BuildID: 742617 PackageID: 23.9.0-0.742617 GitSHA: 5acb07ea RC $ 06/22/24 17:57:41 ** $CondorPlatform: x86_64_AlmaLinux9 $ 06/22/24 17:57:41 ** PID = 26867 06/22/24 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/24 17:57:41 ****************************************************** 06/22/24 17:57:41 Using config source: /etc/condor/condor_config 06/22/24 17:57:41 Using local config sources: 06/22/24 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/24 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/24 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/24 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/24 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/24 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/24 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/24 17:57:41 /etc/condor/condor_config.local Below is where DAGMan realizes that the montage node failed: 06/22/24 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/24 18:08:42 Number of idle job procs: 0 06/22/24 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/24 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/24 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/24 18:08:42 Number of idle job procs: 0 06/22/24 18:08:42 Of 5 nodes total: 06/22/24 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/24 18:08:42 === === === === === === === 06/22/24 18:08:42 4 0 0 0 0 0 1 06/22/24 18:08:42 0 job proc(s) currently held 06/22/24 18:08:42 Aborting DAG... 06/22/24 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/24 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/24 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/24 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/24 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/24 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/24 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because its exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the rescue DAG? Look at the rescue DAG file. It's called a partial DAG because it indicates what part of the DAG has already been completed. username@ap40 $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6 /22/2024 23 :08:42 UTC # Rescue DAG version: 2 .0.1 ( partial ) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG. If you didn't fix the problem, DAGMan would generate another rescue DAG. username@ap40 $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- username@ap40 $ tail -f goatbrot.dag.dagman.out 06/23/24 11:30:53 ****************************************************** 06/23/24 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/24 11:30:53 ** /usr/bin/condor_dagman 06/23/24 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/24 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/24 11:30:53 ** $CondorVersion: 23.9.0 2024-07-02 BuildID: 742617 PackageID: 23.9.0-0.742617 GitSHA: 5acb07ea RC $ 06/23/24 11:30:53 ** $CondorPlatform: x86_64_AlmaLinux9 $ 06/23/24 11:30:53 ** PID = 28576 06/23/24 11:30:53 ** Log last touched 6/22 18:08:42 06/23/24 11:30:53 ****************************************************** 06/23/24 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG 06/23/24 11:30:53 Parsing 1 dagfiles 06/23/24 11:30:53 Parsing goatbrot.dag ... 06/23/24 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/24 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/24 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/24 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished. 06/23/24 11:31:05 Bootstrapping... 06/23/24 11:31:05 Number of pre-completed nodes: 4 06/23/24 11:31:05 Registering condor_event_timer... 06/23/24 11:31:06 Sleeping for one second for log file consistency 06/23/24 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete. 06/23/24 11:31:07 Submitting Condor Node montage job(s)... 06/23/24 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/24 11:31:07 From submit: Submitting job(s). 06/23/24 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/24 11:31:07 assigned Condor ID (84.0.0) 06/23/24 11:31:07 Just submitted 1 job this cycle... 06/23/24 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/24 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/24 11:31:07 Number of idle job procs: 1 06/23/24 11:31:07 Of 5 nodes total: 06/23/24 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/24 11:31:07 === === === === === === === 06/23/24 11:31:07 4 0 1 0 0 0 0 06/23/24 11:31:07 0 job proc(s) currently held 06/23/24 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/24 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/24 11:40:22 Number of idle job procs: 0 06/23/24 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/24 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished. 06/23/24 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/24 11:40:22 Node montage job completed 06/23/24 11:40:22 Number of idle job procs: 0 06/23/24 11:40:22 Of 5 nodes total: 06/23/24 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/24 11:40:22 === === === === === === === 06/23/24 11:40:22 5 0 0 0 0 0 0 06/23/24 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done. 06/23/24 11:40:22 All jobs Completed! 06/23/24 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/24 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/24 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/24 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/24 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/24 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up.","title":"Breaking Things"},{"location":"materials/workflows/part1-ex4-failed-dag/#bonus-challenge","text":"If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the HTCondor manual to see how to describe your post script.","title":"Bonus Challenge"},{"location":"materials/workflows/part1-ex5-challenges/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus Workflows Exercise 1.5: YOUR Jobs and More on Workflows \u00b6 The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. Challenge 1 \u00b6 Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options. Challenge 2 \u00b6 Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie. Challenge 3 \u00b6 Try out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Pegasus on OSG If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu","title":"1.5  - Workflow Challenges"},{"location":"materials/workflows/part1-ex5-challenges/#bonus-workflows-exercise-15-your-jobs-and-more-on-workflows","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Bonus Workflows Exercise 1.5: YOUR Jobs and More on Workflows"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-1","text":"Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options.","title":"Challenge 1"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-2","text":"Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie.","title":"Challenge 2"},{"location":"materials/workflows/part1-ex5-challenges/#challenge-3","text":"Try out Pegasus. Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Pegasus on OSG If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu","title":"Challenge 3"}]}